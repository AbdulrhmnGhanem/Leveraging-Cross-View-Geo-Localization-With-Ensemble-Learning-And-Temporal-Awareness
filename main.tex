\documentclass[10pt,letterpaper]{article}
\usepackage{booktabs, makecell}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{ {./figures/png/} }
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}
\usepackage[linesnumbered]{algorithm2e}
\usepackage[section]{placeins}

\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}


% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

\usepackage{color,soul}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}
\newif\ifhighlight
\highlighttrue
\newcommand{\hlb}[1]{\ifhighlight{\hl{#1}}\else{#1}\fi}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Leveraging cross-view geo-localization with ensemble learning and temporal awareness} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Abdulrahman Ghanem\textsuperscript{1},
Ahmed Abdelhay\textsuperscript{1},
Noor Eldeen Salah\textsuperscript{1},
Ahmed Nour Eldeen\textsuperscript{1},
Mohammed Elhenawy\textsuperscript{2},
Abdallah A. Hassan\textsuperscript{1},
Ammar M. Hassan\textsuperscript{3*},
Mahmoud Masoud\textsuperscript{2}
\\
\bigskip
\textbf{1} Computer and Systems Engineering Department, Faculty of Engineering, Minia University, Minia, Egypt
\\
\textbf{2}  Centre for Accident Research and Road Safety-Queensland (CARRS-Q), Queensland University of Technology, Brisbane, Australia
\\
\textbf{3} Arab Academy for Science, Technology, and Maritime Transport, South Valley Branch, Aswan, Egypt
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% % Additional Equal Contribution Note
% % Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% % Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% % \textcurrency b Insert second current address 
% % \textcurrency c Insert third current address

% % Deceased author note
% \dag Deceased

% % Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* ammar@aast.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
The Global Navigation Satellite System (GNSS) is unreliable in some situations. To mend the poor GNSS signal, an autonomous vehicle can self-localize by matching a ground image against a database of geotagged aerial images. However, this approach has challenges  because of the dramatic differences in viewpoint between aerial and ground views, harsh weather and lighting conditions, and a lack of orientation information in training and deployment environments. In this paper, we show that previous models in this area are complementary, not competitive, and that each model solves a different aspect of the problem. We propose an ensemble model to aggregate the predictions of multiple independently trained state-of-the-art models. Also, we show the effect of making the query process temporal-aware  But none of the existing benchmark datasets is suitable for extensive temporal awareness experiments, we generated a new derivative dataset based on the BDD100K dataset. This paper also proposes an efficient temporal awareness mechanism: naive history, to prove that fusing the temporal information of the trip with the model prediction significantly improves the accuracy. Our ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset (surpassing the current state-of-the-art). Our temporal awareness mechanism converges to R@1 of ~100\% by looking at a few steps back in the trip history. We expect this study to initialize the building of deployable ensemble cross-view geo-localization models that take advantage of temporal proximity between ground images.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
The current standard localization technique is the global navigation satellite system (GNSS). Although the GNSS accuracy declines in cases where there are few lines of sight (e.g., urban canyons~\cite{bib1}). Using cross-view geo-localization, a vehicle localizes itself by matching street view images against a database of geotagged images captured from aerial platforms (e.g., a satellite~\cite{bib2} or a drone~\cite{bib3}). Cross-view geo-localization is gaining popularity in the scene of autonomous vehicles~\cite{bib2} and robotic navigation~\cite{bib25}: it compensates for a bad GNSS signal-to-noise ratio. And,  it’s preferable to other image-based localization techniques (e.g., landmark and ground-to-ground matching) for the ease of covering new areas. Early work~\cite{bib37} on this technique claimed that its main challenge is the lack of visual correspondence between aerial and ground views. Later work, while holding the same claim, showed that there are more challenges: geographic scene changes over time~\cite{bib6,bib7,bib8,bib9,bib10}, poor weather and lighting conditions, lack of orientation information~\cite{bib4}, and misalignment between aerial and ground images during training.
None of the previous models tried to collectively address these five challenges. To build a holistic solution for the problem, we propose an ensemble model to fuse the predictions of five independently trained models. Each of these models addresses a different challenge.

Moreover, most of the recent work treats the problem as a 1-to-1 image-matching task. This overlooks the fact that these models get deployed in environments where there's a continuous stream of input, not a single query image. Taking the temporal nature of the problem into account is a must:  our experiments shows that the recent models can't differentiate between highly similar query images, and in the case of a moving vehicle, the consecutive images have a high degree of similarity. Some recent models considered the temporal nature of the problem.~\cite{bib7}  and~\cite{bib20,bib21,bib22} used \emph{Markov chain Monte Carlo (MCMC)} algorithms to predict the current pose and enforce temporal consistency.~\cite{bib24} enforces temporal consistency by using a transformer-based trajectory smoothing network. We can see that these methods are resource intensive or have strong assumptions about the current state of the vehicle. We explore an efficient technique to achieve the same goal.
Although the existing datasets aren't suitable for conducting experiments to prove that temporal awareness improves accuracy  — \cite{bib24} used BDD100k to generate a derivative dataset, sorrowfully, upon contacting the authors to grant us access to this dataset we received no response. The dataset needs to be realistic and collected as a trajectory of close points over a long-running journey to resemble driving in a real environment.  CVUSA~\cite{bib55} and CVACT~\cite{bib4} include sparse points on the map. VIGOR~\cite{bib5} includes dense points but doesn't form trajectories. RobotCar~\cite{bib6} has a limited number of examples. So we had to introduce a new derivative dataset based on the BDD100K~\cite{bib12} dataset to fill this gap.

\subsection*{In summary, the contributions of this paper are:}
\begin{itemize}
    \item We introduce a new ensemble model based on five of the current state-of-the-art  cross-view matching networks. The ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset.
    \item We construct a new derivative dataset that is suitable for temporal aware cross-view geo-localization models based on BDD100K.
    \item We develop a meta block: naive history, to make the query process temporal aware. We show that taking journey history into account minimizes the search space. This reduction in search space improves the accuracy.  The accuracy converges to ~100\% with a three-step lookback into trip history on our proposed dataset. This meta block is usable with any CV model.
\end{itemize}

\section*{Related work}
In section (A), we start by investigating how different models engineered their features and architectures. Their choices show us how different models tried to approach the problem from different angles. Followed by giving a bird-eye-view of the architectures of these models and feature extraction methods. In section (B), we walk through the models that tried blending the trip history into the image-matching task. By grouping the models into two categories: one that relies on "this place looks familiar" in section (B.1), and one that relies on "where have we been before getting here?" in section (B.2).
\subsection*{A) Features and architectures}
Most of the recent work treated the cross-view geo-localization task as an image retrieval task. They tried to find a feature representation suitable for matching query ground images and aerial ground images. The nature and complexity of used networks changed over time. Here, we conduct a brief comparison between the most common feature representations and their architectures. Table~\ref{table1} summarizes the feature types and the backbones of different cross-view geo-localization networks.


\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
  \centering
  \caption{
  {\bf A summary of the feature types and the backbones of different cross-view geo-localization networks.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Feature type} & {\bf Backbone} & {\bf Used in}&  \\ \midrule
    & Hand-crafted & SIFT & ~\cite{bib20}~\cite{bib26}& \\
    & Hand-crafted & SURF, FREAK, PHOW & ~\cite{bib20}& \\
    & Hand-crafted & SIFT + VLAD & ~\cite{bib7}& \\
    & Semantic & Faster R-CNN & ~\cite{bib32}& \\
    & CNN & VGG + FCN + NetVLAD & ~\cite{bib36}~\cite{bib22}~\cite{bib14}& \\
    & CNN & AlexNet & ~\cite{bib37}~\cite{bib32}& \\
    & CNN & VGG & ~\cite{bib21}~\cite{bib4}~\cite{bib13}& \\
    & CNN & ResNet & ~\cite{bib4}~\cite{bib21}~\cite{bib41}& \\
    & CNN & DenseNet & ~\cite{bib4}~\cite{bib21}& \\
    & CNN & U-net & ~\cite{bib4}& \\
    & CNN & Xception & ~\cite{bib21}& \\
    & CNN & - & ~\cite{bib44}& \\
    & CNN & VGG + FCN & ~\cite{bib43}& \\
    & Attentive & Siam-FCANet (ResNet + FCAM + FCN) & ~\cite{bib45}& \\
    & Attentive & Siam-VFCNet (ResNet + FCAM + NetVLAD) & ~\cite{bib45}& \\
    & Attentive & VGG + SAFA + SPE & ~\cite{bib46}~\cite{bib47}~\cite{bib17}& \\
    & Attentive & VGG + Geo Attention + Geo-temporal Attention & ~\cite{bib24}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib8}& \\
    & Attentive & SAFA & ~\cite{bib5}& \\
    & Attentive & ResNet + SAFA & ~\cite{bib18}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib19}& \\
    & Attentive & VGG + MSAE & ~\cite{bib49}& \\
    & Synthesized & X-Fork & ~\cite{bib50}& \\ \bottomrule
    \end{tabular}
  }
  \label{table1}
\end{table}

\subsubsection*{1) Hand-crafted features}
Early research used hand-crafted features.~\cite{bib20,bib26} used SIFT~\cite{bib27}. Also,~\cite{bib20} experimented with other feature spaces (e.g., SURF~\cite{bib28}, FREAK~\cite{bib29}, PHOW~\cite{bib30}) but SIFT outperformed others.~\cite{bib7} computed dense SIFT features, then embedded them into a higher dimensional vector space using a VLAD~\cite{bib31}. The extracted features are brittle; it fails to adapt to appearance change. Later, it proved to have inferior performance compared to the CNN-based features.
\subsubsection*{2) Semantic features}
In this approach, the networks matched between ground and aerial images based on the meaningful content of the image. This made it more robust to viewpoint changes than local features. But the model performance degraded in areas lacking the pre-selected semantic features.~\cite{bib32} treated the problem as object detection and recognition: the first block of the architecture employed the Faster R-CNN~\cite{bib33} to detect buildings, and the second block of the architecture used AlexNet~\cite{bib34} with the Siamese architecture~\cite{bib35} to recognize the buildings.
\subsubsection*{3) CNN-based features}
Metric learning achieved promising results in bridging the domain gap between aerial and ground image representation.~\cite{bib36} used a fully convolutional network (FCN) with a NetVLAD~\cite{bib34} layer using a Siamese architecture.~\cite{bib37} tried modified versions of AlexNet.~\cite{bib21} experimented with different FCN layers: VGG~\cite{bib31}, ResNet~\cite{bib38}, DenseNet~\cite{bib39}, and Xception~\cite{bib40} with the same architecture of~\cite{bib36}, they found that VGG outperforms other networks. Also,~\cite{bib22} used the CVM-Net-I architecture proposed in~\cite{bib36}.~\cite{bib41} exploited a modified ResNet50 network for ground images and a ResNet18 for aerial ones.~\cite{bib13} used VGG16 to generate the feature maps for the polar transformed aerial and then fed it into the Dynamic Similarity Module (DSM).~\cite{bib4} learned orientation information by using different backbones (VGG, ResNet, DenseNet, and U-net~\cite{bib42}) with the Siamese architecture.~\cite{bib43} employed  the Siamese network with a VGG backbone to extract feature maps, then a fully connected layer aggregates these feature maps.~\cite{bib44} used a CNN to generate feature maps and then transform them from the ground domain to the aerial domain.~\cite{bib14} applied the hybrid perspective mapping method using the CVM-Net-I architecture.
\subsubsection*{4) Attentive features}
For this feature type, the networks used spatial attention to enhance the feature representation.~\cite{bib45} integrated the lightweight attention module (FCAM) into each block of the basic ResNet.~\cite{bib46} used a spatial-aware feature aggregation (SAFA) module to mitigate the distortion in the aerial image. Also, employed the spatial-aware position embedding module (SPE) to encode relative positions among features in the feature maps.~\cite{bib5} proposed the VIGOR network built on top of SAFA.~\cite{bib47,bib17} used the same architecture as~\cite{bib46} with a different loss function: geo-distance weighted loss.~\cite{bib24} proposed a geo-attention module for the aerial branch, and a temporal-attention module for the ground branch.~\cite{bib8} used convolutional block attention modules~\cite{bib48} to generate multi-scale attention features.~\cite{bib18} employed a modified ResNet34 backbone with a spatial-aware attention module.~\cite{bib19} proposed EgoTR network. EgoTR used a ResNet backbone transformer encoder with a self-cross attention mechanism.~\cite{bib49} introduced the Multi-Scale Attention Encoder (MSAE). MSAE employed a VGG backbone with a multi-scale attention encoder followed by FCN to generate feature masks.

\subsubsection*{5) Synthetic features}
In this approach, the networks learned robust feature representation by reversing the task: it learned how to create ground view from aerial views, which made it learn salient features and suppress others.~\cite{bib50} synthesized aerial representation of a ground panorama query using the X-Fork network~\cite{bib51} with edge maps detection by Canny Edge Detection~\cite{bib52}.

\subsection*{B) Contextual awareness}
The fact that these models get deployed in autonomous vehicles, makes it obvious that the models should be aware of the trip context. We can categorize contextual awareness as follows:

\subsubsection*{1) Spatial awareness}
We have to differentiate between two types of spatial awareness.

\begin{itemize}
  \item Some models refer to it as the knowledge about the pose (location and orientation) of the query image and its relative pose to the aerial image frame.
  \cite{bib47,bib17} constructed mini-batches of images within a certain geographic radius and used a modified version of the triplet loss function: Geo-distance weighted loss to favor examples where the images are within the selected radius. The Geo-Attention module used in~\cite{bib24} exploits a similar loss function.~\cite{bib14} employed hybrid perspective mapping to establish correspondence between ground and aerial images.~\cite{bib4} injected the orientation information into the network.~\cite{bib4} used multiplane image (MPI)~\cite{bib13} projections to exploit geometric correspondence between ground and aerial images. Table~\ref{table2} gives an overview of spatial (pose-wise) awareness approaches used in cross-view geo-localization.
  \begin{table}[!ht]
    \centering
    \caption{
    {\bf An overview of the spatial (pose-wise) awareness approaches used in cross-view geo-localization.}}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Geographic proximity & ~\cite{bib47}~\cite{bib17}~\cite{bib24}& \\
    & Polar Transform & ~\cite{bib3,bib4}~\cite{bib5}~\cite{bib13,bib14}~\cite{bib17,bib18,bib19}& \\
    & Inverse Polar Transform & ~\cite{bib49}& \\
    & Orientation & ~\cite{bib4}& \\
    & Dynamic Similarity Matching (DSM) & ~\cite{bib13}& \\
    & Hybrid Perspective Mapping & ~\cite{bib14}& \\ \bottomrule
    \end{tabular}
    \label{table2}
  \end{table}
  \item Other networks refer to it as paying more attention to the salient features, suppressing less important features, and encoding the spatial layout information into the feature representation. We have covered that in section (A.4).
\end{itemize}

\subsubsection*{2) Temporal awareness}
There are two types of temporal awareness too!
\begin{itemize}
  \item Finding a robust feature representation that won’t be affected by scene changes throughout time. These changes can be geographic landmarks (e.g., new constructions) or environmental conditions such as weather conditions.
  ~\cite{bib6,bib54} used time-invariant approaches by capturing the same scene during different conditions across time. But, this technique required a dataset where the same scene is covered during different conditions. Possible solutions for this are as follows. {\bf A)}~\cite{bib8} used semantic object-based data augmentation techniques to remove and add objects (cars, roads, greenery, and sky). {\bf B)}~\cite{bib7} applied PCA projection to make background features less significant in the image descriptor. {\bf C)} Another solution is using synthetic datasets.
  \item Exploiting the fact these models will be deployed on vehicles where a temporally coherent sequence of images is available.
  ~\cite{bib7} and~\cite{bib20,bib21,bib22} used MCMC algorithms, namely, a particle filter~\cite{bib45} with variations of initialization and transition techniques, the algorithms are used to predict current pose and enforce temporal consistency.~\cite{bib24} introduced a geo-temporal attention module, the module attends to all frames in the video to learn better features, it also enforces temporal consistency by using a transformer-based trajectory smoothing network.
\end{itemize}
We give an overview of the temporal awareness approaches used in cross-view geo-localization in Table~\ref{table3}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf An overview of the temporal awareness approaches used in cross-view geo-localization.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Capture multiple examples with different environmental conditions & ~\cite{bib6}~\cite{bib54}& \\
    & Semantic object-based data augmentation & ~\cite{bib8}& \\
    & Observation encoder + PCA projection & ~\cite{bib7}& \\
    & MCMC algorithms & ~\cite{bib7}~\cite{bib20,bib21,bib22}& \\
    & Sequence attention + Trajectory Smoothing Network & ~\cite{bib24}& \\ \bottomrule
    \end{tabular}
  }
  \label{table3}
\end{table}

\section*{Methodology}
\subsection*{A) Evaluation metric}
In this research, we use the same evaluation metric used in~\cite{bib3,bib4,bib5,bib8,bib13,bib14,bib17,bib18,bib19,bib20,bib21,bib22,bib24,bib36,bib43,bib44,bib45,bib46,bib47,bib49,bib50}: the {\bf Recall@k (r@k)}. In r@k, we consider it a match if the corresponding aerial image is in the top k predictions. We use r@1, r@5, r@10, and r@1\%. r@1 means the true image is the first prediction of the model, r@5 means the true image is in the first five predictions of the model, and so on.

\subsection*{B) The ensemble model}

There are many challenges in cross-view matching. To name a few:  missing correspondence and orientation information, scene changes over time, and the high similarity among geographically close points. Recently, several models were developed to solve the cross-view (CV) matching problem, and different models approached the CV matching problem from different angles. Thus, each model has its advantages and disadvantages. We hypothesize that aggregating the outputs of these uncorrelated models might improve the accuracy.
So, in this research, we build an ensemble of independently trained models to solve the CV matching problem. The proposed ensemble used the same datasets to train different neural network architectures. We use the CVUSA~\cite{bib55} and CVACT~\cite{bib4} benchmark datasets, to train five models Each model addresses a  different aspect of the problem: DSM~\cite{bib13} estimates the cross-view orientation alignment.  EgoTR~\cite{bib19} models global dependencies to decrease visual ambiguities and matches geometric configuration between ground and aerial images. SAFA~\cite{bib46} exploits geometric correspondence between aerial and panoramic ground images. Toker~\cite{bib18} biases the localization network via a Generative Adversarial Network (GAN) to learn salient features. SFCANet~\cite{bib45} uses Hard Exemplar Reweighting to assign a greater weight to hard examples. Table~\ref{table4} shows their r@k metrics.

\begin{table}[!ht]
\begin{adjustwidth}{-1.0in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
  \centering
  \caption{
    {\bf The r@k metrics for the networks used to construct the  ensemble model.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllllcrrrrrrr@{}}
      \toprule
      & \multirow{3}{*}{\bf Network} & \multicolumn{4}{c}{\bf CVUSA} & \phantom{abc}& \multicolumn{4}{c}{\bf CVACT} & \phantom{abc} & \\
      \cmidrule{3-6} \cmidrule{8-11}
      & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \\
      &\phantom{abc} & \multicolumn{9}{c}{(\%)}  \\ \bottomrule
      & DSM~\cite{bib13} & 92.07 & 97.33 & 98.33 & 99.60 & \phantom{abc} & 81.93 & 91.83 & 93.95 &  97.42& \\
      & Toker~\cite{bib18} & 92.15 & 97.28 & 98.26 &  99.56 & \phantom{abc}  & 83.52 & 93.89 & 95.48 &  98.17& \\
      & EgoTR~\cite{bib19} & 93.87 & 98.22 & 98.98 & 99.66 & \phantom{abc} & 84.87 & 94.5 & 95.95 &  98.37& \\
      & SFCANet~\cite{bib45} & 51.01 & 78.92 & 86.80 & 98.49 & \phantom{abc} & x & x & x &  x& \\ 
      & SAFA~\cite{bib46} & 88.93 & 96.65 & 97.97 & 99.65 & \phantom{abc} & 74.56 & 89.65 & 92.46 &  97.72& \\ \bottomrule
    \end{tabular}
  }
  \label{table4}
  \end{adjustwidth}
\end{table}

This research investigates two different aggregation methods: soft-voting, and hard-voting. As shown in Fig~\ref{fig1}, for both methods, we try all possible  combinations of the models (their power set).

\begin{figure}[!ht]
\caption{{\bf Different ensemble aggregation methods.} We use 5 state-of-the-art models and combined their outputs using 32 different combinations for each strategy. }
\includegraphics*[width=0.9\linewidth]{fig1.tiff.png}
\label{fig1}
\end{figure}

In soft-voting, we experiment with two calculation methods: {\bf A)} averaging the predictions of the individual models. {\bf B)} calculating the joint probability, using Eq~(\ref{eq1}). Both methods have identical results.

\begin{eqnarray}
\label{eq1}
\text{total vote} = \exp(\log(vote_1) + \log(vote_2) + ..., \log(vote_n)),
\end{eqnarray}

In hard-voting, we select the majority vote of the models. Hard-voting needs at least three models to have a majority vote. There are two cases where a majority vote doesn’t exist:
\begin{enumerate}
  \item Combinations with an even number of models can tie, so we pick the combination containing the most accurate model.
  \item All models’ predictions differ, we try two strategies: {\bf A)} take the prediction of the most accurate model in the combination. {\bf B)} pick a prediction from a random normal distribution of individual models' predictions.
\end{enumerate}

\subsection*{B) BDD trajectories dataset collection}
Proposed approaches in this research exploit the temporal nature of the problem. The existing benchmark datasets (e.g, CVUSA, and CVACT) are collected from sparse points on the map while we need a trajectory of close points. Inspired by the work of Regmi and Shah~\cite{bib24}, we construct a new derivative dataset from the BDD100k~\cite{bib12} dataset. The BDD100K dataset is crowdsourced, diverse, and large-scale, with IMU/GPS recording, and other semantic annotations (irrelevant to this research). All videos in the dataset are  long, though the total distance varies. We chose the videos with a distance greater than 50 m, the statistical summary of the distance covered in the selected videos is in Table~\ref{table5}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf A statistical summary of the distance covered in selected videos.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
    & {\bf Count} & {\bf Mean} & {\bf \emph{std}} & {\bf Min} & {\bf Max} &\\ \midrule
    & 47943 videos & 278.668 m & 181.786 m & 50.000 m & 2560.000 m \\ \bottomrule
    \end{tabular}
  }
  \label{table5}
\end{table}

Our dataset consists of 95,000 examples. Each example consists of five ground images and one aerial image, and some examples are shown in Fig~\ref{fig2}. We sample the ground images from the picked videos with a sampling rate of $1 / frame/10 m$ to have some visual changes between the consecutive frames, but at the same time, the frames still look relatable to one another. The \emph{IMU/GPS} data is captured at $1 \ sample/s$. The distance moved in one second varies; the speed of the vehicles is not constant. In our dataset, we care about the visual changes, not the passage of time. So when the distance between every two consecutive locations is greater than $10 \ m$,  we use following the sampling algorithm:

\begin{algorithm}[!h]
\caption{BDD100K resampling}
\KwData{The GPS/IMU information recorded along with the videos, BDD100K videos.}
\KwResult{Resmapled frames(1 frame/10 m)}
\Comment{ Get the speed at the current ($v_n$), and next ($v_{n+1}$) location from IMU data.
\
Assume the speed between these two locations ($v_{n \to n+1}$) is the average speed of both locations. For all locations on the trajectory which is a multiple of the sampling distance parameter ($d$):}\
$v_{\text{n} \to \text{n+1}} \gets \frac{v_{\text{n}} + v_{\text{n} + 1}}{2}, n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \}$ \;

\Comment{To get the timestamp of the nth frame ($t_{\text{n}}$)}

$t \gets \frac{\text{d} \times  \text{n}}{v_{\text{n}} + v_{\text{n} + 1}} , n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \} $;\

Extract the frames at the selected timestamps using FFmpeg~\cite{bib23};\
\end{algorithm}

\begin{figure}[!ht]
  \caption{{\bf Three examples from our dataset. For ground images.} The time progresses from left to right and the distance between the consecutive frames is 10 m.}
  \includegraphics*[width=0.9\linewidth]{fig2.tiff.png}
  \label{fig2}
\end{figure}

We capture the aerial tiles at the midpoint of the example using the great circle algorithm~\cite{bib16} and then fetch them from Google Maps~\cite{bib15}. We experimented with different zoom levels, Fig~\ref{fig3}. Furthermore, we chose the 20th zoom level; it covers a wide area with great detail. We chose a tile size of  $800 \times 800$ as shown in Fig~\ref{fig4}.

\begin{figure}[!ht]
  \caption{{\bf Examples of three different zoom levels for aerial tile. Markers are 10 m apart on the trajectory.}}
  \includegraphics*[width=0.9\linewidth]{fig3.tiff.png}
  \label{fig3}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf An example of different tile sizes for the same zoom level.}}
  \includegraphics*[width=0.9\linewidth]{fig4.tiff.png}

  \label{fig4}
\end{figure}

After extracting the frames, we remove trajectories where 30\% of the extracted ground frames are mildly lit ($44.5\%$of all trajectories). For example, we take the trajectory of the Bright frame in Fig~\ref{fig5} and drop the other two trajectories. Then we removed the trajectories that have blurry aerial tiles ($13.5\%$ of the bright trajectories) similar to the example shown in Fig~\ref{fig6}. We use a Laplacian filter~\cite{bib11} with a threshold of $200$ to detect blurry aerial tiles, and a grayscale mean filter with a threshold of $85$ to detect dark ground frames. We chose both thresholds empirically to drop all the true positive corrupt examples with some false positives. Fig~\ref{fig7} shows a simplified dataflow of our cleaning pipeline. We scaled the ground and aerial images’ width to $400$ while keeping the height in aspect ratio using the \emph{Lanczos algorithm}.

\begin{figure}[!ht]
  \caption{{\bf Examples of different lighting conditions in the BDD100K dataset.} In the dark frames, we can't construct a meaningful correspondence between the ground and aerial images.}
  \includegraphics*[width=0.9\linewidth]{fig5.tiff.png}

  \label{fig5}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf An example of a blurry aerial image.} Sometimes this is deliberate by the satellite imagery provider for privacy reasons.}
  \includegraphics*[width=0.9\linewidth]{fig6.tiff.png}

  \label{fig6}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf A simplified version of our data cleaning pipeline.} This dataflow pipeline is \emph{embarrassingly parallel}. We had it running across 6 machines cluster, 16 core each.}
  \includegraphics*[width=0.9\linewidth]{fig7.tiff.png}
  \label{fig7}
\end{figure}

\FloatBarrier

\subsection*{C) Naive history}
Inspired by our experiments on the joint probability as a soft-voting strategy for ensemble learning. In a journey setting where the history of the journey is available, we hypothesize that taking the previous predictions into account might cause the prediction to converge while progressing in the journey. This hypothesis relies on the fact that even if the model doesn't return the true location as its top-1 prediction, most of the time, it is still in the top-1\% predictions. Also, the probability that a model will predict N consecutive wrong predictions decreases as N increases. Based on this, we fine-tune the EgoTR model on the new derivative dataset (\emph{BDD-trajectories}). We use this dataset because it has trip trajectories.

\subsubsection*{Fine-tuning EgoTR}
The EgoTR model takes a pair of images as input: a ground image and a satellite image as its input. However, an example in our dataset consists of a hexad: five ground images and a satellite image. So we have to reshape our dataset to be suitable for the EgoTR input format. We choose the examples where there are at least two examples from the same journey, so we have 10 ground images from the same trajectory. We pair every ground image and the satellite image in the examples. This means that each example in \emph{BDD-trajectories} corresponds to five examples in the reshaped dataset. We use a subset of reshaped dataset $19015$ pairs for validation and $75985$ pairs for fine-tuning.

\subsubsection*{Attaching the naive history block to EgoTR}
After fine-tuning EgoTR we generate a distance array for all the images in our validation dataset. Then we use this array as input for Algorithm~\ref{alg:two}.

\begin{algorithm}[H]
  \label{algorithm2}
  \caption{Naive history}\label{alg:two}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$}
  \KwResult{History reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  \Return $m_{ij}$\;
\end{algorithm}


The naive history meta block reinforces the prediction at the current position by looking back into the trip history. The $distanceArray$ parameter is the distance between every ground and aerial image (the output of the model).  The $historyDepth$ parameter controls how deep we look back into history. We shift the total distance array by $historyDepth$ columns and rows to get the distance array at the previous location (Algorithm 2, Line 6). We get a shifted version of the $distanceArray$ to leave the predictions at future steps intact (Algorithm 2, Line 7).  We multiply, element-wise ($\odot$), the distance array at the current step and the distance array at the previous step (Algorithm 2, Line 8). We update the original array with the reinforced predictions (Algorithm 2, Line 9). Repeat the steps 4 through 7 (Algorithm 2, Line 4), with increasing value of $historyDepth$ until reaching the value of final history depth value, for example, if we want to lookback five steps in trip history we will iterate over historyDepth  \{1, 2, 3, 4, 5\}.

\subsubsection*{The effect of prior on naive history}
As mentioned before the cross-view models are complementary to existing GNSS, so we can improve naive history performance by initializing it with a weak prior (the location captured by the GNSS).  We use Algorithm~\ref{alg:three} to initialize the distance array (generated by the model) with a probability of the first image in each example equal to $1\mathrm{e}{-6}$. In other words, (Algorithm 3, Lines 5-9) the probability of the first image in the trajectory is modified to make the probability of the ground truth image equal 1e-6 and set other probabilities to a uniform value of $(1 - (1\mathrm{e}{-6}) / distance array size)$. Our experiments prove that initializing the naive history algorithm with this prior knowledge speeds up the convergence to 100\% accuracy significantly.

\begin{algorithm}[H]
  \caption{Naive history with a weak prior}\label{alg:three}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$, $priori \in R_{+}$, $trajectorySize  \in N$}
  \KwResult{Prior-aware history reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  $normalizer \gets priori/ len$\;
  
  \For{$k \in [0, len] \land k\ \% \ trajectorySize \equiv 0$} {
      $currentPrediction_{ij} \gets (m_{ij})_{\substack{i\\ j \equiv k }}$\;
      $ (currentPrediction_{ij})_{\substack{i \equiv k}} \gets priori$\;
      $(m_{ij})_{\substack{i\\ j \equiv k }} \gets currentPrediction_{ij} - normalizer$\;
  }
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  
  \Return $m_{ij}$\;
\end{algorithm}
\FloatBarrier

\section*{Results \hlb{and discussion}}
\subsection*{A) The ensemble model}
\hlb{Altough there are multiple options for bulding the ensemble model (e.g., stacking, boosting, and mixing models). We only choose mixing models (i.e., voting) for the following reasons:}
\begin{itemize}
  \item \hlb{If we tried stacking models, we would have to train the base models along side the meta model, otherwise, the meta model would overfit to the base models. Base models retraining would require a lot of computational resources.}
  \item \hlb{All member models in the ensemble are trained on the entire dataset which means the meta leaner would also train on the same data as the base model. The other option is to train all the models from scratch, but this would require a lot of training resources.}
  \item \hlb{Boosting models is not a good option because the base models are independent: the models have different weaknesses and strengths.}
  \item \hlb{More importantly, the main goal is to show the need for wholistic solution for the problem, rather than showing the effectiveness of the ensemble model: in real time scenarios, a single model with something like the naive history algorithm would be much more efficient than an ensemble model.}
\end{itemize}
Fig~\ref{fig8} and Fig~\ref{fig9} show the results of the soft-voting strategies for CVUSA and CVACT, respectively. The DSM, EgoTR, and Toker combination outperforms other combinations. Increasing the number of the models doesn’t necessarily improve the accuracy; to improve the accuracy; individual models have to predict different examples correctly.

\begin{figure}[!ht]
  \caption{{\bf CVUSA combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations.}
  \includegraphics*[width=0.9\linewidth]{fig8.tiff.png}
  
  \label{fig8}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf CVACT combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations. }
  \includegraphics*[width=0.9\linewidth]{fig9.tiff.png}
  
  \label{fig9}
\end{figure}

Fig\ref{fig10} and Fig~{fig11} show the results of hard-voting with the most accurate model prediction strategy for CVUSA and CVACT, respectively. The accuracy drops about 2\% for the r@1 and r@5 metrics. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. Fig~\ref{fig12} and Fig~\ref{fig13} show the results of the hard-voting with the random selection strategy for CVUSA and CVACT, respectively. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.


\begin{figure}[!ht]
  \caption{{\bf CVUSA combinations for hard-voting with the most accurate model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \includegraphics*[width=0.9\linewidth]{fig10.tiff.png}
  \label{fig10}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf CVACT combinations for hard-voting with the most accurate  model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \includegraphics*[width=\textwidth,height=0.8\textheight,keepaspectratio]{fig11.tiff.png}
  \label{fig11}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf CVUSA combinations for hard-voting with the random selection strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. }
  \includegraphics*[width=0.9\linewidth]{fig12.tiff.png}
  \label{fig12}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf CVACT combinations for hard-voting with the random selection strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \includegraphics*[width=\textwidth,height=0.8\textheight,keepaspectratio]{fig13.tiff.png}
  \label{fig13}
\end{figure}

\FloatBarrier

Fig~\ref{fig14} illustrates that soft voting performs the best. Although soft voting results with joint probability and averaging are identical, the computational cost of averaging strategy is cheaper. Hard-voting strategies have constant computational complexity. Soft-voting improves accuracy by looking at the {\bf top-k predictions} across different models. For the sake of illustration,  Fig~\ref{fig15} and Fig~\ref{fig16} show an example of the predictions of the individual models. None of the models returned the right prediction as the first prediction, but it was in the top-5 predictions for most models. Their collective prediction (the model ensemble) could return it as the first prediction.

\begin{figure}[!ht]
  \caption{{\bf Comparison between aggregation method for the best performing combinations. } {\bf A}: CVUSA and {\bf B}: CVACT.  The dark bar is the best performing aggregation method. Soft voting outperforms other methods across all r@k metrics for both datasets.}
  \includegraphics*[width=0.9\linewidth]{fig14.tiff.png}
  
  \label{fig14}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf An example that shows the ensemble model r@(1 - 5) compared to the individual models on the CVUSA dataset.} The true satellite has a red border.}
  \includegraphics*[width=0.9\linewidth]{fig15.tiff.png}
  \label{fig15}
\end{figure}

\begin{figure}[!ht]
  \caption{{\bf An example that shows the ensemble model r@(1 - 5) compared to the individual models on the CVACT dataset.}  The true satellite has a red border.}
  \includegraphics*[width=0.9\linewidth]{fig16.tiff.png}
  \label{fig16}
\end{figure}

\FloatBarrier

\subsection*{B) EgoTR fine-tuning}

The training process took $192$ hours for $228$ epochs. Table~\ref{table6} shows the r@k metrics for the model. This drop in accuracy can be attributed to: {\bf A)} the ground images in our dataset aren't panoramic, in contrast to CVUSA and CVACT. {\bf B)} high similarity between the consecutive pairs. {\bf C)} one of the shortcomings of the r@k metric is that it depends on the size of the validation dataset as shown in  Figure~\ref{fig17}, our validation dataset size is more than double the size of CVUSA or the size of CVACT.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf r@k metrics of EgoTR fine-tuned over the reshaped BDD-trajectories dataset.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}ccccccc@{}}
    \toprule
    & r@{\bf 1} & r@{\bf 5} & r@{\bf 10} & r@{\bf 1\%} &\\
    & \multicolumn{4}{c}{(\%)} \\ \midrule
    & 9.99 & 16.72 & 21.95 & 66.58 & \\
    \bottomrule
    \end{tabular}
  }
  \label{table6}
\end{table}

\begin{figure}[!ht]
  \caption{{\bf The effect of the size of the validation dataset on the r@k metric.}  Same model \emph{(EgoTR)} with the same dataset \emph{(BDD-trajectories)}. The accuracy decreases as the size of the validation dataset increases.}
  \includegraphics*[width=0.9\linewidth]{fig17.tiff.png}
  \label{fig17}
\end{figure}

\FloatBarrier

\subsection*{C) Plain naive history}
Our experiments, Fig~\ref{fig18}, show that the more we look back on the history of the journey. And the more accuracy improves. The algorithm has no assumptions about the current state, and its computational cost is negligible compared to generating the distance array.

\begin{figure}[!ht]
  \caption{{\bf The effect of the number of steps we look back on the accuracy.} The accuracy converges to ~100\% after seven steps on our dataset.}
  \includegraphics*[width=0.9\linewidth]{fig18.tiff.png}
  \label{fig18}
\end{figure}
\FloatBarrier

\subsection*{D) Naive history with weak prior}
Fig~\ref{fig19} shows that naive history with weak prior with 2 steps lookback outperforms plain naive history with 5 steps lookback. And it only takes 3 steps for naive history with a prior to converge to 100\%, compared to 7 steps (Fig~\ref{fig18}) for the plain version. 
However promising this is, if the prior is wrong it can result in “trapping” the algorithm in the wrong state which will degrade the accuracy significantly, and this is the “naive” part of the naive history algorithm.

\begin{figure}[!ht]
  \caption{{\bf The effect of weak prior on naive history.}  The accuracy of naive history improves significantly when starting with some prior knowledge about the trip starting point. The brown line(naive history with prior) and the green line (plain naive history) represent the same number of steps into the trip though the brown line shows more accurate predictions due to factoring in the weak prior.}
  \includegraphics*[width=0.9\linewidth]{fig19.tiff.png}
  \label{fig19}
\end{figure}

\FloatBarrier

\section*{Conclusion and future work}
In this work, we streamlined using BDD100K so that we can build an end-to-end model that exploits the temporal correlation during a single trip. But there are no similar datasets with other data modalities which can be the solution for areas with poor lighting conditions. Also, we can see there is a room for analyzing different state-of-art models to identify the most promising building modules and then use the network architecture search (NAS) paradigm to develop an optimal CV matching network. We anticipate that this study will kick-start the development of deployable cross-view geo-localization models, exploring fusing other data modalities and sources during querying and training. Moreover we believe there is a great gap for real-time, weatherproof models that can initiate many research points.

\nolinenumbers

\begin{thebibliography}{10}

  \bibitem{bib1}
  Ben-Moshe B, Elkin E, Levi H, Weissman A.
  \newblock Improving Accuracy of GNSS Devices in Urban Canyons.
  \newblock In: CCCG; 2011. p. 511--515.
  
  \bibitem{bib2}
  Zhai M, Bessinger Z, Workman S, Jacobs N.
  \newblock Predicting ground-level scene layout from aerial imagery.
  \newblock In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition; 2017. p. 867--875.
  
  \bibitem{bib3}
  Wang T, Zheng Z, Yan C, Zhang J, Sun Y, Zheng B, et~al.
  \newblock Each part matters: Local patterns facilitate cross-view
    geo-localization.
  \newblock IEEE Transactions on Circuits and Systems for Video Technology.
    2021;32(2):867--879.
  
  \bibitem{bib25}
  Zeng W, Wu M, Sun W, Xie S.
  \newblock Comprehensive review of autonomous taxi dispatching systems.
  \newblock Comput Sci. 2020;47(05):181--189.
  
  \bibitem{bib37}
  Vo NN, Hays J.
  \newblock Localizing and orienting street views using overhead imagery.
  \newblock In: European conference on computer vision. Springer; 2016. p.
    494--509.
  
  \bibitem{bib6}
  Churchill W, Newman P.
  \newblock Experience-based navigation for long-term localisation.
  \newblock The International Journal of Robotics Research.
    2013;32(14):1645--1661.
  
  \bibitem{bib7}
  Doan AD, Latif Y, Chin TJ, Liu Y, Ch’ng SF, Do TT, et~al.
  \newblock Visual localization under appearance change: filtering approaches.
  \newblock Neural Computing and Applications. 2021;33(13):7325--7338.
  
  \bibitem{bib8}
  Rodrigues R, Tani M.
  \newblock Are these from the same place? seeing the unseen in cross-view image
    geo-localization.
  \newblock In: Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision; 2021. p. 3753--3761.
  
  \bibitem{bib9}
  Doan AD, Latif Y, Chin TJ, Liu Y, Do TT, Reid I.
  \newblock Scalable place recognition under appearance change for autonomous
    driving.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 9319--9328.
  
  \bibitem{bib10}
  Milford MJ, Wyeth GF.
  \newblock SeqSLAM: Visual route-based navigation for sunny summer days and
    stormy winter nights.
  \newblock In: 2012 IEEE international conference on robotics and automation.
    IEEE; 2012. p. 1643--1649.
  
  \bibitem{bib4}
  Liu L, Li H.
  \newblock Lending orientation to neural networks for cross-view
    geo-localization.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2019. p. 5624--5633.
  
  \bibitem{bib20}
  Regmi K.
  \newblock Exploring Relationships Between Ground and Aerial Views by Synthesis
    and Matching. 2021;.
  
  \bibitem{bib21}
  Hu S, Lee GH.
  \newblock Image-based geo-localization using satellite imagery.
  \newblock International Journal of Computer Vision. 2020;128(5):1205--1219.
  
  \bibitem{bib22}
  Dixit D, Verma S, Tokekar P.
  \newblock Evaluation of Cross-View Matching to Improve Ground Vehicle
    Localization with Aerial Perception.
  \newblock arXiv preprint arXiv:200306515. 2020;.
  
  \bibitem{bib24}
  Regmi K, Shah M.
  \newblock Video geo-localization employing geo-temporal feature learning and
    gps trajectory smoothing.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2021. p. 12126--12135.
  
  \bibitem{bib55}
  Workman S, Souvenir R, Jacobs N.
  \newblock Wide-area image geolocalization with aerial reference imagery.
  \newblock In: Proceedings of the IEEE International Conference on Computer
    Vision; 2015. p. 3961--3969.
  
  \bibitem{bib5}
  Zhu S, Yang T, Chen C.
  \newblock Vigor: Cross-view image geo-localization beyond one-to-one retrieval.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2021. p. 3640--3649.
  
  \bibitem{bib12}
  Yu F, Chen H, Wang X, Xian W, Chen Y, Liu F, et~al.
  \newblock Bdd100k: A diverse driving dataset for heterogeneous multitask
    learning.
  \newblock In: Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition; 2020. p. 2636--2645.
  
  \bibitem{bib26}
  Zemene E, Tesfaye YT, Idrees H, Prati A, Pelillo M, Shah M.
  \newblock Large-scale image geo-localization using dominant sets.
  \newblock IEEE transactions on pattern analysis and machine intelligence.
    2018;41(1):148--161.
  
  \bibitem{bib32}
  Tian Y, Chen C, Shah M.
  \newblock Cross-view image matching for geo-localization in urban environments.
  \newblock In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition; 2017. p. 3608--3616.
  
  \bibitem{bib36}
  Hu S, Feng M, Nguyen RM, Lee GH.
  \newblock Cvm-net: Cross-view matching network for image-based ground-to-aerial
    geo-localization.
  \newblock In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition; 2018. p. 7258--7267.
  
  \bibitem{bib14}
  Wang J, Yang Y, Pan M, Zhang M, Zhu M, Fu M.
  \newblock Hybrid Perspective Mapping: Align Method for Cross-View Image-Based
    Geo-Localization.
  \newblock In: 2021 IEEE International Intelligent Transportation Systems
    Conference (ITSC). IEEE; 2021. p. 3040--3046.
  
  \bibitem{bib13}
  Shi Y, Yu X, Campbell D, Li H.
  \newblock Where am i looking at? joint location and orientation estimation by
    cross-view matching.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2020. p. 4064--4072.
  
  \bibitem{bib41}
  Samano N, Zhou M, Calway A.
  \newblock You are here: Geolocation by embedding maps and images.
  \newblock In: European Conference on Computer Vision. Springer; 2020. p.
    502--518.
  
  \bibitem{bib44}
  Shi Y, Yu X, Liu L, Zhang T, Li H.
  \newblock Optimal feature transport for cross-view image geo-localization.
  \newblock In: Proceedings of the AAAI Conference on Artificial Intelligence.
    vol.~34; 2020. p. 11990--11997.
  
  \bibitem{bib43}
  Zhu S, Yang T, Chen C.
  \newblock Revisiting street-to-aerial view image geo-localization and
    orientation estimation.
  \newblock In: Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision; 2021. p. 756--765.
  
  \bibitem{bib45}
  Cai S, Guo Y, Khan S, Hu J, Wen G.
  \newblock Ground-to-aerial image geo-localization with a hard exemplar
    reweighting triplet loss.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 8391--8400.
  
  \bibitem{bib46}
  Shi Y, Liu L, Yu X, Li H.
  \newblock Spatial-aware feature aggregation for image based cross-view
    geo-localization.
  \newblock Advances in Neural Information Processing Systems. 2019;32.
  
  \bibitem{bib47}
  Xia Z, Booij O, Manfredi M, Kooij JF.
  \newblock Geographically local representation learning with a spatial prior for
    visual localization.
  \newblock In: European Conference on Computer Vision. Springer; 2020. p.
    557--573.
  
  \bibitem{bib17}
  Xia Z, Booij O, Manfredi M, Kooij JF.
  \newblock Cross-View Matching for Vehicle Localization by Learning
    Geographically Local Representations.
  \newblock IEEE Robotics and Automation Letters. 2021;6(3):5921--5928.
  
  \bibitem{bib18}
  Toker A, Zhou Q, Maximov M, Leal-Taix{\'e} L.
  \newblock Coming down to earth: Satellite-to-street view synthesis for
    geo-localization.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2021. p. 6488--6497.
  
  \bibitem{bib19}
  Yang H, Lu X, Zhu Y.
  \newblock Cross-view geo-localization with evolving transformer.
  \newblock arXiv preprint arXiv:210700842. 2021;.
  
  \bibitem{bib49}
  Li S, Tu Z, Chen Y, Yu T.
  \newblock Multi-scale attention encoder for street-to-aerial image
    geo-localization.
  \newblock CAAI Transactions on Intelligence Technology. 2022;.
  
  \bibitem{bib50}
  Regmi K, Shah M.
  \newblock Bridging the domain gap for ground-to-aerial image matching.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 470--479.
  
  \bibitem{bib27}
  Lowe DG.
  \newblock Distinctive image features from scale-invariant keypoints.
  \newblock International journal of computer vision. 2004;60(2):91--110.
  
  \bibitem{bib28}
  Bay H, Tuytelaars T, Gool LV.
  \newblock Surf: Speeded up robust features.
  \newblock In: European conference on computer vision. Springer; 2006. p.
    404--417.
  
  \bibitem{bib29}
  Alahi A, Ortiz R, Vandergheynst P.
  \newblock Freak: Fast retina keypoint.
  \newblock In: 2012 IEEE conference on computer vision and pattern recognition.
    Ieee; 2012. p. 510--517.
  
  \bibitem{bib30}
  Bosch A, Zisserman A, Munoz X.
  \newblock Image Classification using Random Forests and Ferns.
  \newblock In: 2007 IEEE 11th International Conference on Computer Vision; 2007.
    p. 1--8.
  
  \bibitem{bib31}
  Simonyan K, Zisserman A.
  \newblock Very deep convolutional networks for large-scale image recognition.
  \newblock arXiv preprint arXiv:14091556. 2014;.
  
  \bibitem{bib33}
  Ren S, He K, Girshick R, Sun J.
  \newblock Faster r-cnn: Towards real-time object detection with region proposal
    networks.
  \newblock Advances in neural information processing systems. 2015;28.
  
  \bibitem{bib34}
  Krizhevsky A, Sutskever I, Hinton GE.
  \newblock Imagenet classification with deep convolutional neural networks.
  \newblock Communications of the ACM. 2017;60(6):84--90.
  
  \bibitem{bib35}
  Chopra S, Hadsell R, LeCun Y.
  \newblock Learning a similarity metric discriminatively, with application to
    face verification.
  \newblock In: 2005 IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition (CVPR'05). vol.~1. IEEE; 2005. p. 539--546.
  
  \bibitem{bib38}
  He K, Zhang X, Ren S, Sun J.
  \newblock Deep residual learning for image recognition.
  \newblock In: Proceedings of the IEEE conference on computer vision and pattern
    recognition; 2016. p. 770--778.
  
  \bibitem{bib39}
  Huang G, Liu Z, Van Der~Maaten L, Weinberger KQ.
  \newblock Densely connected convolutional networks.
  \newblock In: Proceedings of the IEEE conference on computer vision and pattern
    recognition; 2017. p. 4700--4708.
  
  \bibitem{bib40}
  Chollet F.
  \newblock Xception: Deep learning with depthwise separable convolutions.
  \newblock In: Proceedings of the IEEE conference on computer vision and pattern
    recognition; 2017. p. 1251--1258.
  
  \bibitem{bib42}
  Ronneberger O, Fischer P, Brox T.
  \newblock U-net: Convolutional networks for biomedical image segmentation.
  \newblock In: International Conference on Medical image computing and
    computer-assisted intervention. Springer; 2015. p. 234--241.
  
  \bibitem{bib48}
  Woo S, Park J, Lee JY, Kweon IS.
  \newblock Cbam: Convolutional block attention module.
  \newblock In: Proceedings of the European conference on computer vision (ECCV);
    2018. p. 3--19.
  
  \bibitem{bib51}
  Regmi K, Borji A.
  \newblock Cross-view image synthesis using conditional gans.
  \newblock In: Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition; 2018. p. 3501--3510.
  
  \bibitem{bib52}
  Canny J.
  \newblock A computational approach to edge detection.
  \newblock IEEE Transactions on pattern analysis and machine intelligence.
    1986;(6):679--698.
  
  \bibitem{bib54}
  Doan AD, Latif Y, Chin TJ, Liu Y, Do TT, Reid I.
  \newblock Scalable place recognition under appearance change for autonomous
    driving.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 9319--9328.
  
  \bibitem{bib23}
  FFmpeg.org;.
  \newblock Available from: \url{https://ffmpeg.org/}.
  
  \bibitem{bib16}
  {Wikipedia contributors}. Great-circle distance; 2022.
  \newblock Available from:
    \url{https://en.wikipedia.org/wiki/Great-circle_distance}.
  
  \bibitem{bib15}
  Maps Static API;.
  \newblock Available from:
    \url{https://developers.google.com/maps/documentation/maps-static/overview}.
  
  \bibitem{bib11}
  Alazzawi A.
  \newblock EDGE DETECTION-APPLICATION OF (FIRST AND SECOND) ORDER DERIVATIVE IN
    IMAGE PROCESSING: Communication.
  \newblock Diyala Journal of Engineering Sciences. 2015;8(4):430--440.
\end{thebibliography}
\end{document}
