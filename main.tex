% Template for PLoS
% Version 3.6 Aug 2022
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage{booktabs, makecell}

\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Leveraging cross-view geo-localization with ensemble learning and temporal awareness} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Abdulrahman Ghanem\textsuperscript{1},
Ahmed Abdelhay\textsuperscript{1},
Noor Eldeen Salah\textsuperscript{1},
Ahmed Nour Eldeen1\textsuperscript{1},
Mohammed Elhenawy\textsuperscript{2},
Abdallah A. Hassan\textsuperscript{1},
Ammar M. Hassan\textsuperscript{3*},
Mahmoud Masoud\textsuperscript{2}
\\
\bigskip
\textbf{1} Computer and Systems Engineering Department, Faculty of Engineering, Minia University, Minia, Egypt
\\
\textbf{2}  Centre for Accident Research and Road Safety-Queensland (CARRS-Q), Queensland University of Technology, Brisbane, Australia
\\
\textbf{3} Arab Academy for Science, Technology, and Maritime Transport, South Valley Branch, Aswan, Egypt
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% % Additional Equal Contribution Note
% % Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% % Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% % \textcurrency b Insert second current address 
% % \textcurrency c Insert third current address

% % Deceased author note
% \dag Deceased

% % Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* ammar@aast.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
In some situations the Global Navigation Satellite System (GNSS) is unreliable. To mend the poor GNSS signal, an autonomous vehicle can self-localize by matching a ground image against a database of geotagged aerial images. It is easy to construct an aerial dataset, so this approach is preferable to other image-based localization techniques. But, this approach is challenging: there are dramatic viewpoint changes between aerial and ground views, harsh weather and lighting conditions, and lack of orientation information, in training and deployment environments. In this paper, we show that previous models in this area are complementary, not competitive, and that each model solves a different aspect of the problem. We propose an ensemble model to aggregate the predictions of multiple independently trained state-of-the-art models. Also, we show the effect of making the query process temporal-aware  But none of the existing benchmark datasets is suitable for extensive temporal awareness experiments, we generated a new derivative dataset based on the BDD100K dataset. This paper also proposes an efficient temporal awareness mechanism: Naive History, to prove that fusing the temporal information of the trip with the model prediction significantly improves the accuracy. Our ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset (surpassing the current state-of-the-art). Our temporal awareness mechanism converges to R@1 of ~100\% by looking at a few steps back in the trip history. We expect this study to initialize the building of deployable ensemble cross-view geo-localization models that take advantage of temporal proximity between ground images.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
% \section*{Author summary}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
The current standard localization technique is the global navigation satellite system (GNSS). Although the GNSS accuracy declines in cases where there are few lines of sight (e.g., urban canyons~\cite{bib1}). Using cross-view geo-localization, a vehicle localizes itself by matching street view images against a database of geotagged images captured from aerial platforms (e.g., a satellite~\cite{bib2} or a drone~\cite{bib3}). Cross-view geo-localization is gaining popularity in the scene of autonomous vehicles~\cite{bib2} and robotic navigation~\cite{bib25}: it compensates for a bad GNSS signal-to-noise ratio. And,  it’s preferable to other image-based localization techniques (e.g., landmark and ground-to-ground matching) for the ease of covering new areas. Early work~\cite{bib37} on this technique claimed that its main challenge is the lack of visual correspondence between aerial and ground views. Later work, while holding the same claim, showed that there are more challenges: geographic scene changes over time~\cite{bib6,bib7,bib8,bib9,bib10}, poor weather and lighting conditions, lack of orientation information~\cite{bib4}, and misalignment between aerial and ground images during training.
None of the previous models tried to collectively address these five challenges. To build a holistic solution for the problem, we propose an ensemble model to fuse the predictions of five independently trained models. Each of these models addresses a different challenge..
Moreover, most of the recent work treats the problem as a 1-to-1 image-matching task. This overlooks the fact that these models get deployed in environments where there's a continuous stream of input, not a single query image. Taking the temporal nature of the problem into account is a must:  our experiments showed that the recent models can't differentiate between highly similar query images, and in the case of a moving vehicle, the consecutive images have a high degree of similarity. Some recent models considered the temporal nature of the problem.~\cite{bib7}  and~\cite{bib20,bib21,bib22} used Markov chain Monte Carlo (MCMC) algorithms to predict the current pose and enforce temporal consistency.~\cite{bib24} enforces temporal consistency by using a transformer-based trajectory smoothing network. We can see that these methods are resource intensive or have strong assumptions about the current state of the vehicle. We explore an efficient technique to achieve the same goal.
Although the existing datasets aren't suitable for conducting experiments to prove that temporal awareness improves accuracy. The dataset needs to be realistic and collected as a trajectory of close points over a long-running journey to resemble driving in a real environment.  CVUSA~\cite{bib55} and CVACT~\cite{bib4} include sparse points on the map. VIGOR~\cite{bib5} includes dense points but doesn't form trajectories. RobotCar~\cite{bib6} has a limited number of examples. So we had to introduce a new derivative dataset based on the BDD100K~\cite{bib12} dataset to fill this gap.

\subsection*{In summary, the contributions of this paper are:}
\begin{itemize}
    \item We introduce a new ensemble model based on five of the current state-of-the-art  cross-view matching networks. The ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset.
    \item We construct a new derivative dataset that is suitable for temporal aware cross-view geo-localization models based on BDD100K.
    \item We develope a meta block: Naive History, to make the query process temporal aware. We show that taking journey history into account minimizes the search space. This reduction in search space improves the accuracy.  The accuracy converges to ~100\% with a three-step lookback into trip history on our proposed dataset. This meta block is usable with any CV model.
\end{itemize}

The rest of the paper is organized as follows. Section II presents the related work. Section III discusses the details of the methodology and the experimental work. We conclude the paper in Section IV.

\section*{Related work}
In section (A), we start by investigating how different models engineered their features and architectures. Their choices show us how different models tried to approach the problem from different angles. Followed by giving a bird-eye-view of the architectures of these models and feature extraction methods. In section (B), we walk through the models that tried blending the trip history into the image-matching task. By grouping the models into two categories: one that relies on "this place looks familiar" in section (B.1), and one that relies on "where have we been before getting here?" in section (B.2).
\subsection*{A) Features and architectures}
Most of the recent work treated the cross-view geo-localization task as an image retrieval task. They tried to find a feature representation suitable for matching query ground images and aerial ground images. The nature and complexity of used networks changed over time. Here, we conduct a brief comparison between the most common feature representations and their architectures. Table~\ref{table1} summarizes the feature types and the backbones of different cross-view geo-localization networks.


\begin{table}[!ht]
  \centering
  \caption{
  {\bf A summary of the feature types and the backbones of different cross-view geo-localization networks.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Feature type} & {\bf Backbone} & {\bf Used in}&  \\ \midrule
    & Hand-crafted & SIFT & ~\cite{bib20}~\cite{bib26}& \\
    & Hand-crafted & SURF, FREAK, PHOW & ~\cite{bib20}& \\
    & Hand-crafted & SIFT + VLAD & ~\cite{bib7}& \\
    & Semantic & Faster R-CNN & ~\cite{bib32}& \\
    & CNN & VGG + FCN + NetVLAD & ~\cite{bib36}~\cite{bib22}~\cite{bib14}& \\
    & CNN & AlexNet & ~\cite{bib37}~\cite{bib32}& \\
    & CNN & VGG & ~\cite{bib21}~\cite{bib4}~\cite{bib13}& \\
    & CNN & ResNet & ~\cite{bib4}~\cite{bib21}~\cite{bib41}& \\
    & CNN & DenseNet & ~\cite{bib4}~\cite{bib21}& \\
    & CNN & U-net & ~\cite{bib4}& \\
    & CNN & Xception & ~\cite{bib21}& \\
    & CNN & - & ~\cite{bib44}& \\
    & CNN & VGG + FCN & ~\cite{bib43}& \\
    & Attentive & Siam-FCANet (ResNet + FCAM + FCN) & ~\cite{bib45}& \\
    & Attentive & Siam-VFCNet (ResNet + FCAM + NetVLAD) & ~\cite{bib45}& \\
    & Attentive & VGG + SAFA + SPE & ~\cite{bib46}~\cite{bib47}~\cite{bib17}& \\
    & Attentive & VGG + Geo Attention + Geo-temporal Attention & ~\cite{bib24}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib8}& \\
    & Attentive & SAFA & ~\cite{bib5}& \\
    & Attentive & ResNet + SAFA & ~\cite{bib18}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib19}& \\
    & Attentive & VGG + MSAE & ~\cite{bib49}& \\
    & Synthesized & X-Fork & ~\cite{bib50}& \\ \bottomrule
    \end{tabular}
  }
  \label{table1}
\end{table}

\subsubsection*{1) Hand-crafted features}
Early research used hand-crafted features.~\cite{bib20,bib26} used SIFT~\cite{bib27}. Also,~\cite{bib20} experimented with other feature spaces (e.g., SURF~\cite{bib28}, FREAK~\cite{bib29}, PHOW~\cite{bib30}) but SIFT outperformed others.~\cite{bib7} computed dense SIFT features, then embedded them into a higher dimensional vector space using a VLAD~\cite{bib31}. The extracted features are brittle; it fails to adapt to appearance change. Later, it proved to have inferior performance compared to the CNN-based features.
\subsubsection*{2) Semantic features}
In this approach, the networks matched between ground and aerial images based on the meaningful content of the image. This made it more robust to viewpoint changes than local features. Though, the model performance degraded in areas lacking the pre-selected semantic features.~\cite{bib32} treated the problem as object detection and recognition: the first block of the architecture employed the Faster R-CNN~\cite{bib33} to detect buildings, and the second block of the architecture used AlexNet~\cite{bib34} with the Siamese architecture~\cite{bib35} to recognize the buildings.
\subsubsection*{3) CNN-based features}
Metric learning achieved promising results in bridging the domain gap between aerial and ground image representation.~\cite{bib36} used a fully convolutional network (FCN) with a NetVLAD~\cite{bib34} layer using a Siamese architecture.~\cite{bib37} tried modified versions of AlexNet.~\cite{bib21} experimented with different FCN layers: VGG~\cite{bib31}, ResNet~\cite{bib38}, DenseNet~\cite{bib39}, and Xception~\cite{bib40} with the same architecture of~\cite{bib36}, they found that VGG outperforms other networks. Also,~\cite{bib22} used the CVM-Net-I architecture proposed in~\cite{bib36}.~\cite{bib41} exploited a modified ResNet50 network for ground images and a ResNet18 for aerial ones.~\cite{bib13} used VGG16 to generate the feature maps for the polar transformed aerial and then fed it into the Dynamic Similarity Module (DSM).~\cite{bib4} learned orientation information by using different backbones (VGG, ResNet, DenseNet, and U-net~\cite{bib42}) with the Siamese architecture.~\cite{bib43} employed  the Siamese network with a VGG backbone to extract feature maps, then a fully connected layer aggregates these feature maps.~\cite{bib44} used a CNN to generate feature maps and then transform them from the ground domain to the aerial domain.~\cite{bib14} applied the hybrid perspective mapping method using the CVM-Net-I architecture.
\subsubsection*{4) Attentive features}
For this feature type, the networks used spatial attention to enhance the feature representation.~\cite{bib45} integrated the lightweight attention module (FCAM) into each block of the basic ResNet.~\cite{bib46} used a spatial-aware feature aggregation (SAFA) module to mitigate the distortion in the aerial image. Also, employed the spatial-aware position embedding module (SPE) to encode relative positions among features in the feature maps.~\cite{bib5} proposed the VIGOR network built on top of SAFA.~\cite{bib47,bib17} used the same architecture as~\cite{bib46} with a different loss function: geo-distance weighted loss.~\cite{bib24} proposed a geo-attention module for the aerial branch, and a temporal-attention module for the ground branch.~\cite{bib8} used convolutional block attention modules~\cite{bib48} to generate multi-scale attention features.~\cite{bib18} employed a modified ResNet34 backbone with a spatial-aware attention module.~\cite{bib19} proposed EgoTR network. EgoTR used a ResNet backbone transformer encoder with a self-cross attention mechanism.~\cite{bib49} introduced the Multi-Scale Attention Encoder (MSAE). MSAE employed a VGG backbone with a multi-scale attention encoder followed by FCN to generate feature masks.

\subsubsection*{5) Synthetic features}
In this approach, the networks learned robust feature representation by reversing the task: it learned how to create ground view from aerial views, which made it learn salient features and suppress others.~\cite{bib50} synthesized aerial representation of a ground panorama query using the X-Fork network~\cite{bib51} with edge maps detection by Canny Edge Detection~\cite{bib52}.

\subsection*{B) Contextual awareness}
The fact that these models get deployed in autonomous vehicles, makes it obvious that the models should be aware of the trip context. We can categorize contextual awareness as follows:

\subsubsection*{1) Spatial awareness}
We have to differentiate between two types of spatial awareness.

\begin{itemize}
  \item Some models refer to it as the knowledge about the pose (location and orientation) of the query image and its relative pose to the aerial image frame.
  \cite{bib47,bib17} constructed mini-batches of images within a certain geographic radius and used a modified version of the triplet loss function: Geo-distance weighted loss to favor examples where the images are within the selected radius. The Geo-Attention module used in~\cite{bib24} exploits a similar loss function.~\cite{bib14} employed hybrid perspective mapping to establish correspondence between ground and aerial images.~\cite{bib4} injected the orientation information into the network.~\cite{bib4} used multiplane image (MPI)~\cite{bib13} projections to exploit geometric correspondence between ground and aerial images. Table~\ref{table2} gives an overview of spatial (pose-wise) awareness approaches used in cross-view geo-localization.
  \begin{table}[!ht]
    \centering
    \caption{
    {\bf An overview of the spatial (pose-wise) awareness approaches used in cross-view geo-localization.}}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Geographic proximity & ~\cite{bib47}~\cite{bib17}~\cite{bib24}& \\
    & Polar Transform & ~\cite{bib3,bib4}~\cite{bib5}~\cite{bib13,bib14}~\cite{bib17,bib18,bib19}& \\
    & Inverse Polar Transform & ~\cite{bib49}& \\
    & Orientation & ~\cite{bib4}& \\
    & Dynamic Similarity Matching (DSM) & ~\cite{bib13}& \\
    & Hybrid Perspective Mapping & ~\cite{bib14}& \\ \bottomrule
    \end{tabular}
    \label{table2}
  \end{table}
  \item Other networks refer to it as paying more attention to the salient features, suppressing less important features, and encoding the spatial layout information into the feature representation. We have covered that in section (A.4).
\end{itemize}

\subsubsection*{2) Temporal awareness}
There are two types of temporal awareness too!
\begin{itemize}
  \item Finding a robust feature representation that won’t be affected by scene changes throughout time. These changes can be geographic landmarks (e.g., new constructions) or environmental conditions such as weather conditions.
  ~\cite{bib6,bib54} used time-invariant approaches by capturing the same scene during different conditions across time. But, this technique required a dataset where the same scene is covered during different conditions. Possible solutions for this are as follows. {\bf A)}~\cite{bib8} used semantic object-based data augmentation techniques to remove and add objects (cars, roads, greenery, and sky). {\bf B)}~\cite{bib7} applied PCA projection to make background features less significant in the image descriptor. {\bf C)} Another solution is using synthetic datasets.
  \item Exploiting the fact these models will be deployed on vehicles where a temporally coherent sequence of images is available.
  ~\cite{bib7} and~\cite{bib20,bib21,bib22} used MCMC algorithms, namely, a particle filter~\cite{bib45} with variations of initialization and transition techniques, the algorithms are used to predict current pose and enforce temporal consistency.~\cite{bib24} introduced a geo-temporal attention module, the module attends to all frames in the video to learn better features, it also enforces temporal consistency by using a transformer-based trajectory smoothing network.
\end{itemize}
We give an overview of the temporal awareness approaches used in cross-view geo-localization in Table~\ref{table3}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf An overview of the temporal awareness approaches used in cross-view geo-localization.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Capture multiple examples with different environmental conditions & ~\cite{bib6}~\cite{bib54}& \\
    & Semantic object-based data augmentation & ~\cite{bib8}& \\
    & Observation encoder + PCA projection & ~\cite{bib7}& \\
    & MCMC algorithms & ~\cite{bib7}~\cite{bib20,bib21,bib22}& \\
    & Sequence attention + Trajectory Smoothing Network & ~\cite{bib24}& \\ \bottomrule
    \end{tabular}
  }
  \label{table3}
\end{table}
  


% For figure citations, please use "Fig" instead of "Figure".
Nulla mi mi, Fig~\ref{fig1} venenatis sed ipsum varius, volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, \nameref{S1_Video} vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero.

% Place figure captions after the first paragraph in which they are cited.
\begin{figure}[!h]
\caption{{\bf Bold the figure title.}
Figure caption text here, please use this space for the figure panel descriptions instead of using subfigure commands. A: Lorem ipsum dolor sit amet. B: Consectetur adipiscing elit.}
\label{fig1}
\end{figure}

% Results and Discussion can be combined.
\section*{Results}
Nulla mi mi, venenatis sed ipsum varius, Table~\ref{table4} volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero.

% Place tables after the first paragraph in which they are cited.
\begin{table}[!ht]
\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
\centering
\caption{
{\bf Table caption Nulla mi mi, venenatis sed ipsum varius, volutpat euismod diam.}}
\begin{tabular}{|l+l|l|l|l|l|l|l|}
\hline
\multicolumn{4}{|l|}{\bf Heading1} & \multicolumn{4}{|l|}{\bf Heading2}\\ \thickhline
$cell1 row1$ & cell2 row 1 & cell3 row 1 & cell4 row 1 & cell5 row 1 & cell6 row 1 & cell7 row 1 & cell8 row 1\\ \hline
$cell1 row2$ & cell2 row 2 & cell3 row 2 & cell4 row 2 & cell5 row 2 & cell6 row 2 & cell7 row 2 & cell8 row 2\\ \hline
$cell1 row3$ & cell2 row 3 & cell3 row 3 & cell4 row 3 & cell5 row 3 & cell6 row 3 & cell7 row 3 & cell8 row 3\\ \hline
\end{tabular}
\begin{flushleft} Table notes Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed.
\end{flushleft}
\label{table4}
\end{adjustwidth}
\end{table}


%PLOS does not support heading levels beyond the 3rd (no 4th level headings).
\subsection*{\lorem\ and \ipsum\ nunc blandit a tortor}
\subsubsection*{3rd level heading} 
Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque. Quisque augue sem, tincidunt sit amet feugiat eget, ullamcorper sed velit. Sed non aliquet felis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris commodo justo ac dui pretium imperdiet. Sed suscipit iaculis mi at feugiat. 

\begin{enumerate}
	\item{react}
	\item{diffuse free particles}
	\item{increment time by dt and go to 1}
\end{enumerate}


\subsection*{Sed ac quam id nisi malesuada congue}

Nulla mi mi, venenatis sed ipsum varius, volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero.

\begin{itemize}
	\item First bulleted item.
	\item Second bulleted item.
	\item Third bulleted item.
\end{itemize}

\section*{Discussion}
Nulla mi mi, venenatis sed ipsum varius, Table~\ref{table1} volutpat euismod diam. Proin rutrum vel massa non gravida. Quisque tempor sem et dignissim rutrum. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi at justo vitae nulla elementum commodo eu id massa. In vitae diam ac augue semper tincidunt eu ut eros. Fusce fringilla erat porttitor lectus cursus, vel sagittis arcu lobortis. Aliquam in enim semper, aliquam massa id, cursus neque. Praesent faucibus semper libero~\cite{bib3}.

\section*{Conclusion}

CO\textsubscript{2} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque. Quisque augue sem, tincidunt sit amet feugiat eget, ullamcorper sed velit. 

Sed non aliquet felis. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Mauris commodo justo ac dui pretium imperdiet. Sed suscipit iaculis mi at feugiat. Ut neque ipsum, luctus id lacus ut, laoreet scelerisque urna. Phasellus venenatis, tortor nec vestibulum mattis, massa tortor interdum felis, nec pellentesque metus tortor nec nisl. Ut ornare mauris tellus, vel dapibus arcu suscipit sed. Nam condimentum sem eget mollis euismod. Nullam dui urna, gravida venenatis dui et, tincidunt sodales ex. Nunc est dui, sodales sed mauris nec, auctor sagittis leo. Aliquam tincidunt, ex in facilisis elementum, libero lectus luctus est, non vulputate nisl augue at dolor. For more information, see \nameref{S1_Appendix}.

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Fig.}
\label{S1_Fig}
{\bf Bold the title sentence.} Add descriptive text after the title of the item (optional).

\paragraph*{S2 Fig.}
\label{S2_Fig}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 File.}
\label{S1_File}
{\bf Lorem ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Video.}
\label{S1_Video}
{\bf Lorem ipsum.}  Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Appendix.}
\label{S1_Appendix}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\paragraph*{S1 Table.}
\label{S1_Table}
{\bf Lorem ipsum.} Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.

\section*{Acknowledgments}
Cras egestas velit mauris, eu mollis turpis pellentesque sit amet. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam id pretium nisi. Sed ac quam id nisi malesuada congue. Sed interdum aliquet augue, at pellentesque quam rhoncus vitae.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\begin{thebibliography}{10}

\bibitem{bib1} 
ref
\newblock{info} 

\bibitem{bib2} 
ref
\newblock{info} 

\bibitem{bib3} 
ref
\newblock{info} 

\bibitem{bib4} 
ref
\newblock{info} 

\bibitem{bib5} 
ref
\newblock{info} 

\bibitem{bib6} 
ref
\newblock{info} 

\bibitem{bib7} 
ref
\newblock{info} 

\bibitem{bib8} 
ref
\newblock{info} 

\bibitem{bib9} 
ref
\newblock{info} 

\bibitem{bib10} 
ref
\newblock{info} 

\bibitem{bib11} 
ref
\newblock{info} 

\bibitem{bib12} 
ref
\newblock{info} 

\bibitem{bib13} 
ref
\newblock{info} 

\bibitem{bib14} 
ref
\newblock{info} 

\bibitem{bib15} 
ref
\newblock{info} 

\bibitem{bib16} 
ref
\newblock{info} 

\bibitem{bib17} 
ref
\newblock{info} 

\bibitem{bib18} 
ref
\newblock{info} 

\bibitem{bib19} 
ref
\newblock{info} 

\bibitem{bib20} 
ref
\newblock{info} 

\bibitem{bib21} 
ref
\newblock{info} 

\bibitem{bib22} 
ref
\newblock{info} 

\bibitem{bib23} 
ref
\newblock{info} 

\bibitem{bib24} 
ref
\newblock{info} 

\bibitem{bib25} 
ref
\newblock{info} 

\bibitem{bib26} 
ref
\newblock{info} 

\bibitem{bib27} 
ref
\newblock{info} 

\bibitem{bib28} 
ref
\newblock{info} 

\bibitem{bib29} 
ref
\newblock{info} 

\bibitem{bib30} 
ref
\newblock{info} 

\bibitem{bib31} 
ref
\newblock{info} 

\bibitem{bib32} 
ref
\newblock{info} 

\bibitem{bib33} 
ref
\newblock{info} 

\bibitem{bib34} 
ref
\newblock{info} 

\bibitem{bib35} 
ref
\newblock{info} 

\bibitem{bib36} 
ref
\newblock{info} 

\bibitem{bib37} 
ref
\newblock{info} 

\bibitem{bib38} 
ref
\newblock{info} 

\bibitem{bib39} 
ref
\newblock{info} 

\bibitem{bib40} 
ref
\newblock{info} 

\bibitem{bib41} 
ref
\newblock{info} 

\bibitem{bib42} 
ref
\newblock{info} 

\bibitem{bib43} 
ref
\newblock{info} 

\bibitem{bib44} 
ref
\newblock{info} 

\bibitem{bib45} 
ref
\newblock{info} 

\bibitem{bib46} 
ref
\newblock{info} 

\bibitem{bib47} 
ref
\newblock{info} 

\bibitem{bib48} 
ref
\newblock{info} 

\bibitem{bib49} 
ref
\newblock{info} 

\bibitem{bib50} 
ref
\newblock{info} 

\bibitem{bib51} 
ref
\newblock{info} 

\bibitem{bib52} 
ref
\newblock{info} 

\bibitem{bib53} 
ref
\newblock{info} 

\bibitem{bib54} 
ref
\newblock{info} 

\bibitem{bib55} 
ref
\newblock{info} 

\end{thebibliography}



\end{document}

