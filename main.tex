\documentclass[10pt,letterpaper]{article}
\usepackage{booktabs, makecell}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{ {./figures/png/} }
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}
\usepackage[linesnumbered]{algorithm2e}
\usepackage[section]{placeins}

\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}


% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

\usepackage{color,soul}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}



%% Include all macros below
\newif\ifhighlight
\highlightfalse
\highlighttrue
\newcommand{\hlb}[1]{\ifhighlight{\hl{#1}}\else{#1}\fi}


% Use \showblockfalse to turn remove google maps related content globally.
% Use \highlightfalse to turn off highlighting globally.

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Leveraging cross-view geo-localization with ensemble learning and temporal awareness} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Abdulrahman Ghanem\textsuperscript{1},
Ahmed Abdelhay\textsuperscript{1},
Noor Eldeen Salah\textsuperscript{1},
Ahmed Nour Eldeen\textsuperscript{1},
Mohammed Elhenawy\textsuperscript{2},
Abdallah A. Hassan\textsuperscript{1},
Ammar M. Hassan\textsuperscript{3*},
Mahmoud Masoud\textsuperscript{2}
\\
\bigskip
\textbf{1} Computer and Systems Engineering Department, Faculty of Engineering, Minia University, Minia, Egypt
\\
\textbf{2}  Centre for Accident Research and Road Safety-Queensland (CARRS-Q), Queensland University of Technology, Brisbane, Australia
\\
\textbf{3} Arab Academy for Science, Technology, and Maritime Transport, South Valley Branch, Aswan, Egypt
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% % Additional Equal Contribution Note
% % Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% % Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% % \textcurrency b Insert second current address 
% % \textcurrency c Insert third current address

% % Deceased author note
% \dag Deceased

% % Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* ammar@aast.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
The Global Navigation Satellite System (GNSS) is unreliable in some situations. To mend the poor GNSS signal, an autonomous vehicle can self-localize by matching a ground image against a database of geotagged aerial images. However, this approach has challenges because of the dramatic differences in viewpoint between aerial and ground views, harsh weather and lighting conditions, and a lack of orientation information in training and deployment environments. In this paper, \hlb{it was shown that previous models in} this area are complementary, not competitive, and that each model solves a different aspect of the problem. \hlb{There was a need for a wholistic approach.} \hlb{An ensemble model was proposed} to aggregate the predictions of multiple independently trained state-of-the-art models. \hlb{Previous state-of-the-art (SOTA) temporal-aware models  used heavy-weight network to fuse the temporal information into the query process. The effect of making the query process temporal-aware is explored and exploited by an efficient a meta block: naive history}. But none of the existing benchmark datasets \hlb{was} suitable for extensive temporal awareness experiments, \hlb{a new derivative dataset based on the BDD100K dataset was generated}. \hlb{The proposed ensemble model achieves} a recall accuracy R@1 \hlb{(Recall@1: the top most prediction)} of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset (surpassing the current \hlb{SOTA}). The  temporal awareness algorithm converges to R@1 of ~100\% by looking at a few steps back in the trip history.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{\hlb{1} Introduction}
The current standard localization technique is the global navigation satellite system (GNSS). Although the GNSS accuracy declines in cases where there are few lines of sight (e.g., urban canyons~\cite{bib1}). Using cross-view geo-localization, a vehicle localizes itself by matching street view images against a database of geotagged images captured from aerial platforms (e.g., a satellite~\cite{bib2} or a drone~\cite{bib3}). Cross-view geo-localization is gaining popularity in the scene of autonomous vehicles~\cite{bib2} and robotic navigation~\cite{bib25}: it compensates for a bad GNSS signal-to-noise ratio. And, it’s preferable to other image-based localization techniques (e.g., landmark and ground-to-ground matching) for the ease of covering new areas. Early work~\cite{bib37} on this technique \hlb{claims} that its main challenge is the lack of visual correspondence between aerial and ground views. Later work, while holding the same claim, \hlb{shows} that there are more challenges: geographic scene changes over time~\cite{bib6,bib7,bib8,bib9,bib10}, poor weather and lighting conditions, lack of orientation information~\cite{bib4}, and misalignment between aerial and ground images during training.
None of the previous models \hlb{tries} to collectively address these five challenges. To build a holistic solution for the problem, \hlb{An ensemble model is proposed} to fuse the predictions of five independently trained models. Each of these models addresses a different challenge.

Moreover, most of the recent work treats the problem as a 1-to-1 image-matching task. This overlooks the fact that these models get deployed in environments where there's a continuous stream of input, not a single query image. Taking the temporal nature of the problem into account is a must: \hlb{the experiments in this work show} that the recent models can't differentiate between highly similar query images, and in the case of a moving vehicle, the consecutive images have a high degree of similarity. Some recent models \hlb{consider} the temporal nature of the problem. \hlb{For instance, \emph{Markov chain Monte Carlo (MCMC)} algorithms are used in} ~\cite{bib7}  and~\cite{bib20,bib21,bib22} to predict the current pose and enforce temporal consistency. \hlb{In} ~\cite{bib24}, \hlb{the authors enforce} temporal consistency by using a transformer-based trajectory smoothing network. \hlb{These} methods are resource intensive or have strong assumptions about the current state of the vehicle. \hlb{In this paper, an efficient technique, namely, naive history, is explored} to achieve the same goal.

Although the existing datasets aren't suitable for conducting experiments to prove that temporal awareness improves accuracy  — \hlb{the authors of} \cite{bib24} used BDD100k to generate a derivative dataset, sorrowfully, upon contacting the authors to grant us access to this dataset we received no response. The dataset needs to be realistic and collected as a trajectory of close points over a long-running journey to resemble driving in a real environment.  CVUSA~\cite{bib55} and CVACT~\cite{bib4} include sparse points on the map. VIGOR~\cite{bib5} includes dense points but doesn't form trajectories. RobotCar~\cite{bib6} has a limited number of examples. \hlb{A new derivative dataset} based on the BDD100K~\cite{bib12} dataset \hlb{is introduced} to fill this gap. 

\hlb{The rest of the paper is structured as follows: Section 2 gives an overview of the related work. Section 3 covers how the ensemble model was built, dataset collection procedure, and creation of the naive history meta block. Section 4 discusses the results of the experiments done in section 3. Section 5 concludes the paper. In summary, the contributions of this paper are:}
\begin{enumerate}
    \item \hlb{A new ensemble model based on five of the current state-of-the-art  cross-view matching networks is introduced}. The ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset.
    \item \hlb{A new derivative dataset that is suitable for temporal-aware cross-view geo-localization models based on BDD100K is constructed}.
    \item \hlb{A meta block: naive history, is developed to make the query process temporal aware}. \hlb{While showing} that taking journey history into account minimizes the search space. This reduction in search space improves the accuracy.  The accuracy converges to $\sim$100\% with a three-step lookback into trip history on our proposed dataset. This meta block is usable with any \hlb{cross-view} model.
\end{enumerate}

\section*{\hlb{2} Related work}
In section (\hlb{2.1}), we start by investigating how different models engineered their features and architectures. Their choices show how different models tried to approach the problem from different angles. Followed by giving a bird-eye-view of the architectures of these models and feature extraction methods. \hlb{Section (2.2) walks} through the models that tried blending the trip history into the image-matching task. By grouping the models into two categories: one that relies on "this place looks familiar" in section (\hlb{2.2.1}), and one that relies on "where have we been before getting here?" in section (\hlb{2.2.2}).
\subsection*{\hlb{2.1} Features and architectures}
Most of the recent work treated the cross-view geo-localization task as an image retrieval task. They tried to find a feature representation suitable for matching query ground images and aerial ground images. The nature and complexity of used networks changed over time. Here, we conduct a brief comparison between the most common feature representations and their architectures. Table~\ref{table1} summarizes the feature types and the backbones of different cross-view geo-localization networks.


\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
  \centering
  \caption{
  {\bf A summary of the feature types and the backbones of different cross-view geo-localization networks.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Feature type} & {\bf Backbone} & {\bf Used in}&  \\ \midrule
    & Hand-crafted & SIFT & ~\cite{bib20}~\cite{bib26}& \\
    & Hand-crafted & SURF, FREAK, PHOW & ~\cite{bib20}& \\
    & Hand-crafted & SIFT + VLAD & ~\cite{bib7}& \\
    & Semantic & Faster R-CNN & ~\cite{bib32}& \\
    & CNN & VGG + FCN + NetVLAD & ~\cite{bib36}~\cite{bib22}~\cite{bib14}& \\
    & CNN & AlexNet & ~\cite{bib37}~\cite{bib32}& \\
    & CNN & VGG & ~\cite{bib21}~\cite{bib4}~\cite{bib13}& \\
    & CNN & ResNet & ~\cite{bib4}~\cite{bib21}~\cite{bib41}& \\
    & CNN & DenseNet & ~\cite{bib4}~\cite{bib21}& \\
    & CNN & U-net & ~\cite{bib4}& \\
    & CNN & Xception & ~\cite{bib21}& \\
    & CNN & - & ~\cite{bib44}& \\
    & CNN & VGG + FCN & ~\cite{bib43}& \\
    & Attentive & Siam-FCANet (ResNet + FCAM + FCN) & ~\cite{bib45}& \\
    & Attentive & Siam-VFCNet (ResNet + FCAM + NetVLAD) & ~\cite{bib45}& \\
    & Attentive & VGG + SAFA + SPE & ~\cite{bib46}~\cite{bib47}~\cite{bib17}& \\
    & Attentive & VGG + Geo Attention + Geo-temporal Attention & ~\cite{bib24}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib8}& \\
    & Attentive & SAFA & ~\cite{bib5}& \\
    & Attentive & ResNet + SAFA & ~\cite{bib18}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib19}& \\
    & Attentive & VGG + MSAE & ~\cite{bib49}& \\
    & Synthesized & X-Fork & ~\cite{bib50}& \\ \bottomrule
    \end{tabular}
  }
  \label{table1}
\end{table}

\subsubsection*{\hlb{2.1.1} Hand-crafted features}
Early research used hand-crafted features. \hlb{SIFT}~\cite{bib27} \hlb{was used in}~\cite{bib20,bib26}. \hlb{The authors of} ~\cite{bib20} experimented with other feature spaces (e.g., SURF~\cite{bib28}, FREAK~\cite{bib29}, PHOW~\cite{bib30}) but SIFT outperformed others. \hlb{Dense SIFT features were computed in}~\cite{bib7}, then embedded into a higher dimensional vector space using a VLAD~\cite{bib31}. The extracted features \hlb{in this category were brittle; it failed} to adapt to appearance change. Later, it proved to have inferior performance compared to the CNN-based features.

\subsubsection*{\hlb{2.1.2} Semantic features}
In this approach, the networks matched between ground and aerial images based on the meaningful content of the image. This made it more robust to viewpoint changes than local features. But the model performance degraded in areas lacking the pre-selected semantic features. \hlb{In}~\cite{bib32}, \hlb{the authors} treated the problem as object detection and recognition: the first block of the architecture employed the Faster R-CNN~\cite{bib33} to detect buildings, and the second block used AlexNet~\cite{bib34} with the Siamese architecture~\cite{bib35} to recognize the buildings.

\subsubsection*{\hlb{2.1.3} CNN-based features}
Metric learning achieved promising results in bridging the domain gap between aerial and ground image representation. \hlb{In} ~\cite{bib36}, \hlb{the authors} used a fully convolutional network (FCN) with a NetVLAD~\cite{bib34} layer using a Siamese architecture. \hlb{The authors of}~\cite{bib37} tried modified versions of AlexNet. \hlb{The authors of}~\cite{bib21} experimented with different FCN layers: VGG~\cite{bib31}, ResNet~\cite{bib38}, DenseNet~\cite{bib39}, and Xception~\cite{bib40} with the same architecture of~\cite{bib36}, they found that VGG outperforms other networks. \hlb{In}~\cite{bib22}, \hlb{the authors} used the CVM-Net-I architecture proposed in~\cite{bib36}. \hlb{In}~\cite{bib41}, \hlb{the authors} exploited a modified ResNet50 network for ground images and a ResNet18 for aerial ones. \hlb{The authors of}~\cite{bib13} used VGG16 to generate the feature maps for the polar transformed aerial and then fed it into the Dynamic Similarity Module (DSM). \hlb{In}~\cite{bib4}\hlb{, the model} learned orientation information by using different backbones (VGG, ResNet, DenseNet, and U-net~\cite{bib42}) with the Siamese architecture. \hlb{The authors of}~\cite{bib43} employed the Siamese network with a VGG backbone to extract feature maps, then a fully connected layer aggregates these feature maps. \hlb{In}~\cite{bib44}\hlb{, the authors} used a CNN to generate feature maps and then transform them from the ground domain to the aerial domain. \hlb{The authros of}~\cite{bib14} applied the hybrid perspective mapping method using the CVM-Net-I architecture.

\subsubsection*{\hlb{2.1.4} Attentive features}
For this feature type, the networks used spatial attention to enhance the feature representation. \hlb{In}~\cite{bib45}\hlb{, the authors} integrated the lightweight attention module (FCAM) into each block of the basic ResNet. \hlb{The authors of}~\cite{bib46} used a spatial-aware feature aggregation (SAFA) module to mitigate the distortion in the aerial image\hlb{, then} employed the spatial-aware position embedding module (SPE) to encode relative positions among features in the feature maps. \hlb{The authors of}~\cite{bib5} proposed the VIGOR network built on top of SAFA. \hlb{In}~\cite{bib47,bib17}\hlb{, the authors} used the same architecture as~\cite{bib46} with a different loss function: \emph{geo-distance weighted loss}. \hlb{In}~\cite{bib24}\hlb{, the authors} proposed a geo-attention module for the aerial branch, and a temporal-attention module for the ground branch. \hlb{The authors of}~\cite{bib8} used convolutional block attention modules~\cite{bib48} to generate multi-scale attention features. \hlb{The model built in}~\cite{bib18} employed a modified ResNet34 backbone with a spatial-aware attention module. \hlb{In}~\cite{bib19}\hlb{, the authors} proposed EgoTR network. EgoTR used a ResNet backbone transformer encoder with a self-cross attention mechanism. \hlb{The authros of}~\cite{bib49} introduced the Multi-Scale Attention Encoder (MSAE). MSAE employed a VGG backbone with a multi-scale attention encoder followed by FCN to generate feature masks.

\subsubsection*{\hlb{2.1.5} Synthetic features}
In this approach, the networks learned robust feature representation by reversing the task: it learned how to create ground views from aerial views, which made it learn salient features and suppress others. \hlb{The model built in}~\cite{bib50} synthesized aerial representation of a ground panorama query using the X-Fork network~\cite{bib51} with edge maps detection by Canny Edge Detection~\cite{bib52}.

\subsection*{\hlb{2.2} Contextual awareness}
The fact that these models get deployed in autonomous vehicles, makes it obvious that the models should be aware of the trip context. \hlb{Contextual awareness can be categorized} as follows:

\subsubsection*{\hlb{2.2.1} Spatial awareness}
We have to differentiate between two types of spatial awareness.

\begin{itemize}
  \item Some models refer to it as the knowledge about the pose (location and orientation) of the query image and its relative pose to the aerial image frame. \hlb{In}~\cite{bib47,bib17}\hlb{, the authors} constructed mini-batches of images within a certain geographic radius and used a modified version of the triplet loss function: \hlb{\emph{Geo-distance weighted loss}} to favor examples where the images are within the selected radius. The Geo-Attention module used in~\cite{bib24} \hlb{exploited} a similar loss function. \hlb{The authors of}~\cite{bib14} employed hybrid perspective mapping to establish correspondence between ground and aerial images. \hlb{The model built in}~\cite{bib4} injected the orientation information into the network \hlb{and} used multiplane image (MPI)~\cite{bib13} projections to exploit geometric correspondence between ground and aerial images. Table~\ref{table2} gives an overview of spatial (pose-wise) awareness approaches used in cross-view geo-localization.
  \begin{table}[!ht]
    \centering
    \caption{
    {\bf An overview of the spatial (pose-wise) awareness approaches used in cross-view geo-localization.}}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Geographic proximity & ~\cite{bib47}~\cite{bib17}~\cite{bib24}& \\
    & Polar Transform & ~\cite{bib3,bib4}~\cite{bib5}~\cite{bib13,bib14}~\cite{bib17,bib18,bib19}& \\
    & Inverse Polar Transform & ~\cite{bib49}& \\
    & Orientation & ~\cite{bib4}& \\
    & Dynamic Similarity Matching (DSM) & ~\cite{bib13}& \\
    & Hybrid Perspective Mapping & ~\cite{bib14}& \\ \bottomrule
    \end{tabular}
    \label{table2}
  \end{table}
  \item Other networks refer to it as paying more attention to the salient features, suppressing less important features, and encoding the spatial layout information into the feature representation. \hlb{That was covered} in section (\hlb{2.1.4}).
\end{itemize}

\subsubsection*{\hlb{2.2.2} Temporal awareness}
There are two types of temporal awareness too!
\begin{itemize}
  \item Finding a robust feature representation that won’t be affected by scene changes throughout time. These changes can be geographic landmarks (e.g., new constructions) or environmental conditions such as weather conditions.
  \hlb{In}~\cite{bib6,bib54}\hlb{, the authors} used time-invariant approaches by capturing the same scene during different conditions across time. But, this technique required a dataset where the same scene is covered during different conditions which is challenging to collect. Possible solutions for this are as follows: {\bf A)} \hlb{Authros of}~\cite{bib8} used semantic object-based data augmentation techniques to remove and add objects (cars, roads, greenery, and sky). {\bf B)} \hlb{Authors of} ~\cite{bib7} applied PCA projection to make background features less significant in the image descriptor. {\bf C)} Another solution is using synthetic datasets.
  \item Exploiting the fact these models will be deployed on vehicles where a temporally coherent sequence of images is available.
  \hlb{Authors of}~\cite{bib7} and~\cite{bib20,bib21,bib22} used MCMC algorithms, namely, a particle filter~\cite{bib45} with variations of initialization and transition techniques, the algorithms are used to predict current pose and enforce temporal consistency. \hlb{In}~\cite{bib24}\hlb{, the authros} introduced a geo-temporal attention module, the module attends to all frames in a video of ground footage to learn better features, also, it enforces temporal consistency by using a transformer-based trajectory smoothing network.
\end{itemize}
\hlb{Table}~\ref{table3} \hlb{gives} an overview of the temporal awareness approaches used in cross-view geo-localization.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf An overview of the temporal awareness approaches used in cross-view geo-localization.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Capture multiple examples with different environmental conditions & ~\cite{bib6}~\cite{bib54}& \\
    & Semantic object-based data augmentation & ~\cite{bib8}& \\
    & Observation encoder + PCA projection & ~\cite{bib7}& \\
    & MCMC algorithms & ~\cite{bib7}~\cite{bib20,bib21,bib22}& \\
    & Sequence attention + Trajectory Smoothing Network & ~\cite{bib24}& \\ \bottomrule
    \end{tabular}
  }
  \label{table3}
\end{table}

\section*{\hlb{3} Methodology}
\subsection*{\hlb{3.1} Evaluation metric}
In this research, \hlb{the} same evaluation metric used in~\cite{bib3,bib4,bib5,bib8,bib13,bib14,bib17,bib18,bib19,bib20,bib21,bib22,bib24,bib36,bib43,bib44,bib45,bib46,bib47,bib49,bib50} \hlb{is used}: the {\bf Recall@$k$ (r@$k$)}. \hlb{For r@$k$, it's considered a} match if the corresponding aerial image is in the top \hlb{$k$} predictions. r@$1$, r@$5$, r@$10$, and r@$1\%$ are used. r@$1$ means the true image is the first prediction of the model, r@$5$ means the true image is in the first five predictions of the model, and so on.

\subsection*{\hlb{3.2} The ensemble model}

There are many challenges in cross-view matching. To name a few:  missing correspondence and orientation information, scene changes over time, and the high similarity among geographically close points. Recently, several models were developed to solve the cross-view (CV) matching problem, and different models approached the CV matching problem from different angles. Thus, each model has its advantages and disadvantages. We hypothesize that aggregating the outputs of these uncorrelated models might improve the accuracy.
So, in this research, \hlb{an ensemble model of independently trained models was built} to solve the CV matching problem. The proposed ensemble used the same datasets to train different neural network architectures. \hlb{The} CVUSA~\cite{bib55} and CVACT~\cite{bib4} benchmark datasets \hlb{are used} to train five models Each model addresses a  different aspect of the problem: DSM~\cite{bib13} estimates the cross-view orientation alignment.  EgoTR~\cite{bib19} models global dependencies to decrease visual ambiguities and matches geometric configuration between ground and aerial images. SAFA~\cite{bib46} exploits geometric correspondence between aerial and panoramic ground images. Toker~\cite{bib18} biases the localization network via a Generative Adversarial Network (GAN) to learn salient features. SFCANet~\cite{bib45} uses Hard Exemplar Reweighting to assign a greater weight to hard examples. Table~\ref{table4} shows their r@\hlb{$k$} metrics.

\begin{table}[!ht]
\begin{adjustwidth}{-1.0in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
  \centering
  \caption{
    {\bf The r@\hlb{$k$} metrics for the networks used to construct the  ensemble model.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllllcrrrrrrr@{}}
      \toprule
      & \multirow{3}{*}{\bf Network} & \multicolumn{4}{c}{\bf CVUSA} & \phantom{abc}& \multicolumn{4}{c}{\bf CVACT} & \phantom{abc} & \\
      \cmidrule{3-6} \cmidrule{8-11}
      & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \\
      &\phantom{abc} & \multicolumn{9}{c}{(\%)}  \\ \bottomrule
      & DSM~\cite{bib13} & 92.07 & 97.33 & 98.33 & 99.60 & \phantom{abc} & 81.93 & 91.83 & 93.95 &  97.42& \\
      & Toker~\cite{bib18} & 92.15 & 97.28 & 98.26 &  99.56 & \phantom{abc}  & 83.52 & 93.89 & 95.48 &  98.17& \\
      & EgoTR~\cite{bib19} & 93.87 & 98.22 & 98.98 & 99.66 & \phantom{abc} & 84.87 & 94.5 & 95.95 &  98.37& \\
      & SFCANet~\cite{bib45} & 51.01 & 78.92 & 86.80 & 98.49 & \phantom{abc} & x & x & x &  x& \\ 
      & SAFA~\cite{bib46} & 88.93 & 96.65 & 97.97 & 99.65 & \phantom{abc} & 74.56 & 89.65 & 92.46 &  97.72& \\ \bottomrule
    \end{tabular}
  }
  \label{table4}
  \end{adjustwidth}
\end{table}

This research investigates two different aggregation methods: soft-voting, and hard-voting. As shown in \hlb{Fig.} \ref{fig1}, for both methods, \hlb{all possible  combinations of the models (their power set) were tried}.

\begin{figure}[!ht]
\includegraphics*[width=0.9\linewidth]{fig1.tiff.png}
\caption{{\bf Different ensemble aggregation methods.} We use 5 state-of-the-art models and combined their outputs using 32 different combinations for each strategy.}
\label{fig1}
\end{figure}

In soft-voting, \hlb{two calculation methods are tried}: {\bf A)} averaging the predictions of the individual models. {\bf B)} calculating the joint probability, using Eq~(\ref{eq1}). Both methods have identical results.

\begin{eqnarray}
\label{eq1}
\text{total vote} = \exp(\log(vote_1) + \log(vote_2) + ..., \log(vote_n)),
\end{eqnarray}

In hard-voting, \hlb{the majority vote of the models is selected}. Hard-voting needs at least three models to have a majority vote. There are two cases where a majority vote doesn’t exist:
\begin{enumerate}
  \item Combinations with an even number of models can tie, \hlb{the combination containing the most accurate model is picked}.
  \item All models’ predictions differ, \hlb{two strategies are tried}: {\bf A)} take the prediction of the most accurate model in the combination. {\bf B)} pick a prediction from a random normal distribution of individual models' predictions.
\end{enumerate}

\subsection*{\hlb{3.3} BDD trajectories dataset collection}
\hlb{The proposed naive history meta block exploits} the temporal nature of the problem. The existing benchmark datasets (e.g, CVUSA, and CVACT) are collected from sparse points on the map while \hlb{a trajectory of close points is needed}. Inspired by the work of Regmi and Shah~\cite{bib24}, \hlb{A new derivative dataset from the BDD100k}~\cite{bib12} dataset \hlb{is constructed to address this gap}. The BDD100K dataset is crowdsourced, diverse, and large-scale, with IMU/GPS recording, and other semantic annotations (irrelevant to this research). All videos in the dataset are $40s$ long, though the total distance varies. \hlb{The videos with a distance greater than $50$ are chosen}, the statistical summary of the distance covered in the selected videos is in Table~\ref{table5}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf A statistical summary of the distance covered in selected videos.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
    & {\bf Count} & {\bf Mean} & {\bf \emph{std}} & {\bf Min} & {\bf Max} &\\ \midrule
    & 47943 videos & 278.668 m & 181.786 m & 50.000 m & 2560.000 m \\ \bottomrule
    \end{tabular}
  }
  \label{table5}
\end{table}

\hlb{The proposed dataset} consists of 95,000 examples. Each example consists of five ground images and one aerial image, and some examples are shown in \hlb{Fig.} \ref{fig2}. \hlb{The ground images were sampled} from the picked videos with a sampling rate of $1 / frame/10 m$ to have some visual changes between the consecutive frames, but at the same time, the frames still look relatable to one another. The \emph{IMU/GPS} data is captured at $1 \ sample/s$. The distance moved in one second varies; the speed of the vehicles is not constant. \hlb{The proposed dataset} cares about the visual changes, not the passage of time. So when the distance between every two consecutive locations is greater than $10 \ m$, \hlb{the following sampling algorithm is used to sample uniform trajectories}:

\begin{algorithm}[!h]
\caption{BDD100K resampling}
\KwData{The GPS/IMU information recorded along with the videos, BDD100K videos.}
\KwResult{Resmapled frames(1 frame/10 m)}
\Comment{ Get the speed at the current ($v_n$), and next ($v_{n+1}$) location from IMU data.
\
Assume the speed between these two locations ($v_{n \to n+1}$) is the average speed of both locations. For all locations on the trajectory which is a multiple of the sampling distance parameter ($d$):}\
$v_{\text{n} \to \text{n+1}} \gets \frac{v_{\text{n}} + v_{\text{n} + 1}}{2}, n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \}$ \;

\Comment{To get the timestamp of the nth frame ($t_{\text{n}}$)}

$t \gets \frac{\text{d} \times  \text{n}}{v_{\text{n}} + v_{\text{n} + 1}} , n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \} $;\

Extract the frames at the selected timestamps using FFmpeg~\cite{bib23};\
\end{algorithm}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig2.tiff.png}
  \caption{{\bf Three examples from our dataset. For ground images.} The time progresses from left to right and the distance between the consecutive frames is 10 m.}
  \label{fig2}
\end{figure}

\hlb{The aerial tiles are captured} at the midpoint of the example using the great circle algorithm~\cite{bib16} and then fetched from Google Maps~\cite{bib15}. We experimented with different zoom levels. \hlb{The 20th zoom level was chosen}; it covers a wide area with great detail. \hlb{A tile size of $800 \times 800$ was chosen}.

After extracting the frames, \hlb{trajectories where 30\% of the extracted ground frames are mildly lit ($44.5\%$of all trajectories) is removed}. For example, the trajectory of the Bright frame in \hlb{Fig.} \ref{fig5} \hlb{is accepted and the other two trajectories are dropped}. Then the trajectories that have blurry aerial tiles ($13.5\%$ of the bright trajectories) \hlb{are removed} (similar to the example shown in \hlb{Fig.} \ref{fig6}). \hlb{A Laplacian} filter~\cite{bib11} with a threshold of $200$ \hlb{is used} to detect blurry aerial tiles, and a grayscale mean filter with a threshold of $85$ \hlb{is employed} to detect dark ground frames. We chose both thresholds empirically to drop all the true positive corrupt examples with some false positives. \hlb{Fig.} \ref{fig7} shows a simplified dataflow of the data processing pipeline. \hlb{After that the ground} and aerial images’ \hlb{width was scaled to $400$px} while keeping the height in aspect ratio using the \emph{Lanczos algorithm}.

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig5.tiff.png}
  \caption{{\bf Examples of different lighting conditions in the BDD100K dataset.} In the dark frames, we can't construct a meaningful correspondence between the ground and aerial images.}
  \label{fig5}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics*[width=0.3\linewidth]{fig6.tiff.png}
  \caption{{\bf An example of a blurry aerial image.} Sometimes this is deliberate by the satellite imagery provider for privacy reasons.}
  \label{fig6}
\end{figure}

\hlb{In contraray to the GAMa dataset} \cite{bib60}, \hlb{the examples in the BDD-trajectories dataset are evenly-spaced and the examples where the crosspondence between the ground and aerial images is imperceptible (due to lighting conditions or satellite capture policies) are removed.}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig7.tiff.png}
  \caption{{\bf A simplified version of the data processing pipeline.} This dataflow pipeline is \emph{embarrassingly parallel}. We had it running across a 6-machines-cluster, 16 core each.}
  \label{fig7}
\end{figure}

\FloatBarrier

\subsection*{\hlb{3.4} Naive history}
Inspired by \hlb{the} experiments on the joint probability as a soft-voting strategy for ensemble learning. In a journey setting where the history of the journey is available, we hypothesize that taking the previous predictions into account might cause the prediction to converge while progressing in the journey. This hypothesis relies on the fact that even if the model doesn't return the true location as its top-1 prediction, most of the time, it is still in the top-1\% predictions. Also, the probability that a model will predict $N$ consecutive wrong predictions decreases as $N$ increases. Based on this, we fine-tune the EgoTR model on the new derivative dataset (\emph{BDD-trajectories}). \hlb{The BDD-trajectories dataset is used because it has trip trajectories.}

\subsubsection*{\hlb{3.4.1} Fine-tuning EgoTR}
The \emph{EgoTR} model takes a pair of images as input: a ground image and a satellite image as its input. However, an example in \hlb{the proposed} dataset consists of a hexad: five ground images and a satellite image. \hlb{The proposed dataset has to get reshaped} to be suitable for the EgoTR input format. \hlb{The  chosen examples are the ones} where there are at least two examples from the same journey, \hlb{that is} 10 ground images from the same trajectory. \hlb{Every ground image gets paired with the satellite image from the example}. This means that each example in \emph{BDD-trajectories} corresponds to five examples in the reshaped dataset. \hlb{A subset of reshaped dataset is used}: $19015$ pairs for validation and $75985$ pairs for fine-tuning.

\subsubsection*{\hlb{3.4.2} Attaching the naive history block to EgoTR}
After fine-tuning EgoTR\hlb{, the distance array is generated} for all the images in \hlb{the proposed} validation dataset. Then \hlb{this array is fed as an} input for Algorithm~\ref{alg:two}.

\begin{algorithm}[H]
  \label{algorithm2}
  \caption{Naive history}\label{alg:two}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$}
  \KwResult{History reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  \Return $m_{ij}$\;
\end{algorithm}


The \emph{naive history} meta block reinforces the prediction at the current position by looking back into the trip history. The $distanceArray$ parameter is the distance between every ground and aerial image (the output of the model). The $historyDepth$ parameter controls how deep \hlb{the algorithm looks} back into history. \hlb{The total distance array is shifted by} $historyDepth$ columns and rows to get the distance array at the previous location (Algorithm 2, Line 6). \hlb{A shifted version of the $distanceArray$ is kept} to leave the predictions at future steps intact (Algorithm 2, Line 7). \hlb{The distance array at the current step and the distance array at the previous step are multipled, element-wise ($\odot$)} (Algorithm 2, Line 8). \hlb{The original array gets updated} with the reinforced predictions (Algorithm 2, Line 9). Repeat the steps 4 through 7 (Algorithm 2, Line 4), with increasing value of $historyDepth$ until reaching the value of final history depth value, for example, if \hlb{the algorithm wants to look back} five steps in trip history \hlb{it} will iterate over $historyDepth$  \{1, 2, 3, 4, 5\}.

\subsubsection*{\hlb{3.4.3} The effect of prior on naive history}
As mentioned before the cross-view models are complementary to existing GNSS, so we can improve naive history performance by initializing it with a weak prior (the location captured by the GNSS). Algorithm~\ref{alg:three} initializes the distance array (generated by the model) with a probability of the first image in each example equal to $1\mathrm{e}{-6}$. In other words, in (Algorithm 3, Lines 5-9) the probability of the first image in the trajectory is modified to make the probability of the ground truth image equal 1e-6 and set other probabilities to a uniform value of $(1 - (1\mathrm{e}{-6}) / distance array size)$. That experiment proves that initializing the naive history algorithm with this prior knowledge speeds up the convergence to 100\% accuracy significantly.

\begin{algorithm}[H]
  \caption{Naive history with a weak prior}\label{alg:three}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$, $priori \in R_{+}$, $trajectorySize  \in N$}
  \KwResult{Prior-aware history reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  $normalizer \gets priori/ len$\;
  
  \For{$k \in [0, len] \land k\ \% \ trajectorySize \equiv 0$} {
      $currentPrediction_{ij} \gets (m_{ij})_{\substack{i\\ j \equiv k }}$\;
      $ (currentPrediction_{ij})_{\substack{i \equiv k}} \gets priori$\;
      $(m_{ij})_{\substack{i\\ j \equiv k }} \gets currentPrediction_{ij} - normalizer$\;
  }
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  
  \Return $m_{ij}$\;
\end{algorithm}
\FloatBarrier

\section*{\hlb{4} Results \hlb{and discussion}}
\subsection*{\hlb{4.1} The ensemble model}
\hlb{Altough there are multiple options for bulding the ensemble model (e.g., stacking, boosting, and mixing models). We chose mixing models (i.e., voting) for the following reasons:}
\begin{itemize}
  \item \hlb{If we tried stacking models, we would have to train the base models along side the meta model, otherwise, the meta model would overfit to the base models. Base models retraining would require a lot of computational resources.}
  \item \hlb{All member models in the ensemble are trained on the entire dataset which means the meta leaner would also train on the same data as the base model. The other option is to train all the models from scratch, but this would require a lot of training resources.}
  \item \hlb{Boosting models is not a good option because the base models are independent: the models have different weaknesses and strengths.}
  \item \hlb{More importantly, the main goal is to show the need for wholistic solution for the problem, rather than showing the effectiveness of the ensemble model: in real time scenarios, a single model with something like the naive history algorithm would be much more efficient than an ensemble model.}
\end{itemize}
\hlb{Now focusing on the voting ensemble model: } \hlb{Fig.} \ref{fig8} and \hlb{Fig.} \ref{fig9} show the results of the soft-voting strategies for CVUSA and CVACT, respectively. The DSM, EgoTR, and Toker combination outperforms other combinations. \hlb{That is because these three models solve three orthognal parts of the problem: orientation, geometric correspondence, and cooching the right features.}  Increasing the number of the models doesn’t necessarily improve the accuracy; to improve the accuracy; individual models have to predict different examples correctly. \hlb{Although this is mitiageatable by using a weighted voting strategy.}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig8.tiff.png}
  
  \caption{{\bf CVUSA combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations.}
  \label{fig8}
\end{figure}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig9.tiff.png}
  \caption{{\bf CVACT combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations. }
  \label{fig9}
\end{figure}

\hlb{Fig.} \ref{fig10} and \hlb{Fig.} \ref{fig11} show the results of hard-voting with the most accurate model prediction strategy for CVUSA and CVACT, respectively. \hlb{The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
\hlb{The accuracy drops about 2\% for the r@1 and r@5 metrics for CVACT. Rarely all models return the gorund truth in the top-5 predictions for CVACT. In some situations the ensemble only have wrong choices to choose from. This was well-compensated by soft voting: the distances calculated by the joint probability can leverage the ground truth predciton even if it isn't in the top-5. The ensemble didn't have the same issue with CVUSA because the majority of models retrun the ground truth in the top-5 prediction.}

\hlb{Soft-voting improves accuracy by looking at the }{\bf \hlb{top-k predictions}} \hlb{across different models. For the sake of illustration, Fig. }\ref{fig15} \hlb{and Fig.} \ref{fig16} \hlb{show an example of the predictions of the individual models. None of the models returned the right prediction as the first prediction, but it was in the top-5 predictions for most models. Their collective prediction (the model ensemble) could return it as the first prediction.}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig10.tiff.png}
  \caption{{\bf CVUSA combinations for hard-voting with the most accurate model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}  \label{fig10}
\end{figure}

\begin{figure}[!ht]
  \includegraphics*[width=\textwidth,height=0.8\textheight,keepaspectratio]{fig11.tiff.png}
  \caption{{\bf CVACT combinations for hard-voting with the most accurate  model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \label{fig11}
\end{figure}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig15.tiff.png}
  \caption{{\bf An example that shows the ensemble model r@(1 - 5) compared to the individual models on the CVUSA dataset.} The true satellite has a red border.}
  \label{fig15}
\end{figure}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig16.tiff.png}
  \caption{{\bf An example that shows the ensemble model r@(1 - 5) compared to the individual models on the CVACT dataset.}  The true satellite has a red border.}
  \label{fig16}
\end{figure}

\hlb{Fig.} \ref{fig12} and \hlb{Fig.} \ref{fig13} show the results of the hard-voting with the random selection strategy for CVUSA and CVACT, respectively. \hlb{Here after removing the bias for the most accurate model increasing the number of voting nodels improvs the result; there's a higher prebability of having the right ground truth in the votes pool.} The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig12.tiff.png}
  \caption{{\bf CVUSA combinations for hard-voting with the random selection strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. }
  \label{fig12}
\end{figure}

\begin{figure}[!ht]
  \includegraphics*[width=\textwidth,height=0.8\textheight,keepaspectratio]{fig13.tiff.png}
  \caption{{\bf CVACT combinations for hard-voting with the random selection strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \label{fig13}
\end{figure}

\FloatBarrier

\hlb{Fig.} \ref{fig14} illustrates that soft voting performs the best. Although soft voting results with joint probability and averaging are identical, the computational cost of averaging strategy is cheaper. Hard-voting strategies have constant computational complexity. 

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig14.tiff.png}
  \caption{{\bf Comparison between aggregation method for the best performing combinations. } {\bf A}: CVUSA and {\bf B}: CVACT.  The dark bar is the best performing aggregation method. Soft voting outperforms other methods across all r@\hlb{$k$} metrics for both datasets.}
  \label{fig14}
\end{figure}

\FloatBarrier
\hlb{However promising the result of the ensemble model, the practicality of it is limited when it comes to deployment. Deoloying such a model would require running multiple models in parallel and then aggregating the results. This is not feasible in real-time applications for the models used in this work. To the best of our knowledge, this is the first work that uses an ensemble model cross-view goe-localization, and it is a promising direction for future work.}

\subsection*{\hlb{4.2} EgoTR fine-tuning}

The training process took $192$ hours for $228$ epochs. Table~\ref{table6} shows the r@\hlb{$k$} metrics for the model. This drop in accuracy can be attributed to: {\bf A)} the ground images in \hlb{the proposed} dataset aren't panoramic, in contrast to CVUSA. {\bf B)} high similarity between the consecutive pairs. {\bf C)} one of the shortcomings of the r@\hlb{$k$} metric is that it depends on the size of the validation dataset as shown in \hlb{Fig.} \ref{fig17}, \hlb{the proposed} validation dataset size is more than double the size of CVUSA or the size of CVACT.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf r@\hlb{$k$} metrics of EgoTR fine-tuned over the reshaped BDD-trajectories dataset.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}ccccccc@{}}
    \toprule
    & r@{\bf 1} & r@{\bf 5} & r@{\bf 10} & r@{\bf 1\%} &\\
    & \multicolumn{4}{c}{(\%)} \\ \midrule
    & 9.99 & 16.72 & 21.95 & 66.58 & \\
    \bottomrule
    \end{tabular}
  }
  \label{table6}
\end{table}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig17.tiff.png}
  \caption{{\bf The effect of the size of the validation dataset on the r@\hlb{$k$} metric.}  Same model (EgoTR) with the same dataset (BDD-trajectories). The accuracy decreases as the size of the validation dataset increases.}
  \label{fig17}
\end{figure}

\FloatBarrier

\subsection*{\hlb{4.3} Plain naive history}
\hlb{The experiment in Fig.} \ref{fig18}, shows that the more \hlb{the naive history algorithm} looks back into the history of the journey, the more accuracy improves. The algorithm has no assumptions about the current state, and its computational cost is negligible compared to generating the distance array.

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig18.tiff.png}
  \caption{{\bf The effect of the number of steps we look back on the accuracy.} The accuracy converges to ~100\% after seven steps on our dataset.}
  \label{fig18}
\end{figure}
\FloatBarrier

\subsection*{\hlb{4.4} Naive history with weak prior}
\hlb{Fig.} \ref{fig19} shows that naive history with a weak prior with 2 steps lookback outperforms plain naive history with 5 steps lookback. And it only takes 3 steps for naive history with a prior to converge to 100\%, compared to 7 steps (\hlb{Fig.} \ref{fig18}) for the plain version. 

However promising this is, if the prior is wrong it can result in “trapping” the algorithm in the wrong state which will degrade the accuracy significantly, and this is the “naive” part of the naive history algorithm.

\hlb{The "trapping" can be avoided by running the algorithm for a few steps and then run a new instance of the alogrithm with a new prior and comparing their predcitions if they diverge this means one of the instance is trapped and the one most faithful to the GPS system should be picked and the other gets discarded. This can be scaled to a large number of instances by running them in parallel and then comparing their predictions. And compared to the ensemble model, this approach is more practical becuase it doesn't require running multiple models in parallel only the naive history algorithm which is cosnsits exelusively of shifting and multiplication.}

\hlb{The most promising thing about this algorithm is that it can be used in real-time applications and get attached to any cross-view models. For example, it outperforms the GAMa 2D-CNN network} \cite{bib60} \hlb{which takes 8 steps to get an accuracy of 83\% for the r@1\% on their proposed dataset which is based on BDD100K too. And requires minimal resources compared to it during deployment and no training at all.}

\begin{figure}[!ht]
  \includegraphics*[width=0.9\linewidth]{fig19.tiff.png}
  \caption{{\bf The effect of weak prior on naive history.}  The accuracy of naive history improves significantly when starting with some prior knowledge about the trip starting point. The brown line(naive history with prior) and the green line (plain naive history) represent the same number of steps into the trip though the brown line shows more accurate predictions due to factoring in the weak prior.}
  \label{fig19}
\end{figure}

\FloatBarrier

\section*{\hlb{5} Conclusion and future work}
\hlb{In this work, an ensemble model was created to merge the predictions of numerous cutting-edge models. The ensemble model accuracy surpassed the current state-of-the-art. The effect of factoring in trip temporal information was demonstrated. The naive history meta block was proposed, which converges to 100\% after a few steps. But none of the available benchmark datasets was appropriate for extensive temporal awareness experiments, so a new derivative dataset based on BDD100K got collected. That streamlined using this dataset to build an end-to-end model that exploits the temporal correlation during a single trip, and fuses other data modalities and sources during querying and training.} It's evident that there is a room for analyzing different state-of-art models to identify the most promising building modules and then use the network architecture search (NAS) paradigm to develop an optimal CV matching network. We anticipate that this study will kick-start the development of deployable cross-view geo-localization models, exploring fusing other data modalities and sources during querying and training. Moreover we believe there is a great gap for real-time, weatherproof models that can initiate many research points.
\nolinenumbers

\begin{thebibliography}{10}

  \bibitem{bib1}
  Ben-Moshe B, Elkin E, Levi H, Weissman A.
  \newblock Improving Accuracy of GNSS Devices in Urban Canyons.
  \newblock In: CCCG; 2011. p. 511--515.
  
  \bibitem{bib2}
  Zhai M, Bessinger Z, Workman S, Jacobs N.
  \newblock Predicting ground-level scene layout from aerial imagery.
  \newblock In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition; 2017. p. 867--875.
  
  \bibitem{bib3}
  Wang T, Zheng Z, Yan C, Zhang J, Sun Y, Zheng B, et~al.
  \newblock Each part matters: Local patterns facilitate cross-view
    geo-localization.
  \newblock IEEE Transactions on Circuits and Systems for Video Technology.
    2021;32(2):867--879.
  
  \bibitem{bib25}
  Zeng W, Wu M, Sun W, Xie S.
  \newblock Comprehensive review of autonomous taxi dispatching systems.
  \newblock Comput Sci. 2020;47(05):181--189.
  
  \bibitem{bib37}
  Vo NN, Hays J.
  \newblock Localizing and orienting street views using overhead imagery.
  \newblock In: European conference on computer vision. Springer; 2016. p.
    494--509.
  
  \bibitem{bib6}
  Churchill W, Newman P.
  \newblock Experience-based navigation for long-term localisation.
  \newblock The International Journal of Robotics Research.
    2013;32(14):1645--1661.
  
  \bibitem{bib7}
  Doan AD, Latif Y, Chin TJ, Liu Y, Ch’ng SF, Do TT, et~al.
  \newblock Visual localization under appearance change: filtering approaches.
  \newblock Neural Computing and Applications. 2021;33(13):7325--7338.
  
  \bibitem{bib8}
  Rodrigues R, Tani M.
  \newblock Are these from the same place? seeing the unseen in cross-view image
    geo-localization.
  \newblock In: Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision; 2021. p. 3753--3761.
  
  \bibitem{bib9}
  Doan AD, Latif Y, Chin TJ, Liu Y, Do TT, Reid I.
  \newblock Scalable place recognition under appearance change for autonomous
    driving.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 9319--9328.
  
  \bibitem{bib10}
  Milford MJ, Wyeth GF.
  \newblock SeqSLAM: Visual route-based navigation for sunny summer days and
    stormy winter nights.
  \newblock In: 2012 IEEE international conference on robotics and automation.
    IEEE; 2012. p. 1643--1649.
  
  \bibitem{bib4}
  Liu L, Li H.
  \newblock Lending orientation to neural networks for cross-view
    geo-localization.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2019. p. 5624--5633.
  
  \bibitem{bib20}
  Regmi K.
  \newblock Exploring Relationships Between Ground and Aerial Views by Synthesis
    and Matching. 2021;.
  
  \bibitem{bib21}
  Hu S, Lee GH.
  \newblock Image-based geo-localization using satellite imagery.
  \newblock International Journal of Computer Vision. 2020;128(5):1205--1219.
  
  \bibitem{bib22}
  Dixit D, Verma S, Tokekar P.
  \newblock Evaluation of Cross-View Matching to Improve Ground Vehicle
    Localization with Aerial Perception.
  \newblock arXiv preprint arXiv:200306515. 2020;.
  
  \bibitem{bib24}
  Regmi K, Shah M.
  \newblock Video geo-localization employing geo-temporal feature learning and
    gps trajectory smoothing.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2021. p. 12126--12135.
  
  \bibitem{bib55}
  Workman S, Souvenir R, Jacobs N.
  \newblock Wide-area image geolocalization with aerial reference imagery.
  \newblock In: Proceedings of the IEEE International Conference on Computer
    Vision; 2015. p. 3961--3969.
  
  \bibitem{bib5}
  Zhu S, Yang T, Chen C.
  \newblock Vigor: Cross-view image geo-localization beyond one-to-one retrieval.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2021. p. 3640--3649.
  
  \bibitem{bib12}
  Yu F, Chen H, Wang X, Xian W, Chen Y, Liu F, et~al.
  \newblock Bdd100k: A diverse driving dataset for heterogeneous multitask
    learning.
  \newblock In: Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition; 2020. p. 2636--2645.
  
  \bibitem{bib26}
  Zemene E, Tesfaye YT, Idrees H, Prati A, Pelillo M, Shah M.
  \newblock Large-scale image geo-localization using dominant sets.
  \newblock IEEE transactions on pattern analysis and machine intelligence.
    2018;41(1):148--161.
  
  \bibitem{bib32}
  Tian Y, Chen C, Shah M.
  \newblock Cross-view image matching for geo-localization in urban environments.
  \newblock In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition; 2017. p. 3608--3616.
  
  \bibitem{bib36}
  Hu S, Feng M, Nguyen RM, Lee GH.
  \newblock Cvm-net: Cross-view matching network for image-based ground-to-aerial
    geo-localization.
  \newblock In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition; 2018. p. 7258--7267.
  
  \bibitem{bib14}
  Wang J, Yang Y, Pan M, Zhang M, Zhu M, Fu M.
  \newblock Hybrid Perspective Mapping: Align Method for Cross-View Image-Based
    Geo-Localization.
  \newblock In: 2021 IEEE International Intelligent Transportation Systems
    Conference (ITSC). IEEE; 2021. p. 3040--3046.
  
  \bibitem{bib13}
  Shi Y, Yu X, Campbell D, Li H.
  \newblock Where am i looking at? joint location and orientation estimation by
    cross-view matching.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2020. p. 4064--4072.
  
  \bibitem{bib41}
  Samano N, Zhou M, Calway A.
  \newblock You are here: Geolocation by embedding maps and images.
  \newblock In: European Conference on Computer Vision. Springer; 2020. p.
    502--518.
  
  \bibitem{bib44}
  Shi Y, Yu X, Liu L, Zhang T, Li H.
  \newblock Optimal feature transport for cross-view image geo-localization.
  \newblock In: Proceedings of the AAAI Conference on Artificial Intelligence.
    vol.~34; 2020. p. 11990--11997.
  
  \bibitem{bib43}
  Zhu S, Yang T, Chen C.
  \newblock Revisiting street-to-aerial view image geo-localization and
    orientation estimation.
  \newblock In: Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision; 2021. p. 756--765.
  
  \bibitem{bib45}
  Cai S, Guo Y, Khan S, Hu J, Wen G.
  \newblock Ground-to-aerial image geo-localization with a hard exemplar
    reweighting triplet loss.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 8391--8400.
  
  \bibitem{bib46}
  Shi Y, Liu L, Yu X, Li H.
  \newblock Spatial-aware feature aggregation for image based cross-view
    geo-localization.
  \newblock Advances in Neural Information Processing Systems. 2019;32.
  
  \bibitem{bib47}
  Xia Z, Booij O, Manfredi M, Kooij JF.
  \newblock Geographically local representation learning with a spatial prior for
    visual localization.
  \newblock In: European Conference on Computer Vision. Springer; 2020. p.
    557--573.
  
  \bibitem{bib17}
  Xia Z, Booij O, Manfredi M, Kooij JF.
  \newblock Cross-View Matching for Vehicle Localization by Learning
    Geographically Local Representations.
  \newblock IEEE Robotics and Automation Letters. 2021;6(3):5921--5928.
  
  \bibitem{bib18}
  Toker A, Zhou Q, Maximov M, Leal-Taix{\'e} L.
  \newblock Coming down to earth: Satellite-to-street view synthesis for
    geo-localization.
  \newblock In: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition; 2021. p. 6488--6497.
  
  \bibitem{bib19}
  Yang H, Lu X, Zhu Y.
  \newblock Cross-view geo-localization with evolving transformer.
  \newblock arXiv preprint arXiv:210700842. 2021;.
  
  \bibitem{bib49}
  Li S, Tu Z, Chen Y, Yu T.
  \newblock Multi-scale attention encoder for street-to-aerial image
    geo-localization.
  \newblock CAAI Transactions on Intelligence Technology. 2022;.
  
  \bibitem{bib50}
  Regmi K, Shah M.
  \newblock Bridging the domain gap for ground-to-aerial image matching.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 470--479.
  
  \bibitem{bib27}
  Lowe DG.
  \newblock Distinctive image features from scale-invariant keypoints.
  \newblock International journal of computer vision. 2004;60(2):91--110.
  
  \bibitem{bib28}
  Bay H, Tuytelaars T, Gool LV.
  \newblock Surf: Speeded up robust features.
  \newblock In: European conference on computer vision. Springer; 2006. p.
    404--417.
  
  \bibitem{bib29}
  Alahi A, Ortiz R, Vandergheynst P.
  \newblock Freak: Fast retina keypoint.
  \newblock In: 2012 IEEE conference on computer vision and pattern recognition.
    Ieee; 2012. p. 510--517.
  
  \bibitem{bib30}
  Bosch A, Zisserman A, Munoz X.
  \newblock Image Classification using Random Forests and Ferns.
  \newblock In: 2007 IEEE 11th International Conference on Computer Vision; 2007.
    p. 1--8.
  
  \bibitem{bib31}
  Simonyan K, Zisserman A.
  \newblock Very deep convolutional networks for large-scale image recognition.
  \newblock arXiv preprint arXiv:14091556. 2014;.
  
  \bibitem{bib33}
  Ren S, He K, Girshick R, Sun J.
  \newblock Faster r-cnn: Towards real-time object detection with region proposal
    networks.
  \newblock Advances in neural information processing systems. 2015;28.
  
  \bibitem{bib34}
  Krizhevsky A, Sutskever I, Hinton GE.
  \newblock Imagenet classification with deep convolutional neural networks.
  \newblock Communications of the ACM. 2017;60(6):84--90.
  
  \bibitem{bib35}
  Chopra S, Hadsell R, LeCun Y.
  \newblock Learning a similarity metric discriminatively, with application to
    face verification.
  \newblock In: 2005 IEEE Computer Society Conference on Computer Vision and
    Pattern Recognition (CVPR'05). vol.~1. IEEE; 2005. p. 539--546.
  
  \bibitem{bib38}
  He K, Zhang X, Ren S, Sun J.
  \newblock Deep residual learning for image recognition.
  \newblock In: Proceedings of the IEEE conference on computer vision and pattern
    recognition; 2016. p. 770--778.
  
  \bibitem{bib39}
  Huang G, Liu Z, Van Der~Maaten L, Weinberger KQ.
  \newblock Densely connected convolutional networks.
  \newblock In: Proceedings of the IEEE conference on computer vision and pattern
    recognition; 2017. p. 4700--4708.
  
  \bibitem{bib40}
  Chollet F.
  \newblock Xception: Deep learning with depthwise separable convolutions.
  \newblock In: Proceedings of the IEEE conference on computer vision and pattern
    recognition; 2017. p. 1251--1258.
  
  \bibitem{bib42}
  Ronneberger O, Fischer P, Brox T.
  \newblock U-net: Convolutional networks for biomedical image segmentation.
  \newblock In: International Conference on Medical image computing and
    computer-assisted intervention. Springer; 2015. p. 234--241.
  
  \bibitem{bib48}
  Woo S, Park J, Lee JY, Kweon IS.
  \newblock Cbam: Convolutional block attention module.
  \newblock In: Proceedings of the European conference on computer vision (ECCV);
    2018. p. 3--19.
  
  \bibitem{bib51}
  Regmi K, Borji A.
  \newblock Cross-view image synthesis using conditional gans.
  \newblock In: Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition; 2018. p. 3501--3510.
  
  \bibitem{bib52}
  Canny J.
  \newblock A computational approach to edge detection.
  \newblock IEEE Transactions on pattern analysis and machine intelligence.
    1986;(6):679--698.
  
  \bibitem{bib54}
  Doan AD, Latif Y, Chin TJ, Liu Y, Do TT, Reid I.
  \newblock Scalable place recognition under appearance change for autonomous
    driving.
  \newblock In: Proceedings of the IEEE/CVF International Conference on Computer
    Vision; 2019. p. 9319--9328.
  
  \bibitem{bib23}
  FFmpeg.org;.
  \newblock Available from: \url{https://ffmpeg.org/}.
  
  \bibitem{bib16}
  {Wikipedia contributors}. Great-circle distance; 2022.
  \newblock Available from:
    \url{https://en.wikipedia.org/wiki/Great-circle_distance}.
  
  \bibitem{bib15}
  Maps Static API;.
  \newblock Available from:
    \url{https://developers.google.com/maps/documentation/maps-static/overview}.
  
  \bibitem{bib11}
  Alazzawi A.
  \newblock EDGE DETECTION-APPLICATION OF (FIRST AND SECOND) ORDER DERIVATIVE IN
    IMAGE PROCESSING: Communication.
  \newblock Diyala Journal of Engineering Sciences. 2015;8(4):430--440.

  \bibitem{bib60}
  Vyas S, Chen C, Shah M.
  \newblock GAMa: Cross-view Video Geo-localization.
  \newblock In: European Conference on Computer Vision.   Springer; 2022. p.
    440--456.
\end{thebibliography}
\end{document}
