% Template for PLoS
% Version 3.6 Aug 2022
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage{booktabs, makecell}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{ {./figures/png/} }
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}
\usepackage[linesnumbered]{algorithm2e}
\usepackage[section]{placeins}

\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}


% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Leveraging cross-view geo-localization with ensemble learning and temporal awareness} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Abdulrahman Ghanem\textsuperscript{1},
Ahmed Abdelhay\textsuperscript{1},
Noor Eldeen Salah\textsuperscript{1},
Ahmed Nour Eldeen1\textsuperscript{1},
Mohammed Elhenawy\textsuperscript{2},
Abdallah A. Hassan\textsuperscript{1},
Ammar M. Hassan\textsuperscript{3*},
Mahmoud Masoud\textsuperscript{2}
\\
\bigskip
\textbf{1} Computer and Systems Engineering Department, Faculty of Engineering, Minia University, Minia, Egypt
\\
\textbf{2}  Centre for Accident Research and Road Safety-Queensland (CARRS-Q), Queensland University of Technology, Brisbane, Australia
\\
\textbf{3} Arab Academy for Science, Technology, and Maritime Transport, South Valley Branch, Aswan, Egypt
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% % Additional Equal Contribution Note
% % Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% % Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% % \textcurrency b Insert second current address 
% % \textcurrency c Insert third current address

% % Deceased author note
% \dag Deceased

% % Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* ammar@aast.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
In some situations the Global Navigation Satellite System (GNSS) is unreliable. To mend the poor GNSS signal, an autonomous vehicle can self-localize by matching a ground image against a database of geotagged aerial images. It is easy to construct an aerial dataset, so this approach is preferable to other image-based localization techniques. But, this approach is challenging: there are dramatic viewpoint changes between aerial and ground views, harsh weather and lighting conditions, and lack of orientation information, in training and deployment environments. In this paper, we show that previous models in this area are complementary, not competitive, and that each model solves a different aspect of the problem. We propose an ensemble model to aggregate the predictions of multiple independently trained state-of-the-art models. Also, we show the effect of making the query process temporal-aware  But none of the existing benchmark datasets is suitable for extensive temporal awareness experiments, we generated a new derivative dataset based on the BDD100K dataset. This paper also proposes an efficient temporal awareness mechanism: Naive History, to prove that fusing the temporal information of the trip with the model prediction significantly improves the accuracy. Our ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset (surpassing the current state-of-the-art). Our temporal awareness mechanism converges to R@1 of ~100\% by looking at a few steps back in the trip history. We expect this study to initialize the building of deployable ensemble cross-view geo-localization models that take advantage of temporal proximity between ground images.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
% \section*{Author summary}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
The current standard localization technique is the global navigation satellite system (GNSS). Although the GNSS accuracy declines in cases where there are few lines of sight (e.g., urban canyons~\cite{bib1}). Using cross-view geo-localization, a vehicle localizes itself by matching street view images against a database of geotagged images captured from aerial platforms (e.g., a satellite~\cite{bib2} or a drone~\cite{bib3}). Cross-view geo-localization is gaining popularity in the scene of autonomous vehicles~\cite{bib2} and robotic navigation~\cite{bib25}: it compensates for a bad GNSS signal-to-noise ratio. And,  it’s preferable to other image-based localization techniques (e.g., landmark and ground-to-ground matching) for the ease of covering new areas. Early work~\cite{bib37} on this technique claimed that its main challenge is the lack of visual correspondence between aerial and ground views. Later work, while holding the same claim, showed that there are more challenges: geographic scene changes over time~\cite{bib6,bib7,bib8,bib9,bib10}, poor weather and lighting conditions, lack of orientation information~\cite{bib4}, and misalignment between aerial and ground images during training.
None of the previous models tried to collectively address these five challenges. To build a holistic solution for the problem, we propose an ensemble model to fuse the predictions of five independently trained models. Each of these models addresses a different challenge..
Moreover, most of the recent work treats the problem as a 1-to-1 image-matching task. This overlooks the fact that these models get deployed in environments where there's a continuous stream of input, not a single query image. Taking the temporal nature of the problem into account is a must:  our experiments showed that the recent models can't differentiate between highly similar query images, and in the case of a moving vehicle, the consecutive images have a high degree of similarity. Some recent models considered the temporal nature of the problem.~\cite{bib7}  and~\cite{bib20,bib21,bib22} used Markov chain Monte Carlo (MCMC) algorithms to predict the current pose and enforce temporal consistency.~\cite{bib24} enforces temporal consistency by using a transformer-based trajectory smoothing network. We can see that these methods are resource intensive or have strong assumptions about the current state of the vehicle. We explore an efficient technique to achieve the same goal.
Although the existing datasets aren't suitable for conducting experiments to prove that temporal awareness improves accuracy. The dataset needs to be realistic and collected as a trajectory of close points over a long-running journey to resemble driving in a real environment.  CVUSA~\cite{bib55} and CVACT~\cite{bib4} include sparse points on the map. VIGOR~\cite{bib5} includes dense points but doesn't form trajectories. RobotCar~\cite{bib6} has a limited number of examples. So we had to introduce a new derivative dataset based on the BDD100K~\cite{bib12} dataset to fill this gap.

\subsection*{In summary, the contributions of this paper are:}
\begin{itemize}
    \item We introduce a new ensemble model based on five of the current state-of-the-art  cross-view matching networks. The ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset.
    \item We construct a new derivative dataset that is suitable for temporal aware cross-view geo-localization models based on BDD100K.
    \item We develope a meta block: Naive History, to make the query process temporal aware. We show that taking journey history into account minimizes the search space. This reduction in search space improves the accuracy.  The accuracy converges to ~100\% with a three-step lookback into trip history on our proposed dataset. This meta block is usable with any CV model.
\end{itemize}

The rest of the paper is organized as follows. Section II presents the related work. Section III discusses the details of the methodology and the experimental work. We conclude the paper in Section IV.

\section*{Related work}
In section (A), we start by investigating how different models engineered their features and architectures. Their choices show us how different models tried to approach the problem from different angles. Followed by giving a bird-eye-view of the architectures of these models and feature extraction methods. In section (B), we walk through the models that tried blending the trip history into the image-matching task. By grouping the models into two categories: one that relies on "this place looks familiar" in section (B.1), and one that relies on "where have we been before getting here?" in section (B.2).
\subsection*{A) Features and architectures}
Most of the recent work treated the cross-view geo-localization task as an image retrieval task. They tried to find a feature representation suitable for matching query ground images and aerial ground images. The nature and complexity of used networks changed over time. Here, we conduct a brief comparison between the most common feature representations and their architectures. Table~\ref{table1} summarizes the feature types and the backbones of different cross-view geo-localization networks.


\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
  \centering
  \caption{
  {\bf A summary of the feature types and the backbones of different cross-view geo-localization networks.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Feature type} & {\bf Backbone} & {\bf Used in}&  \\ \midrule
    & Hand-crafted & SIFT & ~\cite{bib20}~\cite{bib26}& \\
    & Hand-crafted & SURF, FREAK, PHOW & ~\cite{bib20}& \\
    & Hand-crafted & SIFT + VLAD & ~\cite{bib7}& \\
    & Semantic & Faster R-CNN & ~\cite{bib32}& \\
    & CNN & VGG + FCN + NetVLAD & ~\cite{bib36}~\cite{bib22}~\cite{bib14}& \\
    & CNN & AlexNet & ~\cite{bib37}~\cite{bib32}& \\
    & CNN & VGG & ~\cite{bib21}~\cite{bib4}~\cite{bib13}& \\
    & CNN & ResNet & ~\cite{bib4}~\cite{bib21}~\cite{bib41}& \\
    & CNN & DenseNet & ~\cite{bib4}~\cite{bib21}& \\
    & CNN & U-net & ~\cite{bib4}& \\
    & CNN & Xception & ~\cite{bib21}& \\
    & CNN & - & ~\cite{bib44}& \\
    & CNN & VGG + FCN & ~\cite{bib43}& \\
    & Attentive & Siam-FCANet (ResNet + FCAM + FCN) & ~\cite{bib45}& \\
    & Attentive & Siam-VFCNet (ResNet + FCAM + NetVLAD) & ~\cite{bib45}& \\
    & Attentive & VGG + SAFA + SPE & ~\cite{bib46}~\cite{bib47}~\cite{bib17}& \\
    & Attentive & VGG + Geo Attention + Geo-temporal Attention & ~\cite{bib24}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib8}& \\
    & Attentive & SAFA & ~\cite{bib5}& \\
    & Attentive & ResNet + SAFA & ~\cite{bib18}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib19}& \\
    & Attentive & VGG + MSAE & ~\cite{bib49}& \\
    & Synthesized & X-Fork & ~\cite{bib50}& \\ \bottomrule
    \end{tabular}
  }
  \label{table1}
\end{table}

\subsubsection*{1) Hand-crafted features}
Early research used hand-crafted features.~\cite{bib20,bib26} used SIFT~\cite{bib27}. Also,~\cite{bib20} experimented with other feature spaces (e.g., SURF~\cite{bib28}, FREAK~\cite{bib29}, PHOW~\cite{bib30}) but SIFT outperformed others.~\cite{bib7} computed dense SIFT features, then embedded them into a higher dimensional vector space using a VLAD~\cite{bib31}. The extracted features are brittle; it fails to adapt to appearance change. Later, it proved to have inferior performance compared to the CNN-based features.
\subsubsection*{2) Semantic features}
In this approach, the networks matched between ground and aerial images based on the meaningful content of the image. This made it more robust to viewpoint changes than local features. Though, the model performance degraded in areas lacking the pre-selected semantic features.~\cite{bib32} treated the problem as object detection and recognition: the first block of the architecture employed the Faster R-CNN~\cite{bib33} to detect buildings, and the second block of the architecture used AlexNet~\cite{bib34} with the Siamese architecture~\cite{bib35} to recognize the buildings.
\subsubsection*{3) CNN-based features}
Metric learning achieved promising results in bridging the domain gap between aerial and ground image representation.~\cite{bib36} used a fully convolutional network (FCN) with a NetVLAD~\cite{bib34} layer using a Siamese architecture.~\cite{bib37} tried modified versions of AlexNet.~\cite{bib21} experimented with different FCN layers: VGG~\cite{bib31}, ResNet~\cite{bib38}, DenseNet~\cite{bib39}, and Xception~\cite{bib40} with the same architecture of~\cite{bib36}, they found that VGG outperforms other networks. Also,~\cite{bib22} used the CVM-Net-I architecture proposed in~\cite{bib36}.~\cite{bib41} exploited a modified ResNet50 network for ground images and a ResNet18 for aerial ones.~\cite{bib13} used VGG16 to generate the feature maps for the polar transformed aerial and then fed it into the Dynamic Similarity Module (DSM).~\cite{bib4} learned orientation information by using different backbones (VGG, ResNet, DenseNet, and U-net~\cite{bib42}) with the Siamese architecture.~\cite{bib43} employed  the Siamese network with a VGG backbone to extract feature maps, then a fully connected layer aggregates these feature maps.~\cite{bib44} used a CNN to generate feature maps and then transform them from the ground domain to the aerial domain.~\cite{bib14} applied the hybrid perspective mapping method using the CVM-Net-I architecture.
\subsubsection*{4) Attentive features}
For this feature type, the networks used spatial attention to enhance the feature representation.~\cite{bib45} integrated the lightweight attention module (FCAM) into each block of the basic ResNet.~\cite{bib46} used a spatial-aware feature aggregation (SAFA) module to mitigate the distortion in the aerial image. Also, employed the spatial-aware position embedding module (SPE) to encode relative positions among features in the feature maps.~\cite{bib5} proposed the VIGOR network built on top of SAFA.~\cite{bib47,bib17} used the same architecture as~\cite{bib46} with a different loss function: geo-distance weighted loss.~\cite{bib24} proposed a geo-attention module for the aerial branch, and a temporal-attention module for the ground branch.~\cite{bib8} used convolutional block attention modules~\cite{bib48} to generate multi-scale attention features.~\cite{bib18} employed a modified ResNet34 backbone with a spatial-aware attention module.~\cite{bib19} proposed EgoTR network. EgoTR used a ResNet backbone transformer encoder with a self-cross attention mechanism.~\cite{bib49} introduced the Multi-Scale Attention Encoder (MSAE). MSAE employed a VGG backbone with a multi-scale attention encoder followed by FCN to generate feature masks.

\subsubsection*{5) Synthetic features}
In this approach, the networks learned robust feature representation by reversing the task: it learned how to create ground view from aerial views, which made it learn salient features and suppress others.~\cite{bib50} synthesized aerial representation of a ground panorama query using the X-Fork network~\cite{bib51} with edge maps detection by Canny Edge Detection~\cite{bib52}.

\subsection*{B) Contextual awareness}
The fact that these models get deployed in autonomous vehicles, makes it obvious that the models should be aware of the trip context. We can categorize contextual awareness as follows:

\subsubsection*{1) Spatial awareness}
We have to differentiate between two types of spatial awareness.

\begin{itemize}
  \item Some models refer to it as the knowledge about the pose (location and orientation) of the query image and its relative pose to the aerial image frame.
  \cite{bib47,bib17} constructed mini-batches of images within a certain geographic radius and used a modified version of the triplet loss function: Geo-distance weighted loss to favor examples where the images are within the selected radius. The Geo-Attention module used in~\cite{bib24} exploits a similar loss function.~\cite{bib14} employed hybrid perspective mapping to establish correspondence between ground and aerial images.~\cite{bib4} injected the orientation information into the network.~\cite{bib4} used multiplane image (MPI)~\cite{bib13} projections to exploit geometric correspondence between ground and aerial images. Table~\ref{table2} gives an overview of spatial (pose-wise) awareness approaches used in cross-view geo-localization.
  \begin{table}[!ht]
    \centering
    \caption{
    {\bf An overview of the spatial (pose-wise) awareness approaches used in cross-view geo-localization.}}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Geographic proximity & ~\cite{bib47}~\cite{bib17}~\cite{bib24}& \\
    & Polar Transform & ~\cite{bib3,bib4}~\cite{bib5}~\cite{bib13,bib14}~\cite{bib17,bib18,bib19}& \\
    & Inverse Polar Transform & ~\cite{bib49}& \\
    & Orientation & ~\cite{bib4}& \\
    & Dynamic Similarity Matching (DSM) & ~\cite{bib13}& \\
    & Hybrid Perspective Mapping & ~\cite{bib14}& \\ \bottomrule
    \end{tabular}
    \label{table2}
  \end{table}
  \item Other networks refer to it as paying more attention to the salient features, suppressing less important features, and encoding the spatial layout information into the feature representation. We have covered that in section (A.4).
\end{itemize}

\subsubsection*{2) Temporal awareness}
There are two types of temporal awareness too!
\begin{itemize}
  \item Finding a robust feature representation that won’t be affected by scene changes throughout time. These changes can be geographic landmarks (e.g., new constructions) or environmental conditions such as weather conditions.
  ~\cite{bib6,bib54} used time-invariant approaches by capturing the same scene during different conditions across time. But, this technique required a dataset where the same scene is covered during different conditions. Possible solutions for this are as follows. {\bf A)}~\cite{bib8} used semantic object-based data augmentation techniques to remove and add objects (cars, roads, greenery, and sky). {\bf B)}~\cite{bib7} applied PCA projection to make background features less significant in the image descriptor. {\bf C)} Another solution is using synthetic datasets.
  \item Exploiting the fact these models will be deployed on vehicles where a temporally coherent sequence of images is available.
  ~\cite{bib7} and~\cite{bib20,bib21,bib22} used MCMC algorithms, namely, a particle filter~\cite{bib45} with variations of initialization and transition techniques, the algorithms are used to predict current pose and enforce temporal consistency.~\cite{bib24} introduced a geo-temporal attention module, the module attends to all frames in the video to learn better features, it also enforces temporal consistency by using a transformer-based trajectory smoothing network.
\end{itemize}
We give an overview of the temporal awareness approaches used in cross-view geo-localization in Table~\ref{table3}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf An overview of the temporal awareness approaches used in cross-view geo-localization.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Capture multiple examples with different environmental conditions & ~\cite{bib6}~\cite{bib54}& \\
    & Semantic object-based data augmentation & ~\cite{bib8}& \\
    & Observation encoder + PCA projection & ~\cite{bib7}& \\
    & MCMC algorithms & ~\cite{bib7}~\cite{bib20,bib21,bib22}& \\
    & Sequence attention + Trajectory Smoothing Network & ~\cite{bib24}& \\ \bottomrule
    \end{tabular}
  }
  \label{table3}
\end{table}

\section*{Methodology}
\subsection*{A) Evaluation metric}
In this research, we use the same evaluation metric used in~\cite{bib3,bib4,bib5,bib8,bib13,bib14,bib17,bib18,bib19,bib20,bib21,bib22,bib24,bib36,bib43,bib44,bib45,bib46,bib47,bib49,bib50}, namely, the Recall@k (r@k). In r@k, we consider it a match if the corresponding aerial image is in the top k predictions. We use r@1, r@5, r@10, and r@1\%. r@1 means the true image is the first prediction of the model, r@5 means the true image is in the first five predictions of the model, and so on.

\subsection*{B) The ensemble model}

There are many challenges in cross-view matching. To name a few , missing correspondence and orientation information, scene changes over time, and the high similarity among geographically close points. Recently, several models were developed to solve the cross-view (CV) matching problem, and different models approached the CV matching problem from different angles. Thus, each model has its advantages and disadvantages. We hypothesize that aggregating the outputs of these uncorrelated models might improve the accuracy.
Therefore, in this research, we build an ensemble of models trained independently to solve the CV matching problem. The proposed ensemble used the same datasets to train different neural network architectures. We used the CVUSA~\cite{bib55} and CVACT~\cite{bib4} benchmark datasets, to train five models Each model addresses a  different aspect of the problem: DSM~\cite{bib13} estimates the cross-view orientation alignment.  EgoTR~\cite{bib19} models global dependencies to decrease visual ambiguities and matches geometric configuration between ground and aerial images. SAFA~\cite{bib46} exploits geometric correspondence between aerial and panoramic ground images. Toker~\cite{bib18} biases the localization network via a Generative Adversarial Network (GAN) to learn salient features. SFCANet~\cite{bib45} uses Hard Exemplar Reweighting to assign a greater weight to hard examples. Table~\ref{table4} shows their r@k metrics.

\begin{table}[!ht]
\begin{adjustwidth}{-1.0in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
  \centering
  \caption{
    {\bf The r@k metrics for the networks used to construct the  ensemble model.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllllcrrrrrrr@{}}
      \toprule
      & \multirow{3}{*}{\bf Network} & \multicolumn{4}{c}{\bf CVUSA} & \phantom{abc}& \multicolumn{4}{c}{\bf CVACT} & \phantom{abc} & \\
      \cmidrule{3-6} \cmidrule{8-11}
      & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \\
      &\phantom{abc} & \multicolumn{9}{c}{(\%)}  \\ \bottomrule
      & DSM~\cite{bib13} & 92.07 & 97.33 & 98.33 & 99.60 & \phantom{abc} & 81.93 & 91.83 & 93.95 &  97.42& \\
      & Toker~\cite{bib18} & 92.15 & 97.28 & 98.26 &  99.56 & \phantom{abc}  & 83.52 & 93.89 & 95.48 &  98.17& \\
      & EgoTR~\cite{bib19} & 93.87 & 98.22 & 98.98 & 99.66 & \phantom{abc} & 84.87 & 94.5 & 95.95 &  98.37& \\
      & SFCANet~\cite{bib45} & 51.01 & 78.92 & 86.80 & 98.49 & \phantom{abc} & x & x & x &  x& \\ 
      & SAFA~\cite{bib46} & 88.93 & 96.65 & 97.97 & 99.65 & \phantom{abc} & 74.56 & 89.65 & 92.46 &  97.72& \\ \bottomrule
    \end{tabular}
  }
  \label{table4}
  \end{adjustwidth}
\end{table}

This research investigates two different aggregation methods: soft-voting, and hard-voting. As shown in Fig~\ref{fig1}, for both methods, we try all possible  combinations of the models (their power set).

\begin{figure}[!h]
\caption{{\bf Different ensemble aggregation methods.}}
\includegraphics[width=0.9\linewidth]{fig1.tiff.png}
\label{fig1}
\end{figure}

In soft-voting, we experimented with two calculation methods. a) Averaging the predictions of the individual models. b) Calculating the joint probability, using Eq~(\ref{eq1}). Both methods have identical results.

\begin{eqnarray}
\label{eq1}
\text{total vote} = \exp(\log(vote_1) + \log(vote_2) + ..., \log(vote_n)),
\end{eqnarray}

In hard-voting, we selected the majority vote of the models. Hard-voting needs at least three models to have a majority vote. There are two cases where a majority vote doesn’t exist:
\begin{enumerate}
  \item Combinations with an even number of models can tie, so we pick the combination containing the most accurate model.
  \item All models’ predictions differ, so we tried two strategies. {\bf A)} take the prediction of the most accurate model in the combination. {\bf B)} pick a prediction from a random normal distribution of individual models' predictions.
\end{enumerate}

\subsection*{B) BDD trajectories dataset collection}
Proposed approaches in this research exploit the temporal nature of the problem. The existing benchmark datasets (e.g, CVUSA, and CVACT) are collected from sparse points on the map while we need a trajectory of close points. Inspired by the work of Regmi and Shah~\cite{bib24}, we constructed a new derivative dataset from the BDD100k~\cite{bib12} dataset. The BDD100K dataset is crowdsourced, diverse, and large-scale, with IMU/GPS recording, and other semantic annotations (irrelevant to this research). All videos in the dataset are  long, though the total distance varies. We chose the videos with a distance greater than 50 m, the statistical summary of the distance covered in the selected videos is in Table~\ref{table5}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf A statistical summary of the distance covered in selected videos.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
    & {\bf Count} & {\bf Mean} & {\bf \emph{std}} & {\bf Min} & {\bf Max} &\\ \midrule
    & 47943 videos & 278.668 m & 181.786 m & 50.000 m & 2560.000 m \\ \bottomrule
    \end{tabular}
  }
  \label{table5}
\end{table}

Our dataset consists of 95,000 examples. Each example consists of five ground images and one aerial image, and some examples are shown in Fig~\ref{fig2}. We sampled the ground images from the picked videos with a sampling rate of $1 frame/10 m$ to have some visual changes between the consecutive frames, but at the same time, the frames still look relatable to one another. The IMU/GPS data is captured at $1 sample/s$. The distance moved in one second varies; the speed of the vehicles is not constant. In our dataset, we care about the visual changes, not the passage of time. So when the distance between every two consecutive locations is greater than $10 \ m$,  we use following the sampling algorithm:

\begin{algorithm}[!h]
\caption{BDD100K resampling}
\KwData{The GPS/IMU information recorded along with the videos, BDD100K videos.}
\KwResult{Resmapled frames(1 frame/10 m)}
\Comment{ Get the speed at the current ($v_n$), and next ($v_{n+1}$) location from IMU data.
\
Assume the speed between these two locations ($v_{n \to n+1}$) is the average speed of both locations. For all locations on the trajectory which is a multiple of the sampling distance parameter ($d$):}\
$v_{\text{n} \to \text{n+1}} \gets \frac{v_{\text{n}} + v_{\text{n} + 1}}{2}, n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \}$ \;

\Comment{To get the timestamp of the nth frame ($t_{\text{n}}$)}

$t \gets \frac{\text{d} \times  \text{n}}{v_{\text{n}} + v_{\text{n} + 1}} , n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \} $;\

Extract the frames at the selected timestamps using FFmpeg~\cite{bib23};\
\end{algorithm}

\begin{figure}[!h]
  \caption{{\bf Three examples from our dataset. For ground images, the time progresses from left to right.}}
  \includegraphics[width=0.9\linewidth]{fig2.tiff.png}
  \label{fig2}
\end{figure}

We captured the aerial tiles at the midpoint of the example using the great circle algorithm~\cite{bib16} and then fetched them from Google Maps~\cite{bib15}. We experimented with different zoom levels, Fig~\ref{fig3}. Furthermore, we chose the 20th zoom level; it covers a wide area with great detail. We chose a tile size of  $800 \times 800$ as shown in Fig~\ref{fig4}.

\begin{figure}[!h]
  \caption{{\bf Examples of three different zoom levels for aerial tile. Markers are 10 m apart on the trajectory.}}
  \includegraphics[width=0.9\linewidth]{fig3.tiff.png}
  \label{fig3}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example of different tile sizes for the same zoom level.}}
  \includegraphics[width=0.9\linewidth]{fig4.tiff.png}
  \label{fig4}
\end{figure}

After extracting the frames, we removed trajectories where 30\% of the extracted ground frames are mildly lit ($44.5\%$of all trajectories). For example, we take the trajectory of the Bright frame in Fig~\ref{fig5} and drop the other two trajectories. Then we removed the trajectories that have blurry aerial tiles ($13.5\%$ of the bright trajectories) similar to the example shown in Fig~\ref{fig6}. We used a Laplacian filter~\cite{bib11} with a threshold of  to detect blurry aerial tiles, and a grayscale mean filter with a threshold of $200$ to detect dark ground frames. We chose both thresholds empirically to drop all the true positive corrupt examples with some false positives. Fig~{fig7} shows a simplified version of our data cleaning pipeline. We scaled the ground and aerial images’ width to $400$ while keeping the height in aspect ratio using the Lanczos algorithm.

\begin{figure}[!h]
  \caption{{\bf Examples of different lighting conditions in the BDD100K dataset.}}
  \includegraphics[width=0.9\linewidth]{fig5.tiff.png}
  \label{fig5}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example of a blurry aerial image.}}
  \includegraphics[width=0.9\linewidth]{fig6.tiff.png}
  \label{fig6}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf A simplified version of our data cleaning pipeline.}}
  % \includegraphics[width=0.9\linewidth]{fig6.tiff.png}
  \label{fig7}
\end{figure}
\FloatBarrier

\subsection*{C) Naive history}
Inspired by our experiments on the joint probability as a soft-voting strategy for ensemble learning. In a journey setting where the history of the journey is available, we hypothesize that taking the previous predictions into account might cause the prediction to converge while progressing in the journey. This hypothesis relies on the fact that even if the model doesn't return the true location as its top-1 prediction, most of the time, it is still in the top-1\% predictions. Also, the probability that a model will predict N consecutive wrong predictions decreases as N increases. Based on this, we fine-tuned the EgoTR model on the proposed BDD-trajectories dataset. We used this dataset because it has trip trajectories.

\subsubsection*{Fine-tuning EgoTR}
The EgoTR model takes a pair of images as input: a ground image and a satellite image as its input. However, an example in our dataset consists of a hexad: five ground images and a satellite image. So we had to reshape our dataset to be suitable for the EgoTR input format. We chose the examples where there are at least two examples from the same journey, so we have 10 ground images from the same trajectory. We paired every ground image and the satellite image in the examples. This means that each example in the original dataset corresponds to five examples in the reshaped dataset. We used 19015 pairs for validation and the rest of the examples for training.

\subsubsection*{Using naive history with EgoTR}
After fine-tuning EgoTR we generated a distance array for all the images in our validation dataset. Then we used this array as input for Algorithm~\ref{alg:two}.

\begin{algorithm}[H]
  \label{algorithm2}
  \caption{Naive history}\label{alg:two}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$}
  \KwResult{History reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  \Return $m_{ij}$\;
\end{algorithm}


The naive history meta block reinforces the prediction at the current position by looking back into the trip history. The $distanceArray$ parameter is the distance between every ground and aerial image (the output of the model).  The $historyDepth$ parameter controls how deep we look back into history. We shift the total distance array by $historyDepth$ columns and rows to get the distance array at the previous location (Algorithm 2, Line 6). We get a shifted version of the $distanceArray$ to leave the predictions at future steps intact (Algorithm 2, Line 7).  We multiply, element-wise ($\odot$), the distance array at the current step and the distance array at the previous step (Algorithm 2, Line 8). We update the original array with the reinforced predictions (Algorithm 2, Line 9). Repeat the steps 4 through 7 (Algorithm 2, Line 4), with increasing value of $historyDepth$ until reaching the value of final history depth value, for example, if we want to lookback five steps in trip history we will iterate over historyDepth  \{1, 2, 3, 4, 5\}.

\FloatBarrier

\subsubsection*{The effect of prior on naive history}
As mentioned before the cross-view models are complementary to existing GNSS, so we can improve naive history performance by initializing it with a weak prior (the location captured by the GNSS).  We used Algorithm~\ref{alg:three} to initialize the distance array (generated by the model) with a probability of the first image in each example equal to 1e-6. In other words, (Algorithm 3, Lines 5-9) the probability of the first image in the trajectory is modified to make the probability of the ground truth image equal 1e-6 and set other probabilities to a uniform value of $(1 - (1\mathrm{e}{-6}) / distance array size)$. Our experiments prove that initializing the naive history algorithm with this prior knowledge speeds up the convergence to 100\% accuracy significantly.

\begin{algorithm}[H]
  \caption{Naive history with a weak prior}\label{alg:three}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$, $priori \in R_{+}$, $trajectorySize  \in N$}
  \KwResult{Prior-aware history reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  $normalizer \gets priori/ len$\;
  
  \For{$k \in [0, len] \land k\ \% \ trajectorySize \equiv 0$} {
      $currentPrediction_{ij} \gets (m_{ij})_{\substack{i\\ j \equiv k }}$\;
      $ (currentPrediction_{ij})_{\substack{i \equiv k}} \gets priori$\;
      $(m_{ij})_{\substack{i\\ j \equiv k }} \gets currentPrediction_{ij} - normalizer$\;
  }
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  
  \Return $m_{ij}$\;
  \end{algorithm}

  \FloatBarrier

\section*{Results}
\subsection*{A) The ensemble model}
Fig~\ref{fig8} and Fig~\ref{fig9} show the results of the soft-voting strategies for CVUSA and CVACT, respectively. The DSM, EgoTR, and Toker combination outperforms other combinations. Increasing the number of the models doesn’t necessarily improve the accuracy; to improve the accuracy; individual models have to predict different examples correctly.

\begin{figure}[!h]
  \caption{{\bf CVUSA combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations.}
  \includegraphics[width=0.9\linewidth]{fig8.tiff.png}
  \label{fig8}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVACT combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations. }
  
  \includegraphics[width=0.9\linewidth]{fig9.tiff.png}
  \label{fig9}
\end{figure}

Fig\ref{fig10} and Fig~{fig11} show the results of hard-voting with the most accurate model prediction strategy for CVUSA and CVACT, respectively. The accuracy drops about 2\% for the r@1 and r@5 metrics. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. Fig~\ref{fig12} and Fig~\ref{fig13} show the results of the hard-voting with the random selection strategy for CVUSA and CVACT, respectively. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.


\begin{figure}[!h]
  \caption{{\bf CVUSA combinations for hard-voting with the most accurate model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \includegraphics[width=0.9\linewidth]{fig10.tiff.png}
  \label{fig10}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVACT combinations for hard-voting with the most accurate  model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  
  \includegraphics[width=0.9\linewidth]{fig11.tiff.png}
  \label{fig11}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVUSA combinations for hard-voting with the random selection strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. }
  \includegraphics[width=0.9\linewidth]{fig12.tiff.png}
  \label{fig12}
\end{figure}

\begin{figure}[!h]
  \caption{{ CVACT combinations for hard-voting with the random selection strategy.}   The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \includegraphics[width=0.9\linewidth]{fig13.tiff.png}
  \label{fig13}
\end{figure}

\FloatBarrier


Fig~\ref{fig14} illustrates that soft voting performs  the best. Although soft voting results with joint probability and averaging are identical, the computational cost of averaging strategy is cheaper. Hard-voting strategies have constant computational complexity. Soft-voting improves accuracy by looking at the non-k predictions across different models. For the sake of illustration,  Fig~\ref{fig15} and Fig~\ref{fig16} show an example of the predictions of the individual models. None of the models returned the right prediction as the first prediction, but it was in the top-5 predictions for most models. Their collective prediction (the model ensemble) could return it as the first prediction.

\begin{figure}[!h]
  \caption{{\bf Comparison between aggregation method for the best performing combinations. } {\bf A}: CVUSA and {\bf B} CVACT.  The dark bar is the best performing aggregation method. Soft voting outperforms other methods across all r@k metrics for both datasets.}
  
  \includegraphics[width=0.9\linewidth]{fig14.tiff.png}
  \label{fig14}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example that shows the Mixed Model architecture r@(1 - 5) compared to the individual models on the CVUSA dataset.} The true satellite has a red border.}
  \includegraphics[width=0.9\linewidth]{fig15.tiff.png}
  \label{fig15}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example that shows the Mixed Model architecture r@(1 - 5) compared to the individual models on the CVACT dataset.}  The true satellite has a red border.}
  \includegraphics[width=0.9\linewidth]{fig16.tiff.png}
  \label{fig16}
\end{figure}

\FloatBarrier

\subsection*{B) EgoTR fine-tuning}

The training process took ~$192$ hours for $228$ epochs. Table~\ref{table6} shows the r@k metrics for the model. This drop in accuracy can be attributed to {\bf 1)} The ground images in our dataset aren't panoramic, in contrast to CVUSA and CVACT. {\bf 2)} High similarity between the consecutive pairs. {\bf 3)} One of the shortcomings of the r@k metric is that it depends on the size of the validation dataset as shown in  Figure~\ref{fig17}, our validation dataset size is more than double the size of CVUSA or the size of CVACT.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf r@k metrics of EgoTR fine-tuned over the reshaped BDD-trajectories dataset.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}ccccccc@{}}
    \toprule
    & {\bf r@1 (\%)} & {\bf r@5 (\%)} & {\bf r@10 (\%)} & {\bf r@1\% (\%)} &\\ \midrule
    & 9.99 & 9.99 & 16.72 & 66.98m & \\ \bottomrule
    \end{tabular}
  }
  \label{table6}
\end{table}

\begin{figure}[!h]
  \caption{{\bf The effect of the size of the validation dataset on the r@k metric.}  Same model (EgoTR) with the same dataset (BDD-trajectories). The accuracy decreases as the size of the validation dataset increases.}
  \includegraphics[width=0.9\linewidth]{fig17.tiff.png}
  \label{fig17}
\end{figure}
\FloatBarrier

\subsection*{C) Plain naive history}
Our experiments, Figure~\ref{fig18}, show that the more we look back on the history of the journey, the more accuracy improves. The algorithm has no assumptions about the current state, and its computational cost is negligible compared to generating the distance array.


\begin{figure}[!h]
  \caption{{\bf The effect of the number of steps we look back on the accuracy.} The accuracy converges to ~100\% after seven steps on our dataset.}
  \includegraphics[width=0.9\linewidth]{fig18.tiff.png}
  \label{fig18}
\end{figure}

\FloatBarrier

\subsection*{D) Naive history with weak prior}
Fig~\ref{fig19} shows that naive history with weak prior with 2 steps lookback outperforms plain naive history with 5 steps lookback. And it only takes 3 steps for naive history with a prior to converge to 100\%, compared to 7 steps (Fig~\ref{fig18}) for the plain version. 
However promising this is, if the prior is wrong it can result in “trapping” the algorithm in the wrong state which will degrade the accuracy significantly, and this is the “naive” part of the naive history algorithm.

\begin{figure}[!h]
  \caption{{\bf The effect of weak prior on naive history.}  The accuracy of naive history improves significantly when starting with some prior knowledge about the trip starting point. The brown line(naive history with prior) and the green line (plain naive history) represent the same number of steps into the trip though the brown line shows more accurate predictions due to factoring in the weak prior.}
  \includegraphics[width=0.9\linewidth]{fig19.tiff.png}
  \label{fig19}
\end{figure}

\FloatBarrier
\section*{Conclusion amd future work}
In this work, we created a model ensemble to merge the predictions of numerous cutting-edge models. Our ensemble model accuracy surpassed the current state-of-the-art. Also, we demonstrated the effect of factoring in trip temporal information. And we managed to converge to 100\% using naive history. But none of the available benchmark datasets are appropriate for extensive temporal awareness experiments, so we created a new derivative dataset based on BDD100K. And paved the way to use this dataset to build an end-to-end model that exploits the temporal correlation during a single trip, and fuses other data modalities and sources during querying and training. Also, we can see there is room for analyzing different state-of-art models to identify the most promising building modules and then use the network architecture search (NAS) paradigm to develop an optimal CV matching network. We anticipate that this study will kick-start the development of deployable cross-view geo-localization models, exploring fusing other data modalities and sources during querying and training. And we believe there is a great gap for real-time, weather condition-averse models that can initiate many research points.

\nolinenumbers
\FloatBarrier

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\begin{thebibliography}{10}

\bibitem{bib1} 
ref
\newblock{info} 

\bibitem{bib2} 
ref
\newblock{info} 

\bibitem{bib3} 
ref
\newblock{info} 

\bibitem{bib4} 
ref
\newblock{info} 

\bibitem{bib5} 
ref
\newblock{info} 

\bibitem{bib6} 
ref
\newblock{info} 

\bibitem{bib7} 
ref
\newblock{info} 

\bibitem{bib8} 
ref
\newblock{info} 

\bibitem{bib9} 
ref
\newblock{info} 

\bibitem{bib10} 
ref
\newblock{info} 

\bibitem{bib11} 
ref
\newblock{info} 

\bibitem{bib12} 
ref
\newblock{info} 

\bibitem{bib13} 
ref
\newblock{info} 

\bibitem{bib14} 
ref
\newblock{info} 

\bibitem{bib15} 
ref
\newblock{info} 

\bibitem{bib16} 
ref
\newblock{info} 

\bibitem{bib17} 
ref
\newblock{info} 

\bibitem{bib18} 
ref
\newblock{info} 

\bibitem{bib19} 
ref
\newblock{info} 

\bibitem{bib20} 
ref
\newblock{info} 

\bibitem{bib21} 
ref
\newblock{info} 

\bibitem{bib22} 
ref
\newblock{info} 

\bibitem{bib23} 
ref
\newblock{info} 

\bibitem{bib24} 
ref
\newblock{info} 

\bibitem{bib25} 
ref
\newblock{info} 

\bibitem{bib26} 
ref
\newblock{info} 

\bibitem{bib27} 
ref
\newblock{info} 

\bibitem{bib28} 
ref
\newblock{info} 

\bibitem{bib29} 
ref
\newblock{info} 

\bibitem{bib30} 
ref
\newblock{info} 

\bibitem{bib31} 
ref
\newblock{info} 

\bibitem{bib32} 
ref
\newblock{info} 

\bibitem{bib33} 
ref
\newblock{info} 

\bibitem{bib34} 
ref
\newblock{info} 

\bibitem{bib35} 
ref
\newblock{info} 

\bibitem{bib36} 
ref
\newblock{info} 

\bibitem{bib37} 
ref
\newblock{info} 

\bibitem{bib38} 
ref
\newblock{info} 

\bibitem{bib39} 
ref
\newblock{info} 

\bibitem{bib40} 
ref
\newblock{info} 

\bibitem{bib41} 
ref
\newblock{info} 

\bibitem{bib42} 
ref
\newblock{info} 

\bibitem{bib43} 
ref
\newblock{info} 

\bibitem{bib44} 
ref
\newblock{info} 

\bibitem{bib45} 
ref
\newblock{info} 

\bibitem{bib46} 
ref
\newblock{info} 

\bibitem{bib47} 
ref
\newblock{info} 

\bibitem{bib48} 
ref
\newblock{info} 

\bibitem{bib49} 
ref
\newblock{info} 

\bibitem{bib50} 
ref
\newblock{info} 

\bibitem{bib51} 
ref
\newblock{info} 

\bibitem{bib52} 
ref
\newblock{info} 

\bibitem{bib53} 
ref
\newblock{info} 

\bibitem{bib54} 
ref
\newblock{info} 

\bibitem{bib55} 
ref
\newblock{info} 

\end{thebibliography}



\end{document}

