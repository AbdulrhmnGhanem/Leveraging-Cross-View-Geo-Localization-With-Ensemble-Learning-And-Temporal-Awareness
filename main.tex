\documentclass[10pt,letterpaper]{article}
\usepackage{booktabs, makecell}
\usepackage{multirow}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}
\usepackage[linesnumbered]{algorithm2e}
\usepackage[section]{placeins}

\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}


% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage[nopatch=eqnum]{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Leveraging cross-view geo-localization with ensemble learning and temporal awareness} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Abdulrahman Ghanem\textsuperscript{1},
Ahmed Abdelhay\textsuperscript{1},
Noor Eldeen Salah\textsuperscript{1},
Ahmed Nour Eldeen\textsuperscript{1},
Mohammed Elhenawy\textsuperscript{2},
Abdallah A. Hassan\textsuperscript{1},
Ammar M. Hassan\textsuperscript{3*},
Mahmoud Masoud\textsuperscript{2}
\\
\bigskip
\textbf{1} Computer and Systems Engineering Department, Faculty of Engineering, Minia University, Minia, Egypt
\\
\textbf{2}  Centre for Accident Research and Road Safety-Queensland (CARRS-Q), Queensland University of Technology, Brisbane, Australia
\\
\textbf{3} Arab Academy for Science, Technology, and Maritime Transport, South Valley Branch, Aswan, Egypt
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
% \Yinyang These authors contributed equally to this work.

% % Additional Equal Contribution Note
% % Also use this double-dagger symbol for special authorship notes, such as senior authorship.
% \ddag These authors also contributed equally to this work.

% % Current address notes
% \textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% % \textcurrency b Insert second current address 
% % \textcurrency c Insert third current address

% % Deceased author note
% \dag Deceased

% % Group/Consortium Author Note
% \textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* ammar@aast.edu

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
In some situations the Global Navigation Satellite System (GNSS) is unreliable. To mend the poor GNSS signal, an autonomous vehicle can self-localize by matching a ground image against a database of geotagged aerial images. It is easy to construct an aerial dataset, so this approach is preferable to other image-based localization techniques. But, this approach is challenging: there are dramatic viewpoint changes between aerial and ground views, harsh weather and lighting conditions, and lack of orientation information, in training and deployment environments. In this paper, we show that previous models in this area are complementary, not competitive, and that each model solves a different aspect of the problem. We propose an ensemble model to aggregate the predictions of multiple independently trained state-of-the-art models. Also, we show the effect of making the query process temporal-aware  But none of the existing benchmark datasets is suitable for extensive temporal awareness experiments, we generated a new derivative dataset based on the BDD100K dataset. This paper also proposes an efficient temporal awareness mechanism: naive history, to prove that fusing the temporal information of the trip with the model prediction significantly improves the accuracy. Our ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset (surpassing the current state-of-the-art). Our temporal awareness mechanism converges to R@1 of ~100\% by looking at a few steps back in the trip history. We expect this study to initialize the building of deployable ensemble cross-view geo-localization models that take advantage of temporal proximity between ground images.

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
% \section*{Author summary}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
The current standard localization technique is the global navigation satellite system (GNSS). Although the GNSS accuracy declines in cases where there are few lines of sight (e.g., urban canyons~\cite{bib1}). Using cross-view geo-localization, a vehicle localizes itself by matching street view images against a database of geotagged images captured from aerial platforms (e.g., a satellite~\cite{bib2} or a drone~\cite{bib3}). Cross-view geo-localization is gaining popularity in the scene of autonomous vehicles~\cite{bib2} and robotic navigation~\cite{bib25}: it compensates for a bad GNSS signal-to-noise ratio. And,  it’s preferable to other image-based localization techniques (e.g., landmark and ground-to-ground matching) for the ease of covering new areas. Early work~\cite{bib37} on this technique claimed that its main challenge is the lack of visual correspondence between aerial and ground views. Later work, while holding the same claim, showed that there are more challenges: geographic scene changes over time~\cite{bib6,bib7,bib8,bib9,bib10}, poor weather and lighting conditions, lack of orientation information~\cite{bib4}, and misalignment between aerial and ground images during training.
None of the previous models tried to collectively address these five challenges. To build a holistic solution for the problem, we propose an ensemble model to fuse the predictions of five independently trained models. Each of these models addresses a different challenge.

Moreover, most of the recent work treats the problem as a 1-to-1 image-matching task. This overlooks the fact that these models get deployed in environments where there's a continuous stream of input, not a single query image. Taking the temporal nature of the problem into account is a must:  our experiments shows that the recent models can't differentiate between highly similar query images, and in the case of a moving vehicle, the consecutive images have a high degree of similarity. Some recent models considered the temporal nature of the problem.~\cite{bib7}  and~\cite{bib20,bib21,bib22} used \emph{Markov chain Monte Carlo (MCMC)} algorithms to predict the current pose and enforce temporal consistency.~\cite{bib24} enforces temporal consistency by using a transformer-based trajectory smoothing network. We can see that these methods are resource intensive or have strong assumptions about the current state of the vehicle. We explore an efficient technique to achieve the same goal.
Although the existing datasets aren't suitable for conducting experiments to prove that temporal awareness improves accuracy  — \cite{bib24} used BDD100k to generate a derivative dataset, sorrowfully, upon contacting the authors to grant us access to this dataset we received no response. The dataset needs to be realistic and collected as a trajectory of close points over a long-running journey to resemble driving in a real environment.  CVUSA~\cite{bib55} and CVACT~\cite{bib4} include sparse points on the map. VIGOR~\cite{bib5} includes dense points but doesn't form trajectories. RobotCar~\cite{bib6} has a limited number of examples. So we had to introduce a new derivative dataset based on the BDD100K~\cite{bib12} dataset to fill this gap.

\subsection*{In summary, the contributions of this paper are:}
\begin{itemize}
    \item We introduce a new ensemble model based on five of the current state-of-the-art  cross-view matching networks. The ensemble model achieves a recall accuracy R@1 of 97.74\% on the CVUSA dataset and 91.43\% on the CVACT dataset.
    \item We construct a new derivative dataset that is suitable for temporal aware cross-view geo-localization models based on BDD100K.
    \item We develope a meta block: Naive History, to make the query process temporal aware. We show that taking journey history into account minimizes the search space. This reduction in search space improves the accuracy.  The accuracy converges to ~100\% with a three-step lookback into trip history on our proposed dataset. This meta block is usable with any CV model.
\end{itemize}

\section*{Related work}
In section (A), we start by investigating how different models engineered their features and architectures. Their choices show us how different models tried to approach the problem from different angles. Followed by giving a bird-eye-view of the architectures of these models and feature extraction methods. In section (B), we walk through the models that tried blending the trip history into the image-matching task. By grouping the models into two categories: one that relies on "this place looks familiar" in section (B.1), and one that relies on "where have we been before getting here?" in section (B.2).
\subsection*{A) Features and architectures}
Most of the recent work treated the cross-view geo-localization task as an image retrieval task. They tried to find a feature representation suitable for matching query ground images and aerial ground images. The nature and complexity of used networks changed over time. Here, we conduct a brief comparison between the most common feature representations and their architectures. Table~\ref{table1} summarizes the feature types and the backbones of different cross-view geo-localization networks.


\renewcommand{\arraystretch}{1.2}
\begin{table}[!ht]
  \centering
  \caption{
  {\bf A summary of the feature types and the backbones of different cross-view geo-localization networks.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Feature type} & {\bf Backbone} & {\bf Used in}&  \\ \midrule
    & Hand-crafted & SIFT & ~\cite{bib20}~\cite{bib26}& \\
    & Hand-crafted & SURF, FREAK, PHOW & ~\cite{bib20}& \\
    & Hand-crafted & SIFT + VLAD & ~\cite{bib7}& \\
    & Semantic & Faster R-CNN & ~\cite{bib32}& \\
    & CNN & VGG + FCN + NetVLAD & ~\cite{bib36}~\cite{bib22}~\cite{bib14}& \\
    & CNN & AlexNet & ~\cite{bib37}~\cite{bib32}& \\
    & CNN & VGG & ~\cite{bib21}~\cite{bib4}~\cite{bib13}& \\
    & CNN & ResNet & ~\cite{bib4}~\cite{bib21}~\cite{bib41}& \\
    & CNN & DenseNet & ~\cite{bib4}~\cite{bib21}& \\
    & CNN & U-net & ~\cite{bib4}& \\
    & CNN & Xception & ~\cite{bib21}& \\
    & CNN & - & ~\cite{bib44}& \\
    & CNN & VGG + FCN & ~\cite{bib43}& \\
    & Attentive & Siam-FCANet (ResNet + FCAM + FCN) & ~\cite{bib45}& \\
    & Attentive & Siam-VFCNet (ResNet + FCAM + NetVLAD) & ~\cite{bib45}& \\
    & Attentive & VGG + SAFA + SPE & ~\cite{bib46}~\cite{bib47}~\cite{bib17}& \\
    & Attentive & VGG + Geo Attention + Geo-temporal Attention & ~\cite{bib24}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib8}& \\
    & Attentive & SAFA & ~\cite{bib5}& \\
    & Attentive & ResNet + SAFA & ~\cite{bib18}& \\
    & Attentive & ResNet + Self Cross Attention & ~\cite{bib19}& \\
    & Attentive & VGG + MSAE & ~\cite{bib49}& \\
    & Synthesized & X-Fork & ~\cite{bib50}& \\ \bottomrule
    \end{tabular}
  }
  \label{table1}
\end{table}

\subsubsection*{1) Hand-crafted features}
Early research used hand-crafted features.~\cite{bib20,bib26} used SIFT~\cite{bib27}. Also,~\cite{bib20} experimented with other feature spaces (e.g., SURF~\cite{bib28}, FREAK~\cite{bib29}, PHOW~\cite{bib30}) but SIFT outperformed others.~\cite{bib7} computed dense SIFT features, then embedded them into a higher dimensional vector space using a VLAD~\cite{bib31}. The extracted features are brittle; it fails to adapt to appearance change. Later, it proved to have inferior performance compared to the CNN-based features.
\subsubsection*{2) Semantic features}
In this approach, the networks matched between ground and aerial images based on the meaningful content of the image. This made it more robust to viewpoint changes than local features. But the model performance degraded in areas lacking the pre-selected semantic features.~\cite{bib32} treated the problem as object detection and recognition: the first block of the architecture employed the Faster R-CNN~\cite{bib33} to detect buildings, and the second block of the architecture used AlexNet~\cite{bib34} with the Siamese architecture~\cite{bib35} to recognize the buildings.
\subsubsection*{3) CNN-based features}
Metric learning achieved promising results in bridging the domain gap between aerial and ground image representation.~\cite{bib36} used a fully convolutional network (FCN) with a NetVLAD~\cite{bib34} layer using a Siamese architecture.~\cite{bib37} tried modified versions of AlexNet.~\cite{bib21} experimented with different FCN layers: VGG~\cite{bib31}, ResNet~\cite{bib38}, DenseNet~\cite{bib39}, and Xception~\cite{bib40} with the same architecture of~\cite{bib36}, they found that VGG outperforms other networks. Also,~\cite{bib22} used the CVM-Net-I architecture proposed in~\cite{bib36}.~\cite{bib41} exploited a modified ResNet50 network for ground images and a ResNet18 for aerial ones.~\cite{bib13} used VGG16 to generate the feature maps for the polar transformed aerial and then fed it into the Dynamic Similarity Module (DSM).~\cite{bib4} learned orientation information by using different backbones (VGG, ResNet, DenseNet, and U-net~\cite{bib42}) with the Siamese architecture.~\cite{bib43} employed  the Siamese network with a VGG backbone to extract feature maps, then a fully connected layer aggregates these feature maps.~\cite{bib44} used a CNN to generate feature maps and then transform them from the ground domain to the aerial domain.~\cite{bib14} applied the hybrid perspective mapping method using the CVM-Net-I architecture.
\subsubsection*{4) Attentive features}
For this feature type, the networks used spatial attention to enhance the feature representation.~\cite{bib45} integrated the lightweight attention module (FCAM) into each block of the basic ResNet.~\cite{bib46} used a spatial-aware feature aggregation (SAFA) module to mitigate the distortion in the aerial image. Also, employed the spatial-aware position embedding module (SPE) to encode relative positions among features in the feature maps.~\cite{bib5} proposed the VIGOR network built on top of SAFA.~\cite{bib47,bib17} used the same architecture as~\cite{bib46} with a different loss function: geo-distance weighted loss.~\cite{bib24} proposed a geo-attention module for the aerial branch, and a temporal-attention module for the ground branch.~\cite{bib8} used convolutional block attention modules~\cite{bib48} to generate multi-scale attention features.~\cite{bib18} employed a modified ResNet34 backbone with a spatial-aware attention module.~\cite{bib19} proposed EgoTR network. EgoTR used a ResNet backbone transformer encoder with a self-cross attention mechanism.~\cite{bib49} introduced the Multi-Scale Attention Encoder (MSAE). MSAE employed a VGG backbone with a multi-scale attention encoder followed by FCN to generate feature masks.

\subsubsection*{5) Synthetic features}
In this approach, the networks learned robust feature representation by reversing the task: it learned how to create ground view from aerial views, which made it learn salient features and suppress others.~\cite{bib50} synthesized aerial representation of a ground panorama query using the X-Fork network~\cite{bib51} with edge maps detection by Canny Edge Detection~\cite{bib52}.

\subsection*{B) Contextual awareness}
The fact that these models get deployed in autonomous vehicles, makes it obvious that the models should be aware of the trip context. We can categorize contextual awareness as follows:

\subsubsection*{1) Spatial awareness}
We have to differentiate between two types of spatial awareness.

\begin{itemize}
  \item Some models refer to it as the knowledge about the pose (location and orientation) of the query image and its relative pose to the aerial image frame.
  \cite{bib47,bib17} constructed mini-batches of images within a certain geographic radius and used a modified version of the triplet loss function: Geo-distance weighted loss to favor examples where the images are within the selected radius. The Geo-Attention module used in~\cite{bib24} exploits a similar loss function.~\cite{bib14} employed hybrid perspective mapping to establish correspondence between ground and aerial images.~\cite{bib4} injected the orientation information into the network.~\cite{bib4} used multiplane image (MPI)~\cite{bib13} projections to exploit geometric correspondence between ground and aerial images. Table~\ref{table2} gives an overview of spatial (pose-wise) awareness approaches used in cross-view geo-localization.
  \begin{table}[!ht]
    \centering
    \caption{
    {\bf An overview of the spatial (pose-wise) awareness approaches used in cross-view geo-localization.}}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Geographic proximity & ~\cite{bib47}~\cite{bib17}~\cite{bib24}& \\
    & Polar Transform & ~\cite{bib3,bib4}~\cite{bib5}~\cite{bib13,bib14}~\cite{bib17,bib18,bib19}& \\
    & Inverse Polar Transform & ~\cite{bib49}& \\
    & Orientation & ~\cite{bib4}& \\
    & Dynamic Similarity Matching (DSM) & ~\cite{bib13}& \\
    & Hybrid Perspective Mapping & ~\cite{bib14}& \\ \bottomrule
    \end{tabular}
    \label{table2}
  \end{table}
  \item Other networks refer to it as paying more attention to the salient features, suppressing less important features, and encoding the spatial layout information into the feature representation. We have covered that in section (A.4).
\end{itemize}

\subsubsection*{2) Temporal awareness}
There are two types of temporal awareness too!
\begin{itemize}
  \item Finding a robust feature representation that won’t be affected by scene changes throughout time. These changes can be geographic landmarks (e.g., new constructions) or environmental conditions such as weather conditions.
  ~\cite{bib6,bib54} used time-invariant approaches by capturing the same scene during different conditions across time. But, this technique required a dataset where the same scene is covered during different conditions. Possible solutions for this are as follows. {\bf A)}~\cite{bib8} used semantic object-based data augmentation techniques to remove and add objects (cars, roads, greenery, and sky). {\bf B)}~\cite{bib7} applied PCA projection to make background features less significant in the image descriptor. {\bf C)} Another solution is using synthetic datasets.
  \item Exploiting the fact these models will be deployed on vehicles where a temporally coherent sequence of images is available.
  ~\cite{bib7} and~\cite{bib20,bib21,bib22} used MCMC algorithms, namely, a particle filter~\cite{bib45} with variations of initialization and transition techniques, the algorithms are used to predict current pose and enforce temporal consistency.~\cite{bib24} introduced a geo-temporal attention module, the module attends to all frames in the video to learn better features, it also enforces temporal consistency by using a transformer-based trajectory smoothing network.
\end{itemize}
We give an overview of the temporal awareness approaches used in cross-view geo-localization in Table~\ref{table3}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf An overview of the temporal awareness approaches used in cross-view geo-localization.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllll@{}}
    \toprule
    & {\bf Approach} & {\bf Used in} & \\ \midrule
    & Capture multiple examples with different environmental conditions & ~\cite{bib6}~\cite{bib54}& \\
    & Semantic object-based data augmentation & ~\cite{bib8}& \\
    & Observation encoder + PCA projection & ~\cite{bib7}& \\
    & MCMC algorithms & ~\cite{bib7}~\cite{bib20,bib21,bib22}& \\
    & Sequence attention + Trajectory Smoothing Network & ~\cite{bib24}& \\ \bottomrule
    \end{tabular}
  }
  \label{table3}
\end{table}

\section*{Methodology}
\subsection*{A) Evaluation metric}
In this research, we use the same evaluation metric used in~\cite{bib3,bib4,bib5,bib8,bib13,bib14,bib17,bib18,bib19,bib20,bib21,bib22,bib24,bib36,bib43,bib44,bib45,bib46,bib47,bib49,bib50}: the {\bf Recall@k (r@k)}. In r@k, we consider it a match if the corresponding aerial image is in the top k predictions. We use r@1, r@5, r@10, and r@1\%. r@1 means the true image is the first prediction of the model, r@5 means the true image is in the first five predictions of the model, and so on.

\subsection*{B) The ensemble model}

There are many challenges in cross-view matching. To name a few:  missing correspondence and orientation information, scene changes over time, and the high similarity among geographically close points. Recently, several models were developed to solve the cross-view (CV) matching problem, and different models approached the CV matching problem from different angles. Thus, each model has its advantages and disadvantages. We hypothesize that aggregating the outputs of these uncorrelated models might improve the accuracy.
So, in this research, we build an ensemble of independently trained models to solve the CV matching problem. The proposed ensemble used the same datasets to train different neural network architectures. We use the CVUSA~\cite{bib55} and CVACT~\cite{bib4} benchmark datasets, to train five models Each model addresses a  different aspect of the problem: DSM~\cite{bib13} estimates the cross-view orientation alignment.  EgoTR~\cite{bib19} models global dependencies to decrease visual ambiguities and matches geometric configuration between ground and aerial images. SAFA~\cite{bib46} exploits geometric correspondence between aerial and panoramic ground images. Toker~\cite{bib18} biases the localization network via a Generative Adversarial Network (GAN) to learn salient features. SFCANet~\cite{bib45} uses Hard Exemplar Reweighting to assign a greater weight to hard examples. Table~\ref{table4} shows their r@k metrics.

\begin{table}[!ht]
\begin{adjustwidth}{-1.0in}{0in} % Comment out/remove adjustwidth environment if table fits in text column.
  \centering
  \caption{
    {\bf The r@k metrics for the networks used to construct the  ensemble model.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}lllllcrrrrrrr@{}}
      \toprule
      & \multirow{3}{*}{\bf Network} & \multicolumn{4}{c}{\bf CVUSA} & \phantom{abc}& \multicolumn{4}{c}{\bf CVACT} & \phantom{abc} & \\
      \cmidrule{3-6} \cmidrule{8-11}
      & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \phantom{abc} & r@{\bf 1} & r@{\bf 5} & r@{\bf 10}&  r@{\bf 1\%} & \\
      &\phantom{abc} & \multicolumn{9}{c}{(\%)}  \\ \bottomrule
      & DSM~\cite{bib13} & 92.07 & 97.33 & 98.33 & 99.60 & \phantom{abc} & 81.93 & 91.83 & 93.95 &  97.42& \\
      & Toker~\cite{bib18} & 92.15 & 97.28 & 98.26 &  99.56 & \phantom{abc}  & 83.52 & 93.89 & 95.48 &  98.17& \\
      & EgoTR~\cite{bib19} & 93.87 & 98.22 & 98.98 & 99.66 & \phantom{abc} & 84.87 & 94.5 & 95.95 &  98.37& \\
      & SFCANet~\cite{bib45} & 51.01 & 78.92 & 86.80 & 98.49 & \phantom{abc} & x & x & x &  x& \\ 
      & SAFA~\cite{bib46} & 88.93 & 96.65 & 97.97 & 99.65 & \phantom{abc} & 74.56 & 89.65 & 92.46 &  97.72& \\ \bottomrule
    \end{tabular}
  }
  \label{table4}
  \end{adjustwidth}
\end{table}

This research investigates two different aggregation methods: soft-voting, and hard-voting. As shown in Fig~\ref{fig1}, for both methods, we try all possible  combinations of the models (their power set).

\begin{figure}[!h]
\caption{{\bf Different ensemble aggregation methods.}}
\label{fig1}
\end{figure}

In soft-voting, we experiment with two calculation methods: {\bf A)} averaging the predictions of the individual models. {\bf B)} calculating the joint probability, using Eq~(\ref{eq1}). Both methods have identical results.

\begin{eqnarray}
\label{eq1}
\text{total vote} = \exp(\log(vote_1) + \log(vote_2) + ..., \log(vote_n)),
\end{eqnarray}

In hard-voting, we select the majority vote of the models. Hard-voting needs at least three models to have a majority vote. There are two cases where a majority vote doesn’t exist:
\begin{enumerate}
  \item Combinations with an even number of models can tie, so we pick the combination containing the most accurate model.
  \item All models’ predictions differ, we try two strategies: {\bf A)} take the prediction of the most accurate model in the combination. {\bf B)} pick a prediction from a random normal distribution of individual models' predictions.
\end{enumerate}

\subsection*{B) BDD trajectories dataset collection}
Proposed approaches in this research exploit the temporal nature of the problem. The existing benchmark datasets (e.g, CVUSA, and CVACT) are collected from sparse points on the map while we need a trajectory of close points. Inspired by the work of Regmi and Shah~\cite{bib24}, we construct a new derivative dataset from the BDD100k~\cite{bib12} dataset. The BDD100K dataset is crowdsourced, diverse, and large-scale, with IMU/GPS recording, and other semantic annotations (irrelevant to this research). All videos in the dataset are  long, though the total distance varies. We chose the videos with a distance greater than 50 m, the statistical summary of the distance covered in the selected videos is in Table~\ref{table5}.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf A statistical summary of the distance covered in selected videos.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
    & {\bf Count} & {\bf Mean} & {\bf \emph{std}} & {\bf Min} & {\bf Max} &\\ \midrule
    & 47943 videos & 278.668 m & 181.786 m & 50.000 m & 2560.000 m \\ \bottomrule
    \end{tabular}
  }
  \label{table5}
\end{table}

Our dataset consists of 95,000 examples. Each example consists of five ground images and one aerial image, and some examples are shown in Fig~\ref{fig2}. We sample the ground images from the picked videos with a sampling rate of $1 / frame/10 m$ to have some visual changes between the consecutive frames, but at the same time, the frames still look relatable to one another. The \emph{IMU/GPS} data is captured at $1 \ sample/s$. The distance moved in one second varies; the speed of the vehicles is not constant. In our dataset, we care about the visual changes, not the passage of time. So when the distance between every two consecutive locations is greater than $10 \ m$,  we use following the sampling algorithm:

\begin{algorithm}[!h]
\caption{BDD100K resampling}
\KwData{The GPS/IMU information recorded along with the videos, BDD100K videos.}
\KwResult{Resmapled frames(1 frame/10 m)}
\Comment{ Get the speed at the current ($v_n$), and next ($v_{n+1}$) location from IMU data.
\
Assume the speed between these two locations ($v_{n \to n+1}$) is the average speed of both locations. For all locations on the trajectory which is a multiple of the sampling distance parameter ($d$):}\
$v_{\text{n} \to \text{n+1}} \gets \frac{v_{\text{n}} + v_{\text{n} + 1}}{2}, n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \}$ \;

\Comment{To get the timestamp of the nth frame ($t_{\text{n}}$)}

$t \gets \frac{\text{d} \times  \text{n}}{v_{\text{n}} + v_{\text{n} + 1}} , n \in \{1, 2, 3, \text{\ldots}, \lfloor\frac{\text{trajectory distance}}{d} \} $;\

Extract the frames at the selected timestamps using FFmpeg~\cite{bib23};\
\end{algorithm}

\begin{figure}[!h]
  \caption{{\bf Three examples from our dataset. For ground images, the time progresses from left to right.}}
  \label{fig2}
\end{figure}

We capture the aerial tiles at the midpoint of the example using the great circle algorithm~\cite{bib16} and then fetch them from Google Maps~\cite{bib15}. We experimented with different zoom levels, Fig~\ref{fig3}. Furthermore, we chose the 20th zoom level; it covers a wide area with great detail. We chose a tile size of  $800 \times 800$ as shown in Fig~\ref{fig4}.

\begin{figure}[!h]
  \caption{{\bf Examples of three different zoom levels for aerial tile. Markers are 10 m apart on the trajectory.}}
  \label{fig3}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example of different tile sizes for the same zoom level.}}
  \label{fig4}
\end{figure}

After extracting the frames, we remove trajectories where 30\% of the extracted ground frames are mildly lit ($44.5\%$of all trajectories). For example, we take the trajectory of the Bright frame in Fig~\ref{fig5} and drop the other two trajectories. Then we removed the trajectories that have blurry aerial tiles ($13.5\%$ of the bright trajectories) similar to the example shown in Fig~\ref{fig6}. We use a Laplacian filter~\cite{bib11} with a threshold of $200$ to detect blurry aerial tiles, and a grayscale mean filter with a threshold of $85$ to detect dark ground frames. We chose both thresholds empirically to drop all the true positive corrupt examples with some false positives. Fig~\ref{fig7} shows a simplified dataflow of our cleaning pipeline. We scaled the ground and aerial images’ width to $400$ while keeping the height in aspect ratio using the \emph{Lanczos algorithm}.

\begin{figure}[!h]
  \caption{{\bf Examples of different lighting conditions in the BDD100K dataset.}}
  \label{fig5}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example of a blurry aerial image.}}
  \label{fig6}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf A simplified version of our data cleaning pipeline.}}
  \label{fig7}
\end{figure}

\subsection*{C) Naive history}
Inspired by our experiments on the joint probability as a soft-voting strategy for ensemble learning. In a journey setting where the history of the journey is available, we hypothesize that taking the previous predictions into account might cause the prediction to converge while progressing in the journey. This hypothesis relies on the fact that even if the model doesn't return the true location as its top-1 prediction, most of the time, it is still in the top-1\% predictions. Also, the probability that a model will predict N consecutive wrong predictions decreases as N increases. Based on this, we fine-tune the EgoTR model on the new deeravative dataset (\emph{BDD-trajectories}). We use this dataset because it has trip trajectories.

\subsubsection*{Fine-tuning EgoTR}
The EgoTR model takes a pair of images as input: a ground image and a satellite image as its input. However, an example in our dataset consists of a hexad: five ground images and a satellite image. So we have to reshape our dataset to be suitable for the EgoTR input format. We choose the examples where there are at least two examples from the same journey, so we have 10 ground images from the same trajectory. We pair every ground image and the satellite image in the examples. This means that each example in \emph{BDD-trajectories} corresponds to five examples in the reshaped dataset. We use a subset of reshaped dataset $19015$ pairs for validation and $75985$ pairs for fine-tuning.

\subsubsection*{Attaching the naive history block to EgoTR}
After fine-tuning EgoTR we generate a distance array for all the images in our validation dataset. Then we use this array as input for Algorithm~\ref{alg:two}.

\begin{algorithm}[H]
  \label{algorithm2}
  \caption{Naive history}\label{alg:two}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$}
  \KwResult{History reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  \Return $m_{ij}$\;
\end{algorithm}


The naive history meta block reinforces the prediction at the current position by looking back into the trip history. The $distanceArray$ parameter is the distance between every ground and aerial image (the output of the model).  The $historyDepth$ parameter controls how deep we look back into history. We shift the total distance array by $historyDepth$ columns and rows to get the distance array at the previous location (Algorithm 2, Line 6). We get a shifted version of the $distanceArray$ to leave the predictions at future steps intact (Algorithm 2, Line 7).  We multiply, element-wise ($\odot$), the distance array at the current step and the distance array at the previous step (Algorithm 2, Line 8). We update the original array with the reinforced predictions (Algorithm 2, Line 9). Repeat the steps 4 through 7 (Algorithm 2, Line 4), with increasing value of $historyDepth$ until reaching the value of final history depth value, for example, if we want to lookback five steps in trip history we will iterate over historyDepth  \{1, 2, 3, 4, 5\}.

\subsubsection*{The effect of prior on naive history}
As mentioned before the cross-view models are complementary to existing GNSS, so we can improve naive history performance by initializing it with a weak prior (the location captured by the GNSS).  We use Algorithm~\ref{alg:three} to initialize the distance array (generated by the model) with a probability of the first image in each example equal to $1\mathrm{e}{-6}$. In other words, (Algorithm 3, Lines 5-9) the probability of the first image in the trajectory is modified to make the probability of the ground truth image equal 1e-6 and set other probabilities to a uniform value of $(1 - (1\mathrm{e}{-6}) / distance array size)$. Our experiments prove that initializing the naive history algorithm with this prior knowledge speeds up the convergence to 100\% accuracy significantly.

\begin{algorithm}[H]
  \caption{Naive history with a weak prior}\label{alg:three}
  \KwData{$distanceArray_{ij}$, $historyDepth \geq 1$, $priori \in R_{+}$, $trajectorySize  \in N$}
  \KwResult{Prior-aware history reinforced distance array}
  $m_{ij} \gets distanceArray_{ij}$\;
  $D \gets historyDepth$\;
  $len \gets |m_{ij}|$\;
  $normalizer \gets priori/ len$\;
  
  \For{$k \in [0, len] \land k\ \% \ trajectorySize \equiv 0$} {
      $currentPrediction_{ij} \gets (m_{ij})_{\substack{i\\ j \equiv k }}$\;
      $ (currentPrediction_{ij})_{\substack{i \equiv k}} \gets priori$\;
      $(m_{ij})_{\substack{i\\ j \equiv k }} \gets currentPrediction_{ij} - normalizer$\;
  }
  
  \For{$historyDepth \in [1, D]$} {
      $prevDistanceArrayLen \gets len - historyDepth$\;
      $prevStepDistanceArray \gets (m_{ij})_{\substack{1\le i < prevDistanceArrayLen \\ 1\le j < prevDistanceArrayLen }}$\;
      $partialHistory \gets (m_{ij})_{\substack{D \le i \\ D \le j}}$\;
      $historyReinforcedDistanceArray \gets prevStepDistanceArray \odot partialHistory$\;
      $(m_{ij})_{\substack{D \le i \\ D \le j}} \gets historyReinforcedDistanceArray$\;
  }
  
  \Return $m_{ij}$\;
  \end{algorithm}

\section*{Results}
\subsection*{A) The ensemble model}
Fig~\ref{fig8} and Fig~\ref{fig9} show the results of the soft-voting strategies for CVUSA and CVACT, respectively. The DSM, EgoTR, and Toker combination outperforms other combinations. Increasing the number of the models doesn’t necessarily improve the accuracy; to improve the accuracy; individual models have to predict different examples correctly.

\begin{figure}[!h]
  \caption{{\bf CVUSA combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations.}
  \label{fig8}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVACT combinations for the soft-voting strategies. The dark bar is the best performing combination.} The DSM, EgoTR, and Toker combination outperforms other combinations. }
  
  \label{fig9}
\end{figure}

Fig\ref{fig10} and Fig~{fig11} show the results of hard-voting with the most accurate model prediction strategy for CVUSA and CVACT, respectively. The accuracy drops about 2\% for the r@1 and r@5 metrics. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. Fig~\ref{fig12} and Fig~\ref{fig13} show the results of the hard-voting with the random selection strategy for CVUSA and CVACT, respectively. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.


\begin{figure}[!h]
  \caption{{\bf CVUSA combinations for hard-voting with the most accurate model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \label{fig10}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVACT combinations for hard-voting with the most accurate  model prediction strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  
  \label{fig11}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVUSA combinations for hard-voting with the random selection strategy.} The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations. }
  \label{fig12}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf CVACT combinations for hard-voting with the random selection strategy.}   The dark bar is the best performing combination. The DSM, EgoTR, Toker, and SAFA combination outperforms other combinations.}
  \label{fig13}
\end{figure}


Fig~\ref{fig14} illustrates that soft voting performs  the best. Although soft voting results with joint probability and averaging are identical, the computational cost of averaging strategy is cheaper. Hard-voting strategies have constant computational complexity. Soft-voting improves accuracy by looking at the {\bf top-k predictions} across different models. For the sake of illustration,  Fig~\ref{fig15} and Fig~\ref{fig16} show an example of the predictions of the individual models. None of the models returned the right prediction as the first prediction, but it was in the top-5 predictions for most models. Their collective prediction (the model ensemble) could return it as the first prediction.

\begin{figure}[!h]
  \caption{{\bf Comparison between aggregation method for the best performing combinations. } {\bf A}: CVUSA and {\bf B} CVACT.  The dark bar is the best performing aggregation method. Soft voting outperforms other methods across all r@k metrics for both datasets.}
  
  \label{fig14}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example that shows the Mixed Model architecture r@(1 - 5) compared to the individual models on the CVUSA dataset.} The true satellite has a red border.}
  \label{fig15}
\end{figure}

\begin{figure}[!h]
  \caption{{\bf An example that shows the Mixed Model architecture r@(1 - 5) compared to the individual models on the CVACT dataset.}  The true satellite has a red border.}
  \label{fig16}
\end{figure}

\subsection*{B) EgoTR fine-tuning}

The training process took $192$ hours for $228$ epochs. Table~\ref{table6} shows the r@k metrics for the model. This drop in accuracy can be attributed to: {\bf A)} the ground images in our dataset aren't panoramic, in contrast to CVUSA and CVACT. {\bf B)} high similarity between the consecutive pairs. {\bf C)} one of the shortcomings of the r@k metric is that it depends on the size of the validation dataset as shown in  Figure~\ref{fig17}, our validation dataset size is more than double the size of CVUSA or the size of CVACT.

\begin{table}[!ht]
  \centering
  \caption{
  {\bf r@k metrics of EgoTR fine-tuned over the reshaped BDD-trajectories dataset.}}
  \scalebox{0.96}{
    \begin{tabular}{@{}ccccccc@{}}
    \toprule
    & {\bf r@1 (\%)} & {\bf r@5 (\%)} & {\bf r@10 (\%)} & {\bf r@1\% (\%)} &\\ \midrule
    & 9.99 & 9.99 & 16.72 & 66.98m & \\ \bottomrule
    \end{tabular}
  }
  \label{table6}
\end{table}

\begin{figure}[!h]
  \caption{{\bf The effect of the size of the validation dataset on the r@k metric.}  Same model (EgoTR) with the same dataset (BDD-trajectories). The accuracy decreases as the size of the validation dataset increases.}
  \label{fig17}
\end{figure}
\subsection*{C) Plain naive history}
Our experiments, Fig~\ref{fig18}, show that the more we look back on the history of the journey. And the more accuracy improves. The algorithm has no assumptions about the current state, and its computational cost is negligible compared to generating the distance array.


\begin{figure}[!h]
  \caption{{\bf The effect of the number of steps we look back on the accuracy.} The accuracy converges to ~100\% after seven steps on our dataset.}
  \label{fig18}
\end{figure}

\subsection*{D) Naive history with weak prior}
Fig~\ref{fig19} shows that naive history with weak prior with 2 steps lookback outperforms plain naive history with 5 steps lookback. And it only takes 3 steps for naive history with a prior to converge to 100\%, compared to 7 steps (Fig~\ref{fig18}) for the plain version. 
However promising this is, if the prior is wrong it can result in “trapping” the algorithm in the wrong state which will degrade the accuracy significantly, and this is the “naive” part of the naive history algorithm.

\begin{figure}[!h]
  \caption{{\bf The effect of weak prior on naive history.}  The accuracy of naive history improves significantly when starting with some prior knowledge about the trip starting point. The brown line(naive history with prior) and the green line (plain naive history) represent the same number of steps into the trip though the brown line shows more accurate predictions due to factoring in the weak prior.}
  \label{fig19}
\end{figure}

\section*{Conclusion amd future work}
In this work, we paved the way to use this dataset to build an end-to-end model that exploits the temporal correlation during a single trip, and fuses other data modalities and sources during querying and training by preparing a dataset suitable for this task. Also, we can see there is room for analyzing different state-of-art models to identify the most promising building modules and then use the network architecture search (NAS) paradigm to develop an optimal CV matching network. We anticipate that this study will kick-start the development of deployable cross-view geo-localization models, exploring fusing other data modalities and sources during querying and training. And we believe there is a great gap for real-time, weather condition-averse models that can initiate many research points.

\nolinenumbers
\FloatBarrier

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 
\begin{thebibliography}{10}
  \bibitem{bib1}
  Ben-Moshe, Boaz, et al. "Improving Accuracy of GNSS Devices in Urban Canyons." CCCG. 2011.

  \bibitem{bib2}
  M. Zhai, Z. Bessinger, S. Workman, and N. Jacobs, “Predicting ground-level scene layout from aerial imagery,” in IEEE Conference on Computer Vision and Pattern Recognition, 2017

  \bibitem{bib3}
  Tingyu Wang, Zhedong Zheng, Chenggang Yan, and Yi Yang. Each part matters: Local patterns facilitate cross-view geo-localization. arXiv preprint arXiv:2008.11646, 2020

  \bibitem{bib25}
  Wei‐liang, Z., et al.: Comprehensive review of autonomous taxi dispatching systems. Computer Science. 47(5), 9 (2020)

  \bibitem{bib37}
  Võ, Nam \& Hays, James. (2016). Localizing and Orienting Street Views Using Overhead Imagery. 9905. 494-509. 10.1007/978-3-319-46448-0\_30.

  \bibitem{bib6}
  Churchill, W., Newman, P.: Experience-based navigation for long-term localisation. The International Journal of Robotics Research (2013)

  \bibitem{bib7}
  A.-D. Doan, Y. Latif, T.-J. Chin, Y. Liu, S. F. Ch’ng, T.-T. Do, and I. Reid, “Visual localization under appearance change: A filtering approach,” 2019 Digital Image Computing: Techniques and Applications (DICTA), pp. 1–8, 2019.

  \bibitem{bib8}
  Royston Rodrigues and Masahiro Tani. Are these from the same place? seeing the unseen in cross-view image geo- localization. In Proceedings of the IEEE/CVF Winter Con ference on Applications of Computer Vision (WACV), pages 3753–3761, January 2021.

  \bibitem{bib9}
  Doan, A.D., Latif, Y., Chin, T.J., Liu, Y., Do, T.T., Reid, I.: Scalable place recog-nition under appearance change for autonomous driving. In: ICCV (2019)

  \bibitem{bib10}
  Milford, M. J., Wyeth, G. F., 2012. SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights. Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 1643–1649.
 
  \bibitem{bib4}
  L. Liu and H. Li, “Lending orientation to neural networks for cross-view geo-localization,” in IEEE Conference on Computer Vision and Pattern Recognition, 2019.

  \bibitem{bib20}
  Regmi, Krishna, "Exploring Relationships Between Ground and Aerial Views by Synthesis and Matching" (2021). Electronic Theses and Dissertations, 2020-. 747

  \bibitem{bib21}
  Hu, Sixing \& Lee, Gim. (2019). Image-Based Geo-Localization Using Satellite Imagery. 

  \bibitem{bib22}
  Dixit, Deeksha \& Tokekar, Pratap. (2020). Evaluation of Cross-View Matching to Improve Ground Vehicle Localization with Aerial Perception.

  \bibitem{bib24}
   K. Regmi and M. Shah, "Video Geo-Localization Employing Geo-Temporal Feature Learning and GPS Trajectory Smoothing," 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 12106-12115, doi: 10.1109/ICCV48922.2021.01191.
  
  \bibitem{bib55}
   S. Workman, R. Souvenir, and N. Jacobs. Wide-area image geolocalization with aerial reference imagery. In ICCV, 2015. 1, 2, 4.
  
  \bibitem{bib5}
   S. Zhu, T. Yang, and C. Chen, “Vigor: Cross-view image geo-localization beyond one-to-one retrieval,” 2020, arXiv:2011.12172.
  
  \bibitem{bib12}
   Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2636–2645, 2020.
  
  \bibitem{bib26}
   Mequanint, Eyasu \& Tesfaye, Yonatan \& Idrees, Haroon \& Prati, Andrea \& Pelillo, Marcello \& Shah, Mubarak. (2017). Large-Scale Image Geo-Localization Using Dominant Sets. IEEE Transactions on Pattern Analysis and Machine Intelligence. 41. 10.1109/TPAMI.2017.2787132. 
  
   \bibitem{bib32}
   Y. Tian, C. Chen and M. Shah, "Cross-View Image Matching for Geo-Localization in Urban Environments," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1998-2006, doi: 10.1109/CVPR.2017.216.

  \bibitem{bib36}
   S. Hu, M. Feng, R. M. H. Nguyen and G. H. Lee, "CVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-Localization," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 7258-7267, doi: 10.1109/CVPR.2018.00758.
  
  \bibitem{bib14}
   J. Wang, Y. Yang, M. Pan, M. Zhang, M. Zhu and M. Fu, "Hybrid Perspective Mapping: Align Method for Cross-View Image-Based Geo-Localization," 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), 2021, pp. 3040-3046, doi: 10.1109/ITSC48978.2021.9564573.
  
  \bibitem{bib13}
   Yujiao Shi, Xin Yu, Dylan Campbell, and Hongdong Li. Where am i looking at? joint location and orientation estimation by cross-view matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4064–4072, 2020.
  
  \bibitem{bib41}
   Abonce, Obed \& Zhou, Mengjie \& Calway, Andrew. (2019). You Are Here: Geolocation by Embedding Maps and Images.
  
   \bibitem{bib44}
   Shi, Yujiao \& Yu, Xin \& Liu, Liu \& Zhang, Tong \& Li, Hongdong. (2019). Optimal Feature Transport for Cross-View Image Geo-Localization. 
  
  \bibitem{bib43}
  S. Zhu, T. Yang and C. Chen, "Revisiting Street-to-Aerial View Image Geo-localization and Orientation Estimation," 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), 2021, pp. 756-765, doi: 10.1109/WACV48630.2021.00080.

  \bibitem{bib45}
  S. Cai, Y. Guo, S. Khan, J. Hu and G. Wen, "Ground-to-Aerial Image Geo-Localization With a Hard Exemplar Reweighting Triplet Loss," 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 8390-8399, doi: 10.1109/ICCV.2019.00848.

  \bibitem{bib46}
  Yujiao Shi, Liu Liu, Xin Yu, and Hongdong Li. 2019. Spatial-aware feature aggregation for cross-view image based geo-localization. Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 905, 10090–10100.

  \bibitem{bib47}
  Xia, Zimin \& Booij, Olaf \& Manfredi, Marco \& Kooij, Julian. (2020). Geographically Local Representation Learning with a Spatial Prior for Visual Localization. 10.1007/978-3-030-66096-3\_38.

  \bibitem{bib17}
  Z. Xia, O. Booij, M. Manfredi and J. F. P. Kooij, "Cross-View Matching for Vehicle Localization by Learning Geographically Local Representations," in IEEE Robotics and Automation Letters, vol. 6, no. 3, pp. 5921-5928, July 2021, doi: 10.1109/LRA.2021.3088076.

  \bibitem{bib18}
  A. Toker, Q. Zhou, M. Maximov and L. Leal-Taixé, "Coming Down to Earth: Satellite-to-Street View Synthesis for Geo-Localization," 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 6484-6493, doi: 10.1109/CVPR46437.2021.00642.

  \bibitem{bib19}
  Yang, Hongji \& Lu, Xiufan \& Zhu, Yingying. (2021). Cross-view Geo-localization with Evolving Transformer.

  \bibitem{bib49}
  Li, S., et al.: Multi-scale attention encoder for street-to-aerial image geo-localization. CAAI Trans. Intell. Technol. 1– 11 (2022).

  \bibitem{bib50}
  K. Regmi and M. Shah, "Bridging the Domain Gap for Ground-to-Aerial Image Matching," 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 470-479, doi: 10.1109/ICCV.2019.00056.

  \bibitem{bib27}
  Lowe, D.G. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision 60, 91–110 (2004).

  \bibitem{bib28}
  Bay, Herbert \& Tuytelaars, Tinne \& Van Gool, Luc. (2006). SURF: Speeded up robust features. Computer Vision-ECCV 2006. 3951. 404-417. 10.1007/11744023\_32.

  \bibitem{bib29}
  Alahi, Alexandre \& Ortiz, Raphael \& Vandergheynst, Pierre. (2012). FREAK: Fast retina keypoint. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 510-517. 10.1109/CVPR.2012.6247715. 

  \bibitem{bib30}
  Zisserman, Andrew \& Muñoz, Xavier. (2007). Image Classification using Random Forests and Ferns. ICCV. 1-8. 10.1109/ICCV.2007.4409066. 

  \bibitem{bib31}
  Simonyan, Karen \& Zisserman, Andrew. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv 1409.1556. 

  \bibitem{bib33}
  Ren, Shaoqing \& He, Kaiming \& Girshick, Ross \& Sun, Jian. (2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence. 39. 10.1109/TPAMI.2016.2577031. 

  \bibitem{bib34}
  Krizhevsky, Alex \& Sutskever, Ilya \& Hinton, Geoffrey. (2012). ImageNet Classification with Deep Convolutional Neural Networks. Neural Information Processing Systems. 25. 10.1145/3065386. 

  \bibitem{bib35}
  S. Chopra, R. Hadsell and Y. LeCun, "Learning a similarity metric discriminatively, with application to face verification," 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), 2005, pp. 539-546 vol. 1, doi: 10.1109/CVPR.2005.202.

  \bibitem{bib38}
  He, Kaiming \& Zhang, Xiangyu \& Ren, Shaoqing \& Sun, Jian. (2016). Deep Residual Learning for Image Recognition. 770-778. 10.1109/CVPR.2016.90. 

  \bibitem{bib39}
  G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger, "Densely Connected Convolutional Networks," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2261-2269, doi: 10.1109/CVPR.2017.243.

  \bibitem{bib40}
  F. Chollet, "Xception: Deep Learning with Depthwise Separable Convolutions," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 1800-1807, doi: 10.1109/CVPR.2017.195.

  \bibitem{bib42}
  Ronneberger, Olaf \& Fischer, Philipp \& Brox, Thomas. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. LNCS. 9351. 234-241. 10.1007/978-3-319-24574-4\_28. 

  \bibitem{bib48}
  Woo, S., Park, J., Lee, JY., Kweon, I.S. (2018). CBAM: Convolutional Block Attention Module. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds) Computer Vision – ECCV 2018. 

  \bibitem{bib51}
  Regmi, Krishna \& Borji, Ali. (2018). Cross-View Image Synthesis using Conditional GANs. 10.1109/CVPR.2018.00369. 

  \bibitem{bib52}
  Canny, John. (1986). A Computational Approach To Edge Detection. Pattern Analysis and Machine Intelligence, IEEE Transactions on. PAMI-8. 679 - 698. 10.1109/TPAMI.1986.4767851.

  \bibitem{bib54}
  Doan, Dzung \& Latif, Yasir \& Chin, Tat-Jun \& Liu, Yu \& Do, Thanh-Toan \& Reid, Ian. (2019). Scalable Place Recognition Under Appearance Change for Autonomous Driving. 9318-9327. 10.1109/ICCV.2019.00941.

  \bibitem{bib23}
  FFmpeg. A complete, cross-platform solution to record, convert and stream audio and video. Ffmpeg.org

  \bibitem{bib16}
  Great-circle distance - Wikipedia

  \bibitem{bib15}
  Maps Static API developers.google.com/maps/documentation/maps-static/overview

  \bibitem{bib11}
  Alazzawi, Abdulbasit \& Alsaadi, Husfam \& Shallal, Abidaoun \& Albawi, Saad. (2015). EDGE DETECTION-APPLICATION OF (FIRST AND SECOND) ORDER DERIVATIVE IN IMAGE PROCESSING. 

  \bibitem{bib53}
  Y. Shi, D. J. Campbell, X. Yu and H. Li, "Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery," in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2022.3140750.
\end{thebibliography}
\end{document}

